{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Crafting Digital History, A Workbook by Shawn Graham and Rob Blades This workbook supports #HIST3814o in the Late Summer term at Carleton University . This is the third major reworking of the workbook. The original Winter 2015 version is available on GitHub. For more advanced tutorials and help, please note the following: The Programming Historian Digital History Methods in R Shawn Graham, shawn dot graham at carleton dot ca, @electricarchaeo CAUTION: Photosensitivity and eye strain Some of the digital tools and tutorial videos used in this workbook can potentially cause eye strain or effect those with photosensitivity. Make sure to take breaks often, relax your eyes, and stretch. Video Tutorials Most exercises in this workbook are accompanied by a video tutorial. Please enable closed captions for more direction. Original content by Shawn Graham is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License . Cite this workbook:","title":"Home"},{"location":"#crafting-digital-history-a-workbook-by-shawn-graham-and-rob-blades","text":"This workbook supports #HIST3814o in the Late Summer term at Carleton University . This is the third major reworking of the workbook. The original Winter 2015 version is available on GitHub. For more advanced tutorials and help, please note the following: The Programming Historian Digital History Methods in R Shawn Graham, shawn dot graham at carleton dot ca, @electricarchaeo","title":"Crafting Digital History, A Workbook by Shawn Graham and Rob Blades"},{"location":"#caution-photosensitivity-and-eye-strain","text":"Some of the digital tools and tutorial videos used in this workbook can potentially cause eye strain or effect those with photosensitivity. Make sure to take breaks often, relax your eyes, and stretch.","title":"CAUTION: Photosensitivity and eye strain"},{"location":"#video-tutorials","text":"Most exercises in this workbook are accompanied by a video tutorial. Please enable closed captions for more direction. Original content by Shawn Graham is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License . Cite this workbook:","title":"Video Tutorials"},{"location":"conclusion/Conclusion/","text":"Conclusion There will be a profound and witty concluding paragraph here, some day. But if you've noticed anything at all, it's that the process of crafting digital history never comes to an end. Adam Crymble published a piece recently that throws this into high relief pay attention to his second paragraph in his main diagram! In the meantime, I want you to know that you too can craft excellent digital history. There were some great moments in the first iteration of this course, and you will have great moments too: like when Matt forked one of my tutorials and rewrote it for the better, or built a virtual machine. Or when Patrick finally slayed GitHub! Or when Allison got the Canadiana API to work. Or when Phoebe finally persuaded Inkscape to play nice. Or when Matt conquered TWARC. Or when TEI blew Ryan's mind. Or when Christina forked an Anthropology class project at MSU to repurpose for her project. Or... or... or. We covered a lot of ground. You will too. A word to the wise If, in your other courses, you decide to use some of the methods here, I will be most gratified. However, in course work as in life, know your audience. Will your professor appreciate this work? Is your professor familiar with the underlying issues will she know what to look for, the hidden gotchas, the places where things might get, erm, fudged? It is your responsibility to make the argument, in your work, why a particular methodological choice is appropriate. It is your responsibility to show how these choices are theoretically informed and meaningful. As in all history, you have to make the argument. Never fall into the trap of thinking the method speaks for itself. This is why I compelled you to create the paradata document for your project. As that master historian Yoda once said, Do or do not. There is no try. Alright, not exactly appropriate. But you get the gist: digital history requires explicit paradata. Do. There is no try.","title":"6. Final Thoughts"},{"location":"conclusion/Conclusion/#conclusion","text":"There will be a profound and witty concluding paragraph here, some day. But if you've noticed anything at all, it's that the process of crafting digital history never comes to an end. Adam Crymble published a piece recently that throws this into high relief pay attention to his second paragraph in his main diagram! In the meantime, I want you to know that you too can craft excellent digital history. There were some great moments in the first iteration of this course, and you will have great moments too: like when Matt forked one of my tutorials and rewrote it for the better, or built a virtual machine. Or when Patrick finally slayed GitHub! Or when Allison got the Canadiana API to work. Or when Phoebe finally persuaded Inkscape to play nice. Or when Matt conquered TWARC. Or when TEI blew Ryan's mind. Or when Christina forked an Anthropology class project at MSU to repurpose for her project. Or... or... or. We covered a lot of ground. You will too.","title":"Conclusion"},{"location":"conclusion/Conclusion/#a-word-to-the-wise","text":"If, in your other courses, you decide to use some of the methods here, I will be most gratified. However, in course work as in life, know your audience. Will your professor appreciate this work? Is your professor familiar with the underlying issues will she know what to look for, the hidden gotchas, the places where things might get, erm, fudged? It is your responsibility to make the argument, in your work, why a particular methodological choice is appropriate. It is your responsibility to show how these choices are theoretically informed and meaningful. As in all history, you have to make the argument. Never fall into the trap of thinking the method speaks for itself. This is why I compelled you to create the paradata document for your project. As that master historian Yoda once said, Do or do not. There is no try. Alright, not exactly appropriate. But you get the gist: digital history requires explicit paradata. Do. There is no try.","title":"A word to the wise"},{"location":"conclusion/Past Projects/","text":"So, I have permission to share some of these projects. In no particular order, here are some final projects from HIST3907b. Matt T - The Historical Consciousness of Reddit Matt D - What do Civil Servants Edit on Wikipedia? Ryan - Searching for Residential Schools: How Google Trends can illuminate who is talking about residential schools, where they are, how they're searching, and why. Patrick - Urban and Rural Voting Patterns in Three American Elections Christina - The St. Johns Micro History Mapping Project Luke - Late 20th Century Immigration in Bubbles You're next!","title":"Past Projects"},{"location":"introduction/crafting-digital-history/","text":"Getting yourself ready May 7-14, 2018 \"'Getting Ready for the Fire, Kasier Wilhelm' Bain Collection, Library of Congress hdl.loc.gov/loc.pnp/ggbain.10412 Call Number: LC-B2- 2400-7\" Welcome! This workbook is made by converting several plain-text files into a fully-operational website using the MkDocs static website generator . That means, you can keep a copy of all these files for your own records. Simply click on the 'Edit on GitHub' link at the top right. This will bring you to the repository that houses this workbook. Then, when you're signed into GitHub, you can fork (that is, make a copy) the repository into your own account. Why 'forking'? It seems an odd phrase. Think of it like this: Writing, crafting, coding: these are like a small river, flowing in one direction into the future. You get new ideas, new hunches: the river branches. There's a fork in its path. Sometimes new ideas, new hunches, fold back into that original stream: they merge. GitHub is a way of mapping that stream, and a guide to revisiting the interesting parts of it. It's not the best metaphor, but it'll do. No doubt you've had that experience where, after working on an essay for days, you make a change that you regret. You wish you could go back to fix it, but ctrl+z only goes so far. You realize that everything you've done for the last week needs to be thrown out, and you start over. Well, with 'versioning control', you can travel back upriver, back to where things were good. There are two tools that we will use to make this happen: git and GitHub . You'll learn more about making those things work in Module 1 . You'll see why you'd want to do that, and how to future-proof your work, writing things in a plain-text format called 'Markdown' . In Module 2 , we'll start exploring the wide variety of historical data out there. We'll talk about some of the ethical dilemmas posed by having so much data out there. Data are not neutral 'things given'; rather, they are capta : things taken . NB I am assuming, in this course, that the digital materials we want have already been digitized. Digitizing, adding meta-data (information that describes the data), and structuring it properly are very complex topics on their own that could be the subject of a complete course! If you are interested in those problems, a good place to start is this open course from the University of London's School of Advanced Study on database design for historians . In Module 3 , we'll see that data/capta are messy , and that they make a kind of illusory order that as historians, we are not normally in the habit of thinking about. The big secret of digital history is that the majority of your time on any digital history project won't be finding the capta, won't be analyzing the capta, won't be thinking about the historical meaning of the capta. We'll be cleaning it up . The exercises in this module will help you do that more efficiently (and be aware that 'efficiency' in computing terms is not a theoretically neutral idea!). In Module 4 , we talk about doing the analysis. I present finding, cleaning, and analyzing as if they were sequential steps, but in reality they are circular. Movement in one aspect often requires revisiting another one! This module explores how we do this, and what it means for us as historians. In Module 5 , we begin at last to think about how we communicate all of this to our audiences. Look at how one university lays out the expectations for digital history work (and, do you see how this ties back to ideas about paradata ?). We will think about things like typography and layout. We'll look at ways of making our visualizations more compelling, more effective. We will also learn how to create an online digital exhibit using Omeka . Finally, while there is no formal requirement for you to do this, it would be a great way of launching yourself as a digital historian to think about how you would formally reveal your project work in this class to the wider world: and then do it. There's a lot of noise on the internet, especially when it comes to history. How do you, as a digital historian, make the strongest possible signal? What you need to do this week Follow the instructions below to set up your digital history workspace. Annotate the course manual for any parts of it that are unclear (or alternatively, that have you excited). Respond to the readings and the reading questions by annotating the readings themselves - see the instructions below. Submit your work to the course submission form . Setting up a your workspace A digital historian needs to have a digital workshop/lab/studio/performance space. Such a space serves a number of functions: A scratch pad / fail log and code repository so that we remember what we were doing, or (more importantly) what we we did - that is to say, the actual commands we typed, the sequence of manipulations or data moves . A narrative that connects the dots, that explains the why of that what and how. You can use this narrative to point to when sharing your work with others. Digital history is not done in isolation or in a vaccuum. Sometimes, you will need to share a link to your work (often on twitter) asking, 'does anybody know why this isn't working?' or, 'does anybody know a better way of accomplishing this?', or, 'hey, I'm the first to do this!' A way of keeping notes on things we've read/come across on the web. There are a number of ways of accomplishing this. In this course, I will mandate one particular solution: Hypothes.is . When you're working with academic databases such as JSTOR, then you'll also need a bibliography manager. We don't go into this aspect very much in this course (if you take other courses with me, you will) but you might want to check out Zotero . It can sometimes be useful to make little videos of your work to explain when something isn't working - Screen-cast-o-matic is free and does a good job. Hypothes.is Hypothes.is is an overlay on the web that allows you to highlight or annotate any text you come across (including inside PDFs ). All of your annotations can then be collected together. It is a very effective research tool. Create an account with Hypothes.is . Get the Hypothes.is plugin for Google Chrome . If you don't have/use Google Chrome, go to the Hypothes.is start page and click on the 'Other browsers' link. Once you're logged into Hypothes.is, and you have the plugin installed, highlight THIS TEXT and leave an annotation! Who will be first? There are a few different kinds of annotations you can make, noted in this list with videos showing them . If you need step-by-step instructions for installing and using Hypothes.is, please visit the Hypothes.is help page and/or watch the video below: Annotations are public by default. When you are making a new annotation you can toggle the visibility so that they are private and thus visible only to you. You can also 'tag' any annotation you make. If many people use the same set of tags, you can collect annotations by tag. This can make it easier to do group research projects, for instance. Important information about annotating Please join our private HIST3814o annotation group. Remember to make all annotations to that group! Earlier iterations of this course used the \u2018public\u2019 group, and you can still see their annotations. However, we realized that it wasn\u2019t fair to the people whose work we were annotating to have year after year of our annotations littering their page. Remember then: make your annotations in our HIST3814o group ! GitHub Finally, you need a GitHub account. We will go into more detail about how to use GitHub in the next module. For now, go to GitHub.com and sign up for a new account. Follow all the prompts, and make a note of the direct URL to your account (mine for instance is http://github.com/shawngraham ). You'll learn how to use this space in Module 1, Exercise 3 . Fail Log Each week you will upload a fail log to you GitHub account. You will include a link to your 3 most important Hypothes.is annotations and reflect on how those 3 annotations were meaningful to your learning. In the first week you will also reflect on your worries for the course, any technical issues you encountered, etc. This week, you will fork the fail log example ('fork' is a GitHub term meaning to make a personal copy of a repository). To begin, follow the instructions below: Open GitHub . Login to your GitHub account. Navigate to the example fail log repository on GitHub . Follow the instructions for getting started with the example fail log . Your digital history lab/studio/workshop You now have a digital history lab equipped with all of the necessary ingredients for doing digital history. You have an open notebook for recording what you are up to (both your narrative and your annotations). In GitHub, you have a scratch pad \\ fail logfor keeping track of the actual nuts-and-bolts of any digital work you do (and note that it is entirely possible to do digital history successfully without having to do anything that a Computer Scientist for instance would call coding). You have a domain that you control completely, and where you can install other platforms and services as seems necessary. VPN DH Box To access our virtual computer, the DH Box, you will need to use Carleton's VPN service. Please visit Carleton's VPN help page and follow the instructions for your particular computer. Once you've got it installed, you will need to connect to Carleton through the VPN with your MyCarletonOne credentials. Indeed, you should always connect via a VPN whenever you're using a public wifi point (like in coffee shops). The VPN acts like a private tunnel from your computer to Carleton's servers. To the rest of the internet, it will now look as if you actually are on campus. Once you're connected via the VPN, you can access the DH Box through your browser . Bookmark the site; you'll use it in the exercises in Module 1. Using the DH Box Click the 'Sign up' button Fill in the form. Choose a username and password that you'll remember. You don't have to use a real email by the way, just something that looks email-like (this is handy if, like me, you end up creating multiple DH Boxes - it's a bad idea to have more than one DH Box with the same email address) Select the most time available (which will either be 1 or 2 months). Your personal DH Box will be created. Your username will now appear in the top right side of the purple bar. To enter the DH Box, click the username, select 'Apps'. A new tool ribbon appears below the purple bar. Most of what you will do in this course involves the 'Command line', 'RStudio', and 'File Manager'. Anytime the Command line or RStudio should ask for your username or password, you use the DH Box username and password you just created. A note on using the university computer labs: If you are using an official University computer lab computer to access DH Box, aspects of the University's security system might block the RStudio aspect. I am working on a solution to this problem. If you know that you are going to have to use Carleton computers, get in touch right away. Some Readings To Kick Things Off What is digital history anyway? How is it connected to so-called 'big data'? Read the following pieces. Annotate with Hypothes.is anything that strikes you as interesting using our HIST3814o group ; annotate anything that puzzles you - feel free to just say, 'I'm not sure what this means; does it mean.... does anybody have any ideas?' and if you see someone is asking questions, you can reply to that annotation with thoughts of your own! NB Each week, I expect you to respond to at least someone else's annotation in a substantive way. No \"I agree!\" or \"right on!\" or that sort of thing. Make a meaningful contribution. Once you have read and annotated the works, update your fail log in your GitHub repository . Explain why you're in this class, your level of comfort with digital tech, the kinds of history you're interested in, and what you hope to get out of this course. Your post should link to relevant annotations made by you or by your peers. (Every Hypothes.is annotation has a direct link visible when you click on the 'share' icon for an existing annotation). Readings Excerpts from Chapter 1, the Historian's Macroscope original draft ; read from 'Joys of Big Data' to 'Chapter One Conclusion'. Use Hypothes.is to annotate rather than the 'commenting' function on the site, but remember to annotate using our HIST3814o group . James Baker 'The soft digital history that underpins my book' https://cradledincaricature.com/2017/05/24/the-soft-digital-history-that-underpins-my-book/ James Baker 'The hard digital history that underpins my book' https://cradledincaricature.com/2017/06/06/the-hard-digital-history-that-underpins-my-book/ Jo Guldi and David Armitage, 'The History Manifesto: Chapter 4 ' Tim Hitchcock 'Big Data for Dead People' https://historyonics.blogspot.ca/2013/12/big-data-for-dead-people-digital.html 'On Diversity in Digital History', The Macroscope . Read and follow through the footnotes to at least two more articles. Acknowledgements The writing of this workbook took place alongside the writing of my more formal book on digital methods co-authored with the exceptional Ian Milligan and Scott Weingart . I learned far more about doing digital history from them than they ever from me, and someday, I hope to repay the debt. Other folks who've been instrumental in getting this workbook and course off the ground include Melodee Beals, John Bonnett, Chad Gaffield, Tamara Vaughan, the staff of the EDC at Carleton University, eCampusOntario and of course, the digital history community on Twitter. My thanks to you all. This class was first offered in the Winter 2015 semester at Carleton University in Ottawa Canada as HIST3907b. I am grateful to the participants in that class for the feedback and frank discussions of what worked and what didn't. To see the earlier version of the course, please feel free to browse its GitHub repository","title":"Getting Started"},{"location":"introduction/crafting-digital-history/#getting-yourself-ready-may-7-14-2018","text":"\"'Getting Ready for the Fire, Kasier Wilhelm' Bain Collection, Library of Congress hdl.loc.gov/loc.pnp/ggbain.10412 Call Number: LC-B2- 2400-7\" Welcome! This workbook is made by converting several plain-text files into a fully-operational website using the MkDocs static website generator . That means, you can keep a copy of all these files for your own records. Simply click on the 'Edit on GitHub' link at the top right. This will bring you to the repository that houses this workbook. Then, when you're signed into GitHub, you can fork (that is, make a copy) the repository into your own account. Why 'forking'? It seems an odd phrase. Think of it like this: Writing, crafting, coding: these are like a small river, flowing in one direction into the future. You get new ideas, new hunches: the river branches. There's a fork in its path. Sometimes new ideas, new hunches, fold back into that original stream: they merge. GitHub is a way of mapping that stream, and a guide to revisiting the interesting parts of it. It's not the best metaphor, but it'll do. No doubt you've had that experience where, after working on an essay for days, you make a change that you regret. You wish you could go back to fix it, but ctrl+z only goes so far. You realize that everything you've done for the last week needs to be thrown out, and you start over. Well, with 'versioning control', you can travel back upriver, back to where things were good. There are two tools that we will use to make this happen: git and GitHub . You'll learn more about making those things work in Module 1 . You'll see why you'd want to do that, and how to future-proof your work, writing things in a plain-text format called 'Markdown' . In Module 2 , we'll start exploring the wide variety of historical data out there. We'll talk about some of the ethical dilemmas posed by having so much data out there. Data are not neutral 'things given'; rather, they are capta : things taken . NB I am assuming, in this course, that the digital materials we want have already been digitized. Digitizing, adding meta-data (information that describes the data), and structuring it properly are very complex topics on their own that could be the subject of a complete course! If you are interested in those problems, a good place to start is this open course from the University of London's School of Advanced Study on database design for historians . In Module 3 , we'll see that data/capta are messy , and that they make a kind of illusory order that as historians, we are not normally in the habit of thinking about. The big secret of digital history is that the majority of your time on any digital history project won't be finding the capta, won't be analyzing the capta, won't be thinking about the historical meaning of the capta. We'll be cleaning it up . The exercises in this module will help you do that more efficiently (and be aware that 'efficiency' in computing terms is not a theoretically neutral idea!). In Module 4 , we talk about doing the analysis. I present finding, cleaning, and analyzing as if they were sequential steps, but in reality they are circular. Movement in one aspect often requires revisiting another one! This module explores how we do this, and what it means for us as historians. In Module 5 , we begin at last to think about how we communicate all of this to our audiences. Look at how one university lays out the expectations for digital history work (and, do you see how this ties back to ideas about paradata ?). We will think about things like typography and layout. We'll look at ways of making our visualizations more compelling, more effective. We will also learn how to create an online digital exhibit using Omeka . Finally, while there is no formal requirement for you to do this, it would be a great way of launching yourself as a digital historian to think about how you would formally reveal your project work in this class to the wider world: and then do it. There's a lot of noise on the internet, especially when it comes to history. How do you, as a digital historian, make the strongest possible signal?","title":"Getting yourself ready &mdash; May 7-14, 2018"},{"location":"introduction/crafting-digital-history/#what-you-need-to-do-this-week","text":"Follow the instructions below to set up your digital history workspace. Annotate the course manual for any parts of it that are unclear (or alternatively, that have you excited). Respond to the readings and the reading questions by annotating the readings themselves - see the instructions below. Submit your work to the course submission form .","title":"What you need to do this week"},{"location":"introduction/crafting-digital-history/#setting-up-a-your-workspace","text":"A digital historian needs to have a digital workshop/lab/studio/performance space. Such a space serves a number of functions: A scratch pad / fail log and code repository so that we remember what we were doing, or (more importantly) what we we did - that is to say, the actual commands we typed, the sequence of manipulations or data moves . A narrative that connects the dots, that explains the why of that what and how. You can use this narrative to point to when sharing your work with others. Digital history is not done in isolation or in a vaccuum. Sometimes, you will need to share a link to your work (often on twitter) asking, 'does anybody know why this isn't working?' or, 'does anybody know a better way of accomplishing this?', or, 'hey, I'm the first to do this!' A way of keeping notes on things we've read/come across on the web. There are a number of ways of accomplishing this. In this course, I will mandate one particular solution: Hypothes.is . When you're working with academic databases such as JSTOR, then you'll also need a bibliography manager. We don't go into this aspect very much in this course (if you take other courses with me, you will) but you might want to check out Zotero . It can sometimes be useful to make little videos of your work to explain when something isn't working - Screen-cast-o-matic is free and does a good job.","title":"Setting up a your workspace"},{"location":"introduction/crafting-digital-history/#hypothesis","text":"Hypothes.is is an overlay on the web that allows you to highlight or annotate any text you come across (including inside PDFs ). All of your annotations can then be collected together. It is a very effective research tool. Create an account with Hypothes.is . Get the Hypothes.is plugin for Google Chrome . If you don't have/use Google Chrome, go to the Hypothes.is start page and click on the 'Other browsers' link. Once you're logged into Hypothes.is, and you have the plugin installed, highlight THIS TEXT and leave an annotation! Who will be first? There are a few different kinds of annotations you can make, noted in this list with videos showing them . If you need step-by-step instructions for installing and using Hypothes.is, please visit the Hypothes.is help page and/or watch the video below: Annotations are public by default. When you are making a new annotation you can toggle the visibility so that they are private and thus visible only to you. You can also 'tag' any annotation you make. If many people use the same set of tags, you can collect annotations by tag. This can make it easier to do group research projects, for instance.","title":"Hypothes.is"},{"location":"introduction/crafting-digital-history/#important-information-about-annotating","text":"Please join our private HIST3814o annotation group. Remember to make all annotations to that group! Earlier iterations of this course used the \u2018public\u2019 group, and you can still see their annotations. However, we realized that it wasn\u2019t fair to the people whose work we were annotating to have year after year of our annotations littering their page. Remember then: make your annotations in our HIST3814o group !","title":"Important information about annotating"},{"location":"introduction/crafting-digital-history/#github","text":"Finally, you need a GitHub account. We will go into more detail about how to use GitHub in the next module. For now, go to GitHub.com and sign up for a new account. Follow all the prompts, and make a note of the direct URL to your account (mine for instance is http://github.com/shawngraham ). You'll learn how to use this space in Module 1, Exercise 3 .","title":"GitHub"},{"location":"introduction/crafting-digital-history/#fail-log","text":"Each week you will upload a fail log to you GitHub account. You will include a link to your 3 most important Hypothes.is annotations and reflect on how those 3 annotations were meaningful to your learning. In the first week you will also reflect on your worries for the course, any technical issues you encountered, etc. This week, you will fork the fail log example ('fork' is a GitHub term meaning to make a personal copy of a repository). To begin, follow the instructions below: Open GitHub . Login to your GitHub account. Navigate to the example fail log repository on GitHub . Follow the instructions for getting started with the example fail log .","title":"Fail Log"},{"location":"introduction/crafting-digital-history/#your-digital-history-labstudioworkshop","text":"You now have a digital history lab equipped with all of the necessary ingredients for doing digital history. You have an open notebook for recording what you are up to (both your narrative and your annotations). In GitHub, you have a scratch pad \\ fail logfor keeping track of the actual nuts-and-bolts of any digital work you do (and note that it is entirely possible to do digital history successfully without having to do anything that a Computer Scientist for instance would call coding). You have a domain that you control completely, and where you can install other platforms and services as seems necessary.","title":"Your digital history lab/studio/workshop"},{"location":"introduction/crafting-digital-history/#vpn-dh-box","text":"To access our virtual computer, the DH Box, you will need to use Carleton's VPN service. Please visit Carleton's VPN help page and follow the instructions for your particular computer. Once you've got it installed, you will need to connect to Carleton through the VPN with your MyCarletonOne credentials. Indeed, you should always connect via a VPN whenever you're using a public wifi point (like in coffee shops). The VPN acts like a private tunnel from your computer to Carleton's servers. To the rest of the internet, it will now look as if you actually are on campus. Once you're connected via the VPN, you can access the DH Box through your browser . Bookmark the site; you'll use it in the exercises in Module 1.","title":"VPN &amp; DH Box"},{"location":"introduction/crafting-digital-history/#using-the-dh-box","text":"Click the 'Sign up' button Fill in the form. Choose a username and password that you'll remember. You don't have to use a real email by the way, just something that looks email-like (this is handy if, like me, you end up creating multiple DH Boxes - it's a bad idea to have more than one DH Box with the same email address) Select the most time available (which will either be 1 or 2 months). Your personal DH Box will be created. Your username will now appear in the top right side of the purple bar. To enter the DH Box, click the username, select 'Apps'. A new tool ribbon appears below the purple bar. Most of what you will do in this course involves the 'Command line', 'RStudio', and 'File Manager'. Anytime the Command line or RStudio should ask for your username or password, you use the DH Box username and password you just created. A note on using the university computer labs: If you are using an official University computer lab computer to access DH Box, aspects of the University's security system might block the RStudio aspect. I am working on a solution to this problem. If you know that you are going to have to use Carleton computers, get in touch right away.","title":"Using the DH Box"},{"location":"introduction/crafting-digital-history/#some-readings-to-kick-things-off","text":"What is digital history anyway? How is it connected to so-called 'big data'? Read the following pieces. Annotate with Hypothes.is anything that strikes you as interesting using our HIST3814o group ; annotate anything that puzzles you - feel free to just say, 'I'm not sure what this means; does it mean.... does anybody have any ideas?' and if you see someone is asking questions, you can reply to that annotation with thoughts of your own! NB Each week, I expect you to respond to at least someone else's annotation in a substantive way. No \"I agree!\" or \"right on!\" or that sort of thing. Make a meaningful contribution. Once you have read and annotated the works, update your fail log in your GitHub repository . Explain why you're in this class, your level of comfort with digital tech, the kinds of history you're interested in, and what you hope to get out of this course. Your post should link to relevant annotations made by you or by your peers. (Every Hypothes.is annotation has a direct link visible when you click on the 'share' icon for an existing annotation).","title":"Some Readings To Kick Things Off"},{"location":"introduction/crafting-digital-history/#readings","text":"Excerpts from Chapter 1, the Historian's Macroscope original draft ; read from 'Joys of Big Data' to 'Chapter One Conclusion'. Use Hypothes.is to annotate rather than the 'commenting' function on the site, but remember to annotate using our HIST3814o group . James Baker 'The soft digital history that underpins my book' https://cradledincaricature.com/2017/05/24/the-soft-digital-history-that-underpins-my-book/ James Baker 'The hard digital history that underpins my book' https://cradledincaricature.com/2017/06/06/the-hard-digital-history-that-underpins-my-book/ Jo Guldi and David Armitage, 'The History Manifesto: Chapter 4 ' Tim Hitchcock 'Big Data for Dead People' https://historyonics.blogspot.ca/2013/12/big-data-for-dead-people-digital.html 'On Diversity in Digital History', The Macroscope . Read and follow through the footnotes to at least two more articles.","title":"Readings"},{"location":"introduction/crafting-digital-history/#acknowledgements","text":"The writing of this workbook took place alongside the writing of my more formal book on digital methods co-authored with the exceptional Ian Milligan and Scott Weingart . I learned far more about doing digital history from them than they ever from me, and someday, I hope to repay the debt. Other folks who've been instrumental in getting this workbook and course off the ground include Melodee Beals, John Bonnett, Chad Gaffield, Tamara Vaughan, the staff of the EDC at Carleton University, eCampusOntario and of course, the digital history community on Twitter. My thanks to you all. This class was first offered in the Winter 2015 semester at Carleton University in Ottawa Canada as HIST3907b. I am grateful to the participants in that class for the feedback and frank discussions of what worked and what didn't. To see the earlier version of the course, please feel free to browse its GitHub repository","title":"Acknowledgements"},{"location":"module-1/Exercises/","text":"Module 1 Exercises The exercises in this module are designed to give you the necessary skills to engage with the doing of digital history. To ensure success in the course, please do make it through Exercises 1 to 3. Exercise 4 is a bit more complex and not mission-critical. Push yourself if you can. The exercises in this module cover: Writing in Markdown Using the DH Box command line Converting files with the command line Setting up GitHub Interacting with GitHub from the command line These exercises walk you through the process of using our DH Box, and of keeping notes about what you're doing, and making those notes open on the web. If you run into trouble, ask for help in our Slack space. Annotate this page with where things are going wrong for you. Contact Dr. Graham. You do not have to suffer in silence! To ask for help when doing this work is not a sign of weakness, but of maturity. All 4 exercises are on this page. Remember to scroll! Exercise 1: Learning Markdown syntax with dillinger.io Have you ever fought with Word or another word processor, trying to get things just right ? Word processing is a mess. It conflates writing with typesetting and layout. Sometimes, you just want to get the words out. Othertimes, you want to make your writing as accessible as possible... but your intended recipient can't open your file, because they don't use the same word processor. Or perhaps you wrote up some great notes that you'd love to have in a slideshow; but you can't, because copying and pasting preserves a whole lot of extra gunk that messes up your materials. The answer is to separate your content from your tool . This is where the Markdown syntax shines. Markdown is a syntax for marking semantic elements within a document explicitly, not in some hidden layer. The idea is to identify units that are meaningful to humans, like titles, sections, subsections, footnotes, and illustrations. At the very least, your files will always remain comprehensible to you, even if the editor you are currently using stops working or \"goes out of business.\" Writing in this way liberates the author from the tool. Markdown can be written in any plain text editor and offers a rich ecosystem of software that can render that text into beautiful looking documents (incidentally, Hypothes.is annotations can be written in Markdown). For this reason, Markdown is currently enjoying a period of growth, not just as as means for writing scholarly papers but as a convention for online editing in general. Popular general purpose plain text editors include TextWrangler , Sublime , and Atom for Mac, Notepad++ for Windows, as well as Gedit and Kate for Linux. However, there are also editors that specialize in displaying and editing Markdown. NB A text editor is different from the default notepad app that comes with Windows or Mac. A text editor shows you exactly what is in a file, including tags, code, and other 'hidden' markup. In this exercise, I want you to become familiar with Markdown syntax. Check out Sarah Simpkin's quick primer on Markdown . There are a number of 'flavours' for Markdown, but in essence, they all mimic the kinds of conventions you would see in an email, using asterisks to indicate emphasis and so on. Check out the Markdown cheatsheet . Visit dillinger.io in a new browser window. This looks like a word processor. The left side of the screen is where you write, the right side shows you what your text will look like if you converted the text to HTML. Dillinger 'saves' your work in your browser's memory. You can also point it to save to your Dropbox, Google Drive, or GitHub account (under the cogwheel icon). Write a short 200-500 word piece on the most interesting annotation you've seen one of your classmates make. Why is it interesting? Why has it struck you? Grab at least two Creative Commons images and link outwards to four websites that are relevant to your piece. The Creative Commons license allows re-use. Do you know how to find Creative Commons images ? Make sure you link to the annotation in question. Make sure to add the file type .md at the end, in the 'document name' slot. Select 'Export to' Markdown to save a copy of the file in your downloads folder. Try 'exporting to' PDF or HTML. Since you've separated the content from the format, this illustrates how you can convert your text into other formats as necessary. (The text is converted using a piece of software called Pandoc ). See how easy that was? Don't worry about submitting this.... yet. NB The next time you go to dillinger.io the last document you were working on will load up. That's because dillinger stashes your work in the browser cache. If you clear your cache (from your browser's tools or settings) you'll lose it, which is why in step 7 I suggested exporting. For a richer discussion of some more ways Markdown and Pandoc can make your research sustainable, see Tenen and Wythoff, Sustainable Authorship in Plain Text Using Pandoc and Markdown . Exercise 2: Getting familiar with DH Box Setting up DH Box Many of the exercises in this workbook work without issue on a Mac or Linux computer, at the terminal. (On a Mac, you can find the Terminal under Applications Utilities). If you're on a Windows machine, it can be more difficult to get all of the various bits and pieces properly configured. If you're on a tablet, most digital history things you might like to do are not really feasible. One solution is for all of us to use the same computer. The CUNY Graduate Centre has created a digital-humanities focused virtual computer for just such an occasion, the DH Box . In this exercise, you are going to set up an instance of a DH Box for your own use. We will be using the command line interface which is an essential way to interact with your computer. Normally, when you're on your Mac or your PC, you're clicking on icons or menus or windows of various sorts to get things done. You're clicking and dragging text. All of these graphical elements sit on top of sequences of commands that you don't have to type out, that only the computer sees. The command line (i.e. terminal or command prompt) lets you dispense with the graphical elements, and type the commands you want directly. NB This workbook assumes that you are using a DH Box Carleton students: If you are on campus or are logged into Carleton's systems via a VPN, go to the sign up page at http://134.117.26.132:5000/signup . Other folks: Go to the DH Box sign up page at http://dhbox.org/signup . These are two separate installations of DH Box; whichever one you start with, continue to use. Select a username, password, and your email address. Your username must be four characters or longer. Select '1 month'. Then select launch. You now have a virtual computer that you can use for the next month. Whenever you come back to the DH Box, you can now click 'login' on your personal DH computer and your work will be there waiting for you. Once you've logged in, the site will reload to the DH Box welcome screen, but your user name will show at the top right. Click on your username, and there will be a new option, Apps. Click here to go into your DH Box. Inside the DH Box, you can click on: Home: tells you how many days until your DH Box expires. Keep an eye on this, as you'll want to get your materials out of DH Box before that happens. File Manager: allows you to view all of your files, as well as uploading/downloading materials from DH Box to your local computer. Command Line: allows you to interact with the computer at the terminal prompt or command line this is where you type in commands to your machine. RStudio: is an environment for doing statistical computing. Brackets: is a text editor for web development. Jupyter Notebooks: is an environment for creating documents that have running code inside of them. For HIST3814o, we will use the File Manager , the Command Line , and RStudio. Using the Command Line In the previous exercise, dillinger.io could convert your Markdown document into HTML or PDF. We will now add the Pandoc program to DH Box so that you can convert things for yourself. We will get the Pandoc program and bring it into our DH Box using the wget command. Wget is a program that allows us to download materials off the web. It can be used to only grab certain file types, or all files within a certain directory, or even to take a complete copy of a website! We'll discuss wget more in Module 2 (visit also the Programming Historian on wget ). Select Command Line in your DH Box. You will have to login again with your DH Box username and password. NB In many tutorials or how-tos, you will see the $ sign at the beginning of some text you are meant to type. This is a convention to show that what follows the $ is to be typed at the command prompt. For example, when you type or copy the command $ wget https://github.com/jgm/pandoc/releases/download/1.19.2.1/pandoc-1.19.2.1-1-amd64.deb you omit the $ and type or copy just the command wget https://github.com/jgm/pandoc/releases/download/1.19.2.1/pandoc-1.19.2.1-1-amd64.deb Type $ wget https://github.com/jgm/pandoc/releases/download/1.19.2.1/pandoc-1.19.2.1-1-amd64.deb This wget command gets a copy of the Pandoc program that will work in DH Box. The .deb file extension tells us that this is a Linux file. The next step is to unpack that file. Type $ sudo dpkg -i pandoc-1.19.2.1-1-amd64.deb Some commands can have nasty side effects, and the operating system won't let you do them unless you specify that you really want to do them. That's what the sudo command achieves. The next part, dpkg with the -i 'flag' is a 'package manager' for installing software. The last part is the file that we used wget to copy to our own machine. Type $ pandoc -v to test that Pandoc is installed If all has gone well, DH Box will tell you what version of Pandoc you have installed, who it was built by, and a copyright notice. Type $ history This creates a file to keep a record of what commands we have been typing. DH Box returns a list of every command that you've typed. Type $ history dhbox-work-today.md This will pipe that information into a new file. When you hit enter, the computer seems to pause for a moment, and then it shows you the command prompt again. How do we know if anything happened? Generally, when working at the command prompt, no news is good news. There are two ways you can check to see if the new file dhbox-work-today.md was created. You can click on the File Manager (the first folder will have your username; click on that, and then you'll see a list of files) and there it is! Click on the file name, and it will download to your computer where you can open it with a text editor. Alterntively, type $ ls This lists ls all the files in the directory. How do you know what directory you're in? You can use the pwd command, which prints the working directory. Let's take a look inside that new file you created. There is a text editor that you can use, called Nano . Type $ nano dhbox-work-today.md This opens the text editor, and you can see that the history command has copied its output into the editor. If you got an error: Sometimes, tools or commands we want to use are not present in the system. DH Box uses the Ubuntu operating system (an operating system is the underlying code that makes a PC a PC, a Mac a Mac, etc). We can install all sorts of useful things by using the apt-get command. To get and install Nano, try re-installing Nano with the following command: $ sudo apt-get install nano . The computer makes things a little more difficult sometimes when you're asking it to do something that changes or adds capabilities. Simply typing apt-get wouldn't work. sudo tells the computer that I really do want to execute the command (it stands for 'super-user do'). Any sudo command will make the computer ask for your password to confirm that you really do want to run that command. Read the beginner guide for the Nano text editor on How to Geek now . In Nano, with your file open, make a header (remember to use # to indicate a header) at the start of the file which has today's date in it, and add some text explaining what you're trying to do with this exercise. Hit ctrl+x to exit Nano. Nano will ask you if you want to 'Save modified buffer?'. Hit y , then when it asks you for the file name, hit enter. The entire key sequence for saving and exiting Nano is ctrl-x, y, enter . Use the File Manager to save a copy of dhbox-work-today.md onto your own computer. One last thing: let's convert the Markdown file into both Word and HTML. Pandoc is capable of quite sophisticated transformations, but these are two of the easiest. Type $ pandoc -o todayscommands.docx dhbox-work-today.md This says to Pandoc, create an output file ( the -o ) called todayscommands.docx from dhbox-work-today.md . Type $ ls after running this command to see if you've made the file. Any guesses how to create an HTML file? Pandoc is smart enough to know the kind of output you want from the file extension. Retype the command but use .html instead of .docx this time. Use the file manager to save copies of the .docx and .html files to your own machine. (Incidentally, if you use the arrow up and arrow down keys on your keyboard when you're at the command line, you can page through commands that you've previously typed). You've done some interesting work you've installed software into a remote computer, you've copied all of the commands you typed into a new file, you've used Markdown and a text editor to add information and context to those commands, and you've used Pandoc to transform your basic text into the more complicated formats of Word or HTML. As you do more work in this workbook, I want you to get in the habit of keeping these copies of what you've done. These are your actual lab notes, and they are invaluable for helping you keep track of what you've been doing and what you've been trying. If you remember the course manual, keeping a lab notebook is part of the assessment of the course. In the next exercise, we learn to use another piece of software called 'git' which will enable you to set up a remote location for pushing copies of your work to, for safe keeping and for collaboration. For more on why we go to all this trouble, visit the Programming Historian on 'Preserving Your Research Data' . Visit Ubuntu's handy cheat-sheet of keyboard shortcuts and other useful commands for your DH Box command-line work. Exercise 3: Setting up your GitHub space It's a familiar situation you've been working on a paper. It's where you want it to be, and you're certain you're done. You save it as final.doc . Then, you ask your friend to take a look at it. She spots several typos and that you flubbed an entire paragraph. You open it up, make the changes, and save as final-w-changes.doc . Later that day it occurs to you that you don't like those changes, and you go back to the original final.doc , make some changes, and just overwrite the previous version. Soon, you have a folder resembling the following: |-project |-'finalfinal.doc' |-'final-w-changes.doc' |-'final-w-changes2.doc' |-'isthisone-changes.doc' |-'this.doc' Things can get messy quite quickly. Imagine that you also have several spreadsheets in there as well, images, snippets of code... we don't want this. What we want is a way of managing the evolution of your files. We do this with a program called git . Git is not a user-friendly piece of software, and it takes some work to get your head around. Git is also very powerful, but fortunately, the basic uses to which most of us put it to are more or less straightforward. There are many other programs that make use of git for version control; these programs weld a graphical user interface on top of the main git program. For now, we'll content ourselves with the GitHub website , which does the same thing more or less, but from a browser. Firstly, let's define some terms. git is a program that keeps snapshots of the contents of a designated folder; it watches for 'difs' or differences between one snapshot and the next. You 'commit' these snapshots to a repository, which lives on your computer (it's just a folder). GitHub is a webservice that allows you to share those repositories, and to keep track of those versions online, and what's more, to share the repositories and files with multiple collaborators, and keep everyone's changes straight! There are other services that work like GitHub, such as Bitbucket . GitHub is just one of the better known services. You can browse GitHub repositories for code that solves a particular problem for you, software, and data. NB Because we are going to use the free version, we cannot make the repositories we create private. In this exercise, we're going to create a repository via the GitHub website and use it as a kind of back-up space for the files you created in the previous exercise. In the follow up exercise, you will learn how to do this from the command line. Go to GitHub and sign up for an account. Remember, you don't have to use your real name. If you use a pseudonym, please communicate to me privately what your account is called. Once you're logged in, we will create a new repository called hist3814o . Click on the + at the top right of the screen, beside your avatar image. Write a short description in the 'description box', and tick off the 'initialize the repository with a readme'. You can also select a license from the drop down box this will put some standard wording on your repository page about the conditions under which someone else might use (or cite) your code. Click 'Create repository'. At this point, you now have a folder a repository on the GitHub website into which you can deposit your files. It will be at http://github.com/your-account-name/hist3814o . So let's put some materials into that repository. Notice, when you're on your repository's page, that there is a button to 'create new file' and another for 'upload files'. Click on upload files. Drag the HTML file you created in the previous exercise (the one you made with Pandoc, and then saved to your computer via the DH Box File Manager) into the large grey box. This will upload the file. You can drag multiple files into the box to upload them into your repository. Enter a brief commit message in the commit message box. Then hit the green commit changes button. Git and GitHub attach messages to any 'commits' you make. These messages are brief notes explaining why you were making the commit. This way, if you ever had to roll back (go back to an earlier version) you can understand the evolution of the repository and find the spot you want. Instead of creating multiple versions of a file, you have a single file that has a version history. Neat, eh? This is perhaps the simplest use case for GitHub. You can create files directly in the repository as well, by hitting the 'create new file' button, and following the prompts. GitHub has a brief tutorial on using the website to collaborate with other people on a repository . For the remainder of the course, use your hist3814o repository as your scratch pad, your fail log, and your open notebook for showing your work across these modules . Check out my example 'fail log' as a model . 'Fail' is a pretty harsh word I use it to point out that for everything that works perfectly, there's an awful lot of trial-and-error that happened first upon which our successes are built. We need to keep track of this! James Baker calls this, de-wizardification . Some useful vocabulary when discussing git, GitHub, and version control: repository a single folder that holds all of the files and subfolders of your project. commit this means, 'take a snapshot of the current state of my repository'. publish take a folder on my computer, and copy it and its contents to the web as a repository at github.com/myusername/repositoryname . sync update the web repository with the latest commit from the folder on my computer. branch make a copy of my repository with a 'working name'. merge fold the changes I have made on a branch into another branch (typically, either master or gh-pages ). fork to make a copy of someone else's repo. clone to copy a repo online onto your own computer. pull request to ask the original maker of a repo to 'pull' your changes into their master, original, repository. push to move your changes from your computer to the online repo. An aside Many websites including this workbook use a GitHub repository as a way of hosting a website. The video below by historian Jack Dougherty shows how this could be done. Note that the HTML code that he pastes into an index.html file (the first page of any website is usually called index.html ) he got from a different service. You could write a document in Markdown, then use Pandoc to convert that into an index.html , for example. Exercise 4: A detailed look at using git on the command line At its heart, git is a way of taking 'snapshots' of the current state of a folder, and saving those snapshots in sequence. (For an excellent brief presentation on git, visit Alice Bartlett's presentation on the subject ; Bartlett is a senior developer for the Financial Times). In git's lingo, as stated earlier, a folder on your computer is known as a repository or repo. This sequence of snapshots in total lets you see how your project unfolded over time. Each time you wish to take a snapshot, you make a commit . A commit is a git command to take a snapshot of the entire repository. Thus, your folder we discussed above, with its proliferation of documents becomes: |-project |-'final.doc' But its commit history could be visualized like a string of pearls, where each pearl is a unique commit. Each one of those pearls represents a point in time when you the writer made a commit; git compared the state of the file to the earlier state, and saved a snapshot of the differences . What is particularly useful about making a commit is that git requires two more pieces of information about the git: who is making it, and when. The final useful bit about a commit is that you can save a detailed message about why the commit is being made. In our hypothetical situation, your first commit message might look like this: Fixed conclusion Julie pointed out that I had missed the critical bit in the assignment regarding stratigraphy. This was added in the concluding section. This information is stored in the history of the commits. In this way, you can see exactly how the project evolved and why. Each one of these commits has what is called a hash . This is a unique fingerprint that you can use to 'time travel' (in Bartlett's felicitous phrasing). If you want to see what your project looked like a few months ago, you checkout that commit. This has the effect of 'rewinding' the project. Once you've checked out a commit, don't be alarmed when you look at the folder: your folder (your repository) looks like how it once did all those weeks ago! Any files written after that commit seem as if they've disappeared. Don't worry, they still exist! What would happen if you wanted to experiment or take your project in a new direction from that point forward? Git lets you do this. What you will do is create a new branch of your project from that point. You can think of a branch as like the branch of a tree, or perhaps better, a branch of a river that eventually merges back to the source. (Another way of thinking about branches is that it is a label that sticks with these particular commits.) It is generally considered best practice to leave your master branch alone, in the sense that it represents the best version of your project. When you want to experiment or do something new, you create a branch and work there. If the work on the branch ultimately proves fruitless, you can discard it. But , if you decide that you like how it's going, you can merge that branch back into your master. A merge is a commit that folds all of the commits from the branch with the commits from the master. Git is also a powerful tool for backing up your work. You can work quite happily with git on your own machine, but when you store those files and the history of commits somewhere remote, you open up the possibility of collaboration and a safe place where your materials can be recalled if perish the thought something happened to your computer. In git-speak, the remote location is, well, the remote . There are many different places on the web that can function as a remote for git repositories. You can even set one up on your own server, if you want. To get material out of GitHub and onto your own computer, you clone it. If that hypothetical paper you were writing was part of a group project, your partners could clone it from your GitHub space, and work on it as well! Let us imagine a scenario.... You and Anna are working together on the project. You have made a new project repository in your GitHub space, and you have cloned it to your computer. Anna has cloned it to hers. Let's assume that you have a very productive weekend and you make some real headway on the project. You commit your changes, and then push them from your computer to the GitHub version of your repository. That repository is now one commit ahead of Anna's version. Anna pulls those changes from GitHub to her own version of the repository, which now looks exactly like your version. What happens if you make changes to the exact same part of the exact same file? This is called a conflict . Git will make a version of the file that contains text clearly marking off the part of the file where the conflict occurs, with the conflicting information marked out as well. The way to resolve the conflict is to open the file (typically with a text editor) and to delete the added git text, making a decision on which information is the correct information. Caution What follows might take a bit of time. It walks you through setting up a git repository in your DH Box; making changes to it; making different branches; and publishing the repository to your space on GitHub's website. 4.1. git init How do you turn a folder into a repository? With the git init command. At the command line (remember, the $ just shows you the prompt; you don't have to type it!): Type $ mkdir first-repo to make a new directory. Type $ ls (list) to see that the directory exists. Then change directory into it: cd first-repo . (Remember: if you're ever not sure what directory you're in, type $ pwd , or print working directory.) Make a new file called readme.md . You do this by calling the text editor: nano readme.md . Type an explanation of what this exercise is about. Hit ctrl+x to exit, then type y to save, leave the file name as it is. Hit enter. If you get an error to the effect that Nano is not found you just need to install it with $ sudo apt-get install nano . DH Box will ask you for your password again. Once the dust settles, you can make the new file with $ nano readme.md . Type $ ls again to check that the file is there. Type $ git init to tell the git program that this folder is to be tracked as a repository. If all goes correctly, you should see a variation on this message: Initialized empty Git repository in /home/demonstration/first-repo/.git/ . Type $ ls again. What do you (not) see? The changes in your repo will now be stored in that hidden directory, .git . Most of the time, you will never have reason to search that folder out. But know that the config file that describes your repo is in that folder. There might come a time in the future where you want to alter some of the default behaviour of the git program. You do that by opening the config file (which you can read with a text editor). Google 'show hidden files and folders' for your operating system when that time comes. 4.2. git status Open your readme.md file again with the Nano text editor, from the command line. Add some more information to it, then save and exit the text editor. Type $ git status Git will respond with a couple of pieces of information. It will tell you which branch you are on. It will list any untracked files present or new changes that are unstaged. We now will stage those changes to be added to our commit history by typing $ git add -A . (the bit that says -A adds any new, modified, or deleted files to your commit when you make it. There are other options or flags where you add only the new and modified files, or only the modified and deleted files.) Let's check our git status again: type $ git status You should see something like this: On branch master Initial commit Changes to be committed: (use \"git rm --cached file ...\" to unstage) new file: readme.md Let's take a snapshot: type $ git commit -m \"My first commit\" . This command represents a bit of a shortcut for making commit messages by using the -m flag to associate the text in the quotation marks with the commit. What happened? Remember, git keeps track not only of the changes, but who is making them. If this is your first time working with git in the DH Box, git will ask you for your name and email. Helpfully, the Git error message tells you exactly what to do: type $ git config --global user.email \"you\\@example.com\" and then type $ git config --global user.name \"Your Name\" . Now try making your first commit. Open up your readme.md file again, and add some more text to it. Save readme.md and exit the text editor. Add the new changes to the snapshot that we will take. Type $ git commit . Git automatically opens up the text editor so you can type a longer, more substantive commit message. In this message (unlike in Markdown) the # indicates a line to be ignored. You'll see that there is already some default text in there telling you what to do. Type a message indicating the nature of the changes you have made. Save and exit the text editor. DO NOT change the filename! Congratulations, you are now able to track your changes, and keep your materials under version control! 4.3. git merge Go ahead and make some more changes to your repository. Add some new files. Commit your changes after each new file is created. Now we're going to view the history of your commits. Type $ git log . What do you notice about this list of changes? Look at the time stamps. You'll see that the entries are listed in reverse chronological order. Each entry has its own 'hash' or unique ID, the person who made the commit and time are listed, as well as the commit message eg: commit 253506bc23070753c123accbe7c495af0e8b5a43 Author: Shawn Graham shawn.graham@carleton.ca Date: Tue Feb 14 18:42:31 2017 +0000 Fixed the headings that were broken in the about section of readme.md We're going to go back in time and create a new branch. You can escape the git log by typing q . Here's how the command will look: $ git checkout -b branchname commit where branch is the name you want the branch to be called, and commit is that unique ID. Make a new branch from your second last commit (don't use the or symbols). We typed git checkout -b experiment 253506bc23070753c123accbe7c495af0e8b5a43 . The response: Switched to a new branch 'experiment' Check git status and then list the contents of your repository. What do you see? You should notice that some of the files you had created before seem to have disappeared congratulations, you've time travelled! Those files are not missing; but they are on a different branch (the master branch) and you can't harm them now. Add a number of new files, making commits after each one. Check your git status, and check your git log as you go to make sure you're getting everything. Make sure there are no unstaged changes everything's been committed. 4.4. git merge continued Now let's assume that your experiment branch was successful everything you did there you were happy with and you want to integrate all of those changes back into your master branch. We're going to merge things. To merge, we have to go back to the master branch: $ git checkout master . (Good practice is to keep separate branches for all major experiments or directions you go. In case you lose track of the names of the branches you've created, this command: git branch -va will list them for you.) Now, we merge with $ git merge experiment . Remember, a merge is a special kind of commit that rolls all previous commits from both branches into one git will open your text editor and prompt you to add a message (it will have a default message already there if you want it). Save and exit and ta da! Your changes have been merged together. 4.5 git push One of the most powerful aspects of using git is the possibility of using it to manage collaborations. To do this, we have to make a copy of your repository available to others as a remote . There are a variety of places on the web where this can be done; one of the most popular at the moment is GitHub . GitHub allows a user to have an unlimited number of public repositories. Public repositories can be viewed and copied by anyone. Private repositories require a paid account, and access is controlled. If you are working on sensitive materials that can only be shared amongst the collaborators on a project, you should invest in an upgraded account (note that you can also control which files get included in commit; visit GitHub for help on ignoring files . In essence, you simply list the file names you do not want committed; visit GitHub for an example on listing ignored files ). Let's assume that your materials are not sensitive . Login to GitHub. On the upper right part of the screen there is a large + sign. Click on that + sign, and select new repository . On the following screen, give your repo a name. Leave the repository set to 'Public'. DO NOT 'initialize this repo with a readme.md'. Leave add .gitignore and add license set to NONE. Click the green 'Create Repository' button. You now have a space into which you will publish the repository on your machine. At the command line, we now need to tell git the location of this space. We do that with the following command, where you will change your-username and your-new-repo appropriately: $ git remote add origin https://github.com/YOUR-USERNAME/YOUR-NEW-REPO.git Now we push your local copy of the repository onto the web, to the GitHub version of your repo: $ git push -u origin master NB If you wanted to push a branch to your repository on the web instead, do you see how you would do that? If your branch was called experiment , the command would look like this: $ git push origin experiment The changes can sometimes take a few minutes to show up on the website. Now, the next time you make changes to this repository, you can push them to your GitHub account which is the 'origin' in the command above. Add a new text file. Commit the changes. Push the changes to your account. 4.6. git clone Imagine you are collaborating with one of your classmates. Your classmate is in charge of the project, and is keeping track of the 'official' folder of materials (i.e. the repo). You wish to make some changes to the files in that repository. You can manage that collaboration via GitHub by making a copy, what GitHub calls a fork . Make sure you're logged into your GitHub account on the GitHub website. We're going to fork an example repository right now by going to GitHub's forking example page . Click the 'fork' button at top-right. GitHub now makes a copy of the repository in your own GitHub account! To make a copy of that repository on your own machine, you will now clone it with the git clone command. (Remember: a 'fork' copies someone's GitHub repo into a repo in your OWN GitHub account; a 'clone' makes a copy on your own MACHINE). Type: $ cd.. $ pwd We do that to make sure you're not inside any other repo you've made! Make sure you're not inside the repository we used in Exercises 1 to 4, then proceed: $ git clone https://github.com/YOUR-USERNAME/Spoon-Knife.git $ ls You now have a folder called 'Spoon-Knife' on your machine! Any changes you make inside that folder can be tracked with commits. You can also git push -u origin master when you're inside it, and the changes will show up on your OWN copy (your fork) on github.com . Make a fork of, and then clone, one of your classmates' repositories. Create a new branch. Add a new file to the repository on your machine, and then push it to your fork on GitHub. Remember, your new file will appear on the new branch you created, NOT the master branch. 4.7. pull request Now, you let your collaborator know that you've made a change that you want her to merge into the original repository. You do this by issuing a pull request . But first, we have to tell git to keep an eye on that original repository, which we will call upstream . You do this by adding that repository's location like so: Type (but change the address appropriately): $ git remote add upstream THE-FULL-URL-TO-THEIR-REPO-ENDING-WITH-.git You can keep your version of the remote up-to-date by fetching any new changes your classmate has done: $ git fetch upstream Now let's make a pull request (you might want to bookmark GitHub's help document for pull requests ). Go to your copy of your classmate's repository at your GitHub account. Make sure you've selected the correct branch you pushed your changes to, by selecting it from the Branches menu drop down list. Click the 'new pull request' button. The new page that appears can be confusing, but it is trying to double check with you which changes you want to make, and where. Make sure these are set properly. Base branch is the branch where you want your changes to go (ie. your classmate's repository). Head branch is the branch where you made your changes. Remember: the Base branch is the TO , the Head branch is the FROM the place where you want your changes to go TO , FROM the place where you made the changes. For example, say I clone Dr. Graham's R repository and create a new experiment branch: If I make changes to my experiment branch that is the Head branch. Then I push those changes to Dr. Graham's master branch that is the Base branch. A pull request has to have a message attached to it, so that your classmate knows what kind of change you're proposing. Fill in the message fields appropriately, then hit the 'create pull request' button. 4.8. git merge again Finally, the last bit of work to be done is to accept the pull request and merge the changes into the original repository. Go to your repository on your GitHub account. Check to see if there are any 'pull requests' these will be listed under the 'pull requests' tab. Click on that tab. You can merge from the command line, but for now, you can simply click on the green 'merge pull request' button, and then the 'confirm merge' button. The changes your classmate has made have now been folded into your repository. To get the updates on your local machine, go back to the command line and type $ git pull origin master Phew. You might want to do $ history recentcommands.md just to remember what you've done. And then commit that to a repository. Conclusion In the modules to come, we will be using DH Box as we find data, fetch data, wrangle data, analyze data, and visualize data. It becomes very important that you note the kinds of commands you use or try, the thinking that you were doing at that point, and so on. You want to leave yourself (and anybody who comes after) breadcrumbs so that you understand what you were doing. Quick notes written in Markdown, piping the history of what you've done to a file, and keeping those files in a repository alongside any other code or files that you may make will set you on the path to open access research and computational reproducibility. As Martha might say, 'and that's a good thing'.","title":"Exercises"},{"location":"module-1/Exercises/#module-1-exercises","text":"The exercises in this module are designed to give you the necessary skills to engage with the doing of digital history. To ensure success in the course, please do make it through Exercises 1 to 3. Exercise 4 is a bit more complex and not mission-critical. Push yourself if you can. The exercises in this module cover: Writing in Markdown Using the DH Box command line Converting files with the command line Setting up GitHub Interacting with GitHub from the command line These exercises walk you through the process of using our DH Box, and of keeping notes about what you're doing, and making those notes open on the web. If you run into trouble, ask for help in our Slack space. Annotate this page with where things are going wrong for you. Contact Dr. Graham. You do not have to suffer in silence! To ask for help when doing this work is not a sign of weakness, but of maturity. All 4 exercises are on this page. Remember to scroll!","title":"Module 1 Exercises"},{"location":"module-1/Exercises/#exercise-1-learning-markdown-syntax-with-dillingerio","text":"Have you ever fought with Word or another word processor, trying to get things just right ? Word processing is a mess. It conflates writing with typesetting and layout. Sometimes, you just want to get the words out. Othertimes, you want to make your writing as accessible as possible... but your intended recipient can't open your file, because they don't use the same word processor. Or perhaps you wrote up some great notes that you'd love to have in a slideshow; but you can't, because copying and pasting preserves a whole lot of extra gunk that messes up your materials. The answer is to separate your content from your tool . This is where the Markdown syntax shines. Markdown is a syntax for marking semantic elements within a document explicitly, not in some hidden layer. The idea is to identify units that are meaningful to humans, like titles, sections, subsections, footnotes, and illustrations. At the very least, your files will always remain comprehensible to you, even if the editor you are currently using stops working or \"goes out of business.\" Writing in this way liberates the author from the tool. Markdown can be written in any plain text editor and offers a rich ecosystem of software that can render that text into beautiful looking documents (incidentally, Hypothes.is annotations can be written in Markdown). For this reason, Markdown is currently enjoying a period of growth, not just as as means for writing scholarly papers but as a convention for online editing in general. Popular general purpose plain text editors include TextWrangler , Sublime , and Atom for Mac, Notepad++ for Windows, as well as Gedit and Kate for Linux. However, there are also editors that specialize in displaying and editing Markdown. NB A text editor is different from the default notepad app that comes with Windows or Mac. A text editor shows you exactly what is in a file, including tags, code, and other 'hidden' markup. In this exercise, I want you to become familiar with Markdown syntax. Check out Sarah Simpkin's quick primer on Markdown . There are a number of 'flavours' for Markdown, but in essence, they all mimic the kinds of conventions you would see in an email, using asterisks to indicate emphasis and so on. Check out the Markdown cheatsheet . Visit dillinger.io in a new browser window. This looks like a word processor. The left side of the screen is where you write, the right side shows you what your text will look like if you converted the text to HTML. Dillinger 'saves' your work in your browser's memory. You can also point it to save to your Dropbox, Google Drive, or GitHub account (under the cogwheel icon). Write a short 200-500 word piece on the most interesting annotation you've seen one of your classmates make. Why is it interesting? Why has it struck you? Grab at least two Creative Commons images and link outwards to four websites that are relevant to your piece. The Creative Commons license allows re-use. Do you know how to find Creative Commons images ? Make sure you link to the annotation in question. Make sure to add the file type .md at the end, in the 'document name' slot. Select 'Export to' Markdown to save a copy of the file in your downloads folder. Try 'exporting to' PDF or HTML. Since you've separated the content from the format, this illustrates how you can convert your text into other formats as necessary. (The text is converted using a piece of software called Pandoc ). See how easy that was? Don't worry about submitting this.... yet. NB The next time you go to dillinger.io the last document you were working on will load up. That's because dillinger stashes your work in the browser cache. If you clear your cache (from your browser's tools or settings) you'll lose it, which is why in step 7 I suggested exporting. For a richer discussion of some more ways Markdown and Pandoc can make your research sustainable, see Tenen and Wythoff, Sustainable Authorship in Plain Text Using Pandoc and Markdown .","title":"Exercise 1: Learning Markdown syntax with dillinger.io"},{"location":"module-1/Exercises/#exercise-2-getting-familiar-with-dh-box","text":"","title":"Exercise 2: Getting familiar with DH Box"},{"location":"module-1/Exercises/#setting-up-dh-box","text":"Many of the exercises in this workbook work without issue on a Mac or Linux computer, at the terminal. (On a Mac, you can find the Terminal under Applications Utilities). If you're on a Windows machine, it can be more difficult to get all of the various bits and pieces properly configured. If you're on a tablet, most digital history things you might like to do are not really feasible. One solution is for all of us to use the same computer. The CUNY Graduate Centre has created a digital-humanities focused virtual computer for just such an occasion, the DH Box . In this exercise, you are going to set up an instance of a DH Box for your own use. We will be using the command line interface which is an essential way to interact with your computer. Normally, when you're on your Mac or your PC, you're clicking on icons or menus or windows of various sorts to get things done. You're clicking and dragging text. All of these graphical elements sit on top of sequences of commands that you don't have to type out, that only the computer sees. The command line (i.e. terminal or command prompt) lets you dispense with the graphical elements, and type the commands you want directly. NB This workbook assumes that you are using a DH Box Carleton students: If you are on campus or are logged into Carleton's systems via a VPN, go to the sign up page at http://134.117.26.132:5000/signup . Other folks: Go to the DH Box sign up page at http://dhbox.org/signup . These are two separate installations of DH Box; whichever one you start with, continue to use. Select a username, password, and your email address. Your username must be four characters or longer. Select '1 month'. Then select launch. You now have a virtual computer that you can use for the next month. Whenever you come back to the DH Box, you can now click 'login' on your personal DH computer and your work will be there waiting for you. Once you've logged in, the site will reload to the DH Box welcome screen, but your user name will show at the top right. Click on your username, and there will be a new option, Apps. Click here to go into your DH Box. Inside the DH Box, you can click on: Home: tells you how many days until your DH Box expires. Keep an eye on this, as you'll want to get your materials out of DH Box before that happens. File Manager: allows you to view all of your files, as well as uploading/downloading materials from DH Box to your local computer. Command Line: allows you to interact with the computer at the terminal prompt or command line this is where you type in commands to your machine. RStudio: is an environment for doing statistical computing. Brackets: is a text editor for web development. Jupyter Notebooks: is an environment for creating documents that have running code inside of them. For HIST3814o, we will use the File Manager , the Command Line , and RStudio.","title":"Setting up DH Box"},{"location":"module-1/Exercises/#using-the-command-line","text":"In the previous exercise, dillinger.io could convert your Markdown document into HTML or PDF. We will now add the Pandoc program to DH Box so that you can convert things for yourself. We will get the Pandoc program and bring it into our DH Box using the wget command. Wget is a program that allows us to download materials off the web. It can be used to only grab certain file types, or all files within a certain directory, or even to take a complete copy of a website! We'll discuss wget more in Module 2 (visit also the Programming Historian on wget ). Select Command Line in your DH Box. You will have to login again with your DH Box username and password. NB In many tutorials or how-tos, you will see the $ sign at the beginning of some text you are meant to type. This is a convention to show that what follows the $ is to be typed at the command prompt. For example, when you type or copy the command $ wget https://github.com/jgm/pandoc/releases/download/1.19.2.1/pandoc-1.19.2.1-1-amd64.deb you omit the $ and type or copy just the command wget https://github.com/jgm/pandoc/releases/download/1.19.2.1/pandoc-1.19.2.1-1-amd64.deb Type $ wget https://github.com/jgm/pandoc/releases/download/1.19.2.1/pandoc-1.19.2.1-1-amd64.deb This wget command gets a copy of the Pandoc program that will work in DH Box. The .deb file extension tells us that this is a Linux file. The next step is to unpack that file. Type $ sudo dpkg -i pandoc-1.19.2.1-1-amd64.deb Some commands can have nasty side effects, and the operating system won't let you do them unless you specify that you really want to do them. That's what the sudo command achieves. The next part, dpkg with the -i 'flag' is a 'package manager' for installing software. The last part is the file that we used wget to copy to our own machine. Type $ pandoc -v to test that Pandoc is installed If all has gone well, DH Box will tell you what version of Pandoc you have installed, who it was built by, and a copyright notice. Type $ history This creates a file to keep a record of what commands we have been typing. DH Box returns a list of every command that you've typed. Type $ history dhbox-work-today.md This will pipe that information into a new file. When you hit enter, the computer seems to pause for a moment, and then it shows you the command prompt again. How do we know if anything happened? Generally, when working at the command prompt, no news is good news. There are two ways you can check to see if the new file dhbox-work-today.md was created. You can click on the File Manager (the first folder will have your username; click on that, and then you'll see a list of files) and there it is! Click on the file name, and it will download to your computer where you can open it with a text editor. Alterntively, type $ ls This lists ls all the files in the directory. How do you know what directory you're in? You can use the pwd command, which prints the working directory. Let's take a look inside that new file you created. There is a text editor that you can use, called Nano . Type $ nano dhbox-work-today.md This opens the text editor, and you can see that the history command has copied its output into the editor. If you got an error: Sometimes, tools or commands we want to use are not present in the system. DH Box uses the Ubuntu operating system (an operating system is the underlying code that makes a PC a PC, a Mac a Mac, etc). We can install all sorts of useful things by using the apt-get command. To get and install Nano, try re-installing Nano with the following command: $ sudo apt-get install nano . The computer makes things a little more difficult sometimes when you're asking it to do something that changes or adds capabilities. Simply typing apt-get wouldn't work. sudo tells the computer that I really do want to execute the command (it stands for 'super-user do'). Any sudo command will make the computer ask for your password to confirm that you really do want to run that command. Read the beginner guide for the Nano text editor on How to Geek now . In Nano, with your file open, make a header (remember to use # to indicate a header) at the start of the file which has today's date in it, and add some text explaining what you're trying to do with this exercise. Hit ctrl+x to exit Nano. Nano will ask you if you want to 'Save modified buffer?'. Hit y , then when it asks you for the file name, hit enter. The entire key sequence for saving and exiting Nano is ctrl-x, y, enter . Use the File Manager to save a copy of dhbox-work-today.md onto your own computer. One last thing: let's convert the Markdown file into both Word and HTML. Pandoc is capable of quite sophisticated transformations, but these are two of the easiest. Type $ pandoc -o todayscommands.docx dhbox-work-today.md This says to Pandoc, create an output file ( the -o ) called todayscommands.docx from dhbox-work-today.md . Type $ ls after running this command to see if you've made the file. Any guesses how to create an HTML file? Pandoc is smart enough to know the kind of output you want from the file extension. Retype the command but use .html instead of .docx this time. Use the file manager to save copies of the .docx and .html files to your own machine. (Incidentally, if you use the arrow up and arrow down keys on your keyboard when you're at the command line, you can page through commands that you've previously typed). You've done some interesting work you've installed software into a remote computer, you've copied all of the commands you typed into a new file, you've used Markdown and a text editor to add information and context to those commands, and you've used Pandoc to transform your basic text into the more complicated formats of Word or HTML. As you do more work in this workbook, I want you to get in the habit of keeping these copies of what you've done. These are your actual lab notes, and they are invaluable for helping you keep track of what you've been doing and what you've been trying. If you remember the course manual, keeping a lab notebook is part of the assessment of the course. In the next exercise, we learn to use another piece of software called 'git' which will enable you to set up a remote location for pushing copies of your work to, for safe keeping and for collaboration. For more on why we go to all this trouble, visit the Programming Historian on 'Preserving Your Research Data' . Visit Ubuntu's handy cheat-sheet of keyboard shortcuts and other useful commands for your DH Box command-line work.","title":"Using the Command Line"},{"location":"module-1/Exercises/#exercise-3-setting-up-your-github-space","text":"It's a familiar situation you've been working on a paper. It's where you want it to be, and you're certain you're done. You save it as final.doc . Then, you ask your friend to take a look at it. She spots several typos and that you flubbed an entire paragraph. You open it up, make the changes, and save as final-w-changes.doc . Later that day it occurs to you that you don't like those changes, and you go back to the original final.doc , make some changes, and just overwrite the previous version. Soon, you have a folder resembling the following: |-project |-'finalfinal.doc' |-'final-w-changes.doc' |-'final-w-changes2.doc' |-'isthisone-changes.doc' |-'this.doc' Things can get messy quite quickly. Imagine that you also have several spreadsheets in there as well, images, snippets of code... we don't want this. What we want is a way of managing the evolution of your files. We do this with a program called git . Git is not a user-friendly piece of software, and it takes some work to get your head around. Git is also very powerful, but fortunately, the basic uses to which most of us put it to are more or less straightforward. There are many other programs that make use of git for version control; these programs weld a graphical user interface on top of the main git program. For now, we'll content ourselves with the GitHub website , which does the same thing more or less, but from a browser. Firstly, let's define some terms. git is a program that keeps snapshots of the contents of a designated folder; it watches for 'difs' or differences between one snapshot and the next. You 'commit' these snapshots to a repository, which lives on your computer (it's just a folder). GitHub is a webservice that allows you to share those repositories, and to keep track of those versions online, and what's more, to share the repositories and files with multiple collaborators, and keep everyone's changes straight! There are other services that work like GitHub, such as Bitbucket . GitHub is just one of the better known services. You can browse GitHub repositories for code that solves a particular problem for you, software, and data. NB Because we are going to use the free version, we cannot make the repositories we create private. In this exercise, we're going to create a repository via the GitHub website and use it as a kind of back-up space for the files you created in the previous exercise. In the follow up exercise, you will learn how to do this from the command line. Go to GitHub and sign up for an account. Remember, you don't have to use your real name. If you use a pseudonym, please communicate to me privately what your account is called. Once you're logged in, we will create a new repository called hist3814o . Click on the + at the top right of the screen, beside your avatar image. Write a short description in the 'description box', and tick off the 'initialize the repository with a readme'. You can also select a license from the drop down box this will put some standard wording on your repository page about the conditions under which someone else might use (or cite) your code. Click 'Create repository'. At this point, you now have a folder a repository on the GitHub website into which you can deposit your files. It will be at http://github.com/your-account-name/hist3814o . So let's put some materials into that repository. Notice, when you're on your repository's page, that there is a button to 'create new file' and another for 'upload files'. Click on upload files. Drag the HTML file you created in the previous exercise (the one you made with Pandoc, and then saved to your computer via the DH Box File Manager) into the large grey box. This will upload the file. You can drag multiple files into the box to upload them into your repository. Enter a brief commit message in the commit message box. Then hit the green commit changes button. Git and GitHub attach messages to any 'commits' you make. These messages are brief notes explaining why you were making the commit. This way, if you ever had to roll back (go back to an earlier version) you can understand the evolution of the repository and find the spot you want. Instead of creating multiple versions of a file, you have a single file that has a version history. Neat, eh? This is perhaps the simplest use case for GitHub. You can create files directly in the repository as well, by hitting the 'create new file' button, and following the prompts. GitHub has a brief tutorial on using the website to collaborate with other people on a repository . For the remainder of the course, use your hist3814o repository as your scratch pad, your fail log, and your open notebook for showing your work across these modules . Check out my example 'fail log' as a model . 'Fail' is a pretty harsh word I use it to point out that for everything that works perfectly, there's an awful lot of trial-and-error that happened first upon which our successes are built. We need to keep track of this! James Baker calls this, de-wizardification . Some useful vocabulary when discussing git, GitHub, and version control: repository a single folder that holds all of the files and subfolders of your project. commit this means, 'take a snapshot of the current state of my repository'. publish take a folder on my computer, and copy it and its contents to the web as a repository at github.com/myusername/repositoryname . sync update the web repository with the latest commit from the folder on my computer. branch make a copy of my repository with a 'working name'. merge fold the changes I have made on a branch into another branch (typically, either master or gh-pages ). fork to make a copy of someone else's repo. clone to copy a repo online onto your own computer. pull request to ask the original maker of a repo to 'pull' your changes into their master, original, repository. push to move your changes from your computer to the online repo.","title":"Exercise 3: Setting up your GitHub space"},{"location":"module-1/Exercises/#an-aside","text":"Many websites including this workbook use a GitHub repository as a way of hosting a website. The video below by historian Jack Dougherty shows how this could be done. Note that the HTML code that he pastes into an index.html file (the first page of any website is usually called index.html ) he got from a different service. You could write a document in Markdown, then use Pandoc to convert that into an index.html , for example.","title":"An aside"},{"location":"module-1/Exercises/#exercise-4-a-detailed-look-at-using-git-on-the-command-line","text":"At its heart, git is a way of taking 'snapshots' of the current state of a folder, and saving those snapshots in sequence. (For an excellent brief presentation on git, visit Alice Bartlett's presentation on the subject ; Bartlett is a senior developer for the Financial Times). In git's lingo, as stated earlier, a folder on your computer is known as a repository or repo. This sequence of snapshots in total lets you see how your project unfolded over time. Each time you wish to take a snapshot, you make a commit . A commit is a git command to take a snapshot of the entire repository. Thus, your folder we discussed above, with its proliferation of documents becomes: |-project |-'final.doc' But its commit history could be visualized like a string of pearls, where each pearl is a unique commit. Each one of those pearls represents a point in time when you the writer made a commit; git compared the state of the file to the earlier state, and saved a snapshot of the differences . What is particularly useful about making a commit is that git requires two more pieces of information about the git: who is making it, and when. The final useful bit about a commit is that you can save a detailed message about why the commit is being made. In our hypothetical situation, your first commit message might look like this: Fixed conclusion Julie pointed out that I had missed the critical bit in the assignment regarding stratigraphy. This was added in the concluding section. This information is stored in the history of the commits. In this way, you can see exactly how the project evolved and why. Each one of these commits has what is called a hash . This is a unique fingerprint that you can use to 'time travel' (in Bartlett's felicitous phrasing). If you want to see what your project looked like a few months ago, you checkout that commit. This has the effect of 'rewinding' the project. Once you've checked out a commit, don't be alarmed when you look at the folder: your folder (your repository) looks like how it once did all those weeks ago! Any files written after that commit seem as if they've disappeared. Don't worry, they still exist! What would happen if you wanted to experiment or take your project in a new direction from that point forward? Git lets you do this. What you will do is create a new branch of your project from that point. You can think of a branch as like the branch of a tree, or perhaps better, a branch of a river that eventually merges back to the source. (Another way of thinking about branches is that it is a label that sticks with these particular commits.) It is generally considered best practice to leave your master branch alone, in the sense that it represents the best version of your project. When you want to experiment or do something new, you create a branch and work there. If the work on the branch ultimately proves fruitless, you can discard it. But , if you decide that you like how it's going, you can merge that branch back into your master. A merge is a commit that folds all of the commits from the branch with the commits from the master. Git is also a powerful tool for backing up your work. You can work quite happily with git on your own machine, but when you store those files and the history of commits somewhere remote, you open up the possibility of collaboration and a safe place where your materials can be recalled if perish the thought something happened to your computer. In git-speak, the remote location is, well, the remote . There are many different places on the web that can function as a remote for git repositories. You can even set one up on your own server, if you want. To get material out of GitHub and onto your own computer, you clone it. If that hypothetical paper you were writing was part of a group project, your partners could clone it from your GitHub space, and work on it as well! Let us imagine a scenario.... You and Anna are working together on the project. You have made a new project repository in your GitHub space, and you have cloned it to your computer. Anna has cloned it to hers. Let's assume that you have a very productive weekend and you make some real headway on the project. You commit your changes, and then push them from your computer to the GitHub version of your repository. That repository is now one commit ahead of Anna's version. Anna pulls those changes from GitHub to her own version of the repository, which now looks exactly like your version. What happens if you make changes to the exact same part of the exact same file? This is called a conflict . Git will make a version of the file that contains text clearly marking off the part of the file where the conflict occurs, with the conflicting information marked out as well. The way to resolve the conflict is to open the file (typically with a text editor) and to delete the added git text, making a decision on which information is the correct information. Caution What follows might take a bit of time. It walks you through setting up a git repository in your DH Box; making changes to it; making different branches; and publishing the repository to your space on GitHub's website.","title":"Exercise 4: A detailed look at using git on the command line"},{"location":"module-1/Exercises/#41-git-init","text":"How do you turn a folder into a repository? With the git init command. At the command line (remember, the $ just shows you the prompt; you don't have to type it!): Type $ mkdir first-repo to make a new directory. Type $ ls (list) to see that the directory exists. Then change directory into it: cd first-repo . (Remember: if you're ever not sure what directory you're in, type $ pwd , or print working directory.) Make a new file called readme.md . You do this by calling the text editor: nano readme.md . Type an explanation of what this exercise is about. Hit ctrl+x to exit, then type y to save, leave the file name as it is. Hit enter. If you get an error to the effect that Nano is not found you just need to install it with $ sudo apt-get install nano . DH Box will ask you for your password again. Once the dust settles, you can make the new file with $ nano readme.md . Type $ ls again to check that the file is there. Type $ git init to tell the git program that this folder is to be tracked as a repository. If all goes correctly, you should see a variation on this message: Initialized empty Git repository in /home/demonstration/first-repo/.git/ . Type $ ls again. What do you (not) see? The changes in your repo will now be stored in that hidden directory, .git . Most of the time, you will never have reason to search that folder out. But know that the config file that describes your repo is in that folder. There might come a time in the future where you want to alter some of the default behaviour of the git program. You do that by opening the config file (which you can read with a text editor). Google 'show hidden files and folders' for your operating system when that time comes.","title":"4.1. git init"},{"location":"module-1/Exercises/#42-git-status","text":"Open your readme.md file again with the Nano text editor, from the command line. Add some more information to it, then save and exit the text editor. Type $ git status Git will respond with a couple of pieces of information. It will tell you which branch you are on. It will list any untracked files present or new changes that are unstaged. We now will stage those changes to be added to our commit history by typing $ git add -A . (the bit that says -A adds any new, modified, or deleted files to your commit when you make it. There are other options or flags where you add only the new and modified files, or only the modified and deleted files.) Let's check our git status again: type $ git status You should see something like this: On branch master Initial commit Changes to be committed: (use \"git rm --cached file ...\" to unstage) new file: readme.md Let's take a snapshot: type $ git commit -m \"My first commit\" . This command represents a bit of a shortcut for making commit messages by using the -m flag to associate the text in the quotation marks with the commit. What happened? Remember, git keeps track not only of the changes, but who is making them. If this is your first time working with git in the DH Box, git will ask you for your name and email. Helpfully, the Git error message tells you exactly what to do: type $ git config --global user.email \"you\\@example.com\" and then type $ git config --global user.name \"Your Name\" . Now try making your first commit. Open up your readme.md file again, and add some more text to it. Save readme.md and exit the text editor. Add the new changes to the snapshot that we will take. Type $ git commit . Git automatically opens up the text editor so you can type a longer, more substantive commit message. In this message (unlike in Markdown) the # indicates a line to be ignored. You'll see that there is already some default text in there telling you what to do. Type a message indicating the nature of the changes you have made. Save and exit the text editor. DO NOT change the filename! Congratulations, you are now able to track your changes, and keep your materials under version control!","title":"4.2. git status"},{"location":"module-1/Exercises/#43-git-merge","text":"Go ahead and make some more changes to your repository. Add some new files. Commit your changes after each new file is created. Now we're going to view the history of your commits. Type $ git log . What do you notice about this list of changes? Look at the time stamps. You'll see that the entries are listed in reverse chronological order. Each entry has its own 'hash' or unique ID, the person who made the commit and time are listed, as well as the commit message eg: commit 253506bc23070753c123accbe7c495af0e8b5a43 Author: Shawn Graham shawn.graham@carleton.ca Date: Tue Feb 14 18:42:31 2017 +0000 Fixed the headings that were broken in the about section of readme.md We're going to go back in time and create a new branch. You can escape the git log by typing q . Here's how the command will look: $ git checkout -b branchname commit where branch is the name you want the branch to be called, and commit is that unique ID. Make a new branch from your second last commit (don't use the or symbols). We typed git checkout -b experiment 253506bc23070753c123accbe7c495af0e8b5a43 . The response: Switched to a new branch 'experiment' Check git status and then list the contents of your repository. What do you see? You should notice that some of the files you had created before seem to have disappeared congratulations, you've time travelled! Those files are not missing; but they are on a different branch (the master branch) and you can't harm them now. Add a number of new files, making commits after each one. Check your git status, and check your git log as you go to make sure you're getting everything. Make sure there are no unstaged changes everything's been committed.","title":"4.3. git merge"},{"location":"module-1/Exercises/#44-git-merge-continued","text":"Now let's assume that your experiment branch was successful everything you did there you were happy with and you want to integrate all of those changes back into your master branch. We're going to merge things. To merge, we have to go back to the master branch: $ git checkout master . (Good practice is to keep separate branches for all major experiments or directions you go. In case you lose track of the names of the branches you've created, this command: git branch -va will list them for you.) Now, we merge with $ git merge experiment . Remember, a merge is a special kind of commit that rolls all previous commits from both branches into one git will open your text editor and prompt you to add a message (it will have a default message already there if you want it). Save and exit and ta da! Your changes have been merged together.","title":"4.4. git merge continued"},{"location":"module-1/Exercises/#45-git-push","text":"One of the most powerful aspects of using git is the possibility of using it to manage collaborations. To do this, we have to make a copy of your repository available to others as a remote . There are a variety of places on the web where this can be done; one of the most popular at the moment is GitHub . GitHub allows a user to have an unlimited number of public repositories. Public repositories can be viewed and copied by anyone. Private repositories require a paid account, and access is controlled. If you are working on sensitive materials that can only be shared amongst the collaborators on a project, you should invest in an upgraded account (note that you can also control which files get included in commit; visit GitHub for help on ignoring files . In essence, you simply list the file names you do not want committed; visit GitHub for an example on listing ignored files ). Let's assume that your materials are not sensitive . Login to GitHub. On the upper right part of the screen there is a large + sign. Click on that + sign, and select new repository . On the following screen, give your repo a name. Leave the repository set to 'Public'. DO NOT 'initialize this repo with a readme.md'. Leave add .gitignore and add license set to NONE. Click the green 'Create Repository' button. You now have a space into which you will publish the repository on your machine. At the command line, we now need to tell git the location of this space. We do that with the following command, where you will change your-username and your-new-repo appropriately: $ git remote add origin https://github.com/YOUR-USERNAME/YOUR-NEW-REPO.git Now we push your local copy of the repository onto the web, to the GitHub version of your repo: $ git push -u origin master NB If you wanted to push a branch to your repository on the web instead, do you see how you would do that? If your branch was called experiment , the command would look like this: $ git push origin experiment The changes can sometimes take a few minutes to show up on the website. Now, the next time you make changes to this repository, you can push them to your GitHub account which is the 'origin' in the command above. Add a new text file. Commit the changes. Push the changes to your account.","title":"4.5 git push"},{"location":"module-1/Exercises/#46-git-clone","text":"Imagine you are collaborating with one of your classmates. Your classmate is in charge of the project, and is keeping track of the 'official' folder of materials (i.e. the repo). You wish to make some changes to the files in that repository. You can manage that collaboration via GitHub by making a copy, what GitHub calls a fork . Make sure you're logged into your GitHub account on the GitHub website. We're going to fork an example repository right now by going to GitHub's forking example page . Click the 'fork' button at top-right. GitHub now makes a copy of the repository in your own GitHub account! To make a copy of that repository on your own machine, you will now clone it with the git clone command. (Remember: a 'fork' copies someone's GitHub repo into a repo in your OWN GitHub account; a 'clone' makes a copy on your own MACHINE). Type: $ cd.. $ pwd We do that to make sure you're not inside any other repo you've made! Make sure you're not inside the repository we used in Exercises 1 to 4, then proceed: $ git clone https://github.com/YOUR-USERNAME/Spoon-Knife.git $ ls You now have a folder called 'Spoon-Knife' on your machine! Any changes you make inside that folder can be tracked with commits. You can also git push -u origin master when you're inside it, and the changes will show up on your OWN copy (your fork) on github.com . Make a fork of, and then clone, one of your classmates' repositories. Create a new branch. Add a new file to the repository on your machine, and then push it to your fork on GitHub. Remember, your new file will appear on the new branch you created, NOT the master branch.","title":"4.6. git clone"},{"location":"module-1/Exercises/#47-pull-request","text":"Now, you let your collaborator know that you've made a change that you want her to merge into the original repository. You do this by issuing a pull request . But first, we have to tell git to keep an eye on that original repository, which we will call upstream . You do this by adding that repository's location like so: Type (but change the address appropriately): $ git remote add upstream THE-FULL-URL-TO-THEIR-REPO-ENDING-WITH-.git You can keep your version of the remote up-to-date by fetching any new changes your classmate has done: $ git fetch upstream Now let's make a pull request (you might want to bookmark GitHub's help document for pull requests ). Go to your copy of your classmate's repository at your GitHub account. Make sure you've selected the correct branch you pushed your changes to, by selecting it from the Branches menu drop down list. Click the 'new pull request' button. The new page that appears can be confusing, but it is trying to double check with you which changes you want to make, and where. Make sure these are set properly. Base branch is the branch where you want your changes to go (ie. your classmate's repository). Head branch is the branch where you made your changes. Remember: the Base branch is the TO , the Head branch is the FROM the place where you want your changes to go TO , FROM the place where you made the changes. For example, say I clone Dr. Graham's R repository and create a new experiment branch: If I make changes to my experiment branch that is the Head branch. Then I push those changes to Dr. Graham's master branch that is the Base branch. A pull request has to have a message attached to it, so that your classmate knows what kind of change you're proposing. Fill in the message fields appropriately, then hit the 'create pull request' button.","title":"4.7. pull request"},{"location":"module-1/Exercises/#48-git-merge-again","text":"Finally, the last bit of work to be done is to accept the pull request and merge the changes into the original repository. Go to your repository on your GitHub account. Check to see if there are any 'pull requests' these will be listed under the 'pull requests' tab. Click on that tab. You can merge from the command line, but for now, you can simply click on the green 'merge pull request' button, and then the 'confirm merge' button. The changes your classmate has made have now been folded into your repository. To get the updates on your local machine, go back to the command line and type $ git pull origin master Phew. You might want to do $ history recentcommands.md just to remember what you've done. And then commit that to a repository.","title":"4.8. git merge again"},{"location":"module-1/Exercises/#conclusion","text":"In the modules to come, we will be using DH Box as we find data, fetch data, wrangle data, analyze data, and visualize data. It becomes very important that you note the kinds of commands you use or try, the thinking that you were doing at that point, and so on. You want to leave yourself (and anybody who comes after) breadcrumbs so that you understand what you were doing. Quick notes written in Markdown, piping the history of what you've done to a file, and keeping those files in a repository alongside any other code or files that you may make will set you on the path to open access research and computational reproducibility. As Martha might say, 'and that's a good thing'.","title":"Conclusion"},{"location":"module-1/Open-Access-Research/","text":"Open Access Research May 14-21, 2018 Concepts As historians, we aren't all that accustomed to sharing our research notes. We go to the archives, we take our photographs, we spend hours pouring over documents, photographs, diaries, newspapers... why should someone else benefit from our work? There are a number of reasons why you should want to do this. This week we will read and discuss the arguments advanced by various historians, including: Trevor Owens Caleb McDaniel Ian Milligan another post by Milligan Michelle Moravec Kathleen Fitzpatrick Sheila Brennan But most importantly, change is coming whether historians like it or not . Here in Canada, SSHRC has a research data archiving policy All research data collected with the use of SSHRC funds must be preserved and made available for use by others within a reasonable period of time. SSHRC considers \u201ca reasonable period\u201d to be within two years of the completion of the research project for which the data was collected. Note the conversation that ensued on Twitter after Milligan mentioned all this and also Ian's tweet on documenting research We will explore why and how to make our research notes open, what that implies for how we do research, and how we can use this process to maintain our scholarly voice online. Really, it's also a kind of 'knowledge mobilization' . In this module you will find exercises related to setting up your GitHub account, how to commit, fork, push and pull files to your own repository and to others'. Really, it's about sustainable authorship and preserving your research data . By the end of this module you will know how to work with GitHub to foster collaboration how to set up, fork, and make changes to files and repositories the rationale for historians to make their work public Remember: I do expect you to click through every link I provide, and to read these materials. What you need to do this week Respond to the readings and the reading questions through annotation (taking care to respond to others' annotations as well) - see the instructions below. Remember to tag your annotations with 'hist3814o' so that we can find them on the course Hypothes.is group . Remember to annotate using our HIST3814o group . Do the exercises for this module, pushing yourself as far as you can. Annotate the instructions where they might be unclear or confusing; see if others have annotated them as well, and respond to them with help if you can. Keep an eye on our Slack channel - you can always offer help or seek out help there. Write a blog post describing what happened as you went through the exercises (your successes, your failures, the help you may have found/received), and link to your 'faillog' (ie. the notes you upload to your GitHub account - for more on that, see the exercises!). Submit your work to the course submission form . Readings As you read the posts linked to above (Brennan, Fitzgerald, Guldi and Armitage, McDaniel, Milligan, Moravec, Owens) click through to their 'about' or 'portfolio' pages. How are these scholars portraying themselves? How do they approach the idea of 'openness'? How is your own work 'generous'? Please annotate their work with your observations and questions; please also respond to someone else's annotation with a substantive observation of your own. Remember to annotate using our HIST3814o group . Then, make an entry on your blog that contrasts this picture of 'open access research' with what you may have learned about doing history in your other courses. Where are the dangers and where are the opportunities? What does 'open access' mean for you as a student?","title":"Why you should be open"},{"location":"module-1/Open-Access-Research/#open-access-research-may-14-21-2018","text":"","title":"Open Access Research &mdash; May 14-21, 2018"},{"location":"module-1/Open-Access-Research/#concepts","text":"As historians, we aren't all that accustomed to sharing our research notes. We go to the archives, we take our photographs, we spend hours pouring over documents, photographs, diaries, newspapers... why should someone else benefit from our work? There are a number of reasons why you should want to do this. This week we will read and discuss the arguments advanced by various historians, including: Trevor Owens Caleb McDaniel Ian Milligan another post by Milligan Michelle Moravec Kathleen Fitzpatrick Sheila Brennan But most importantly, change is coming whether historians like it or not . Here in Canada, SSHRC has a research data archiving policy All research data collected with the use of SSHRC funds must be preserved and made available for use by others within a reasonable period of time. SSHRC considers \u201ca reasonable period\u201d to be within two years of the completion of the research project for which the data was collected. Note the conversation that ensued on Twitter after Milligan mentioned all this and also Ian's tweet on documenting research","title":"Concepts"},{"location":"module-1/Open-Access-Research/#we-will-explore","text":"why and how to make our research notes open, what that implies for how we do research, and how we can use this process to maintain our scholarly voice online. Really, it's also a kind of 'knowledge mobilization' . In this module you will find exercises related to setting up your GitHub account, how to commit, fork, push and pull files to your own repository and to others'. Really, it's about sustainable authorship and preserving your research data .","title":"We will explore"},{"location":"module-1/Open-Access-Research/#by-the-end-of-this-module-you-will-know","text":"how to work with GitHub to foster collaboration how to set up, fork, and make changes to files and repositories the rationale for historians to make their work public Remember: I do expect you to click through every link I provide, and to read these materials.","title":"By the end of this module you will know"},{"location":"module-1/Open-Access-Research/#what-you-need-to-do-this-week","text":"Respond to the readings and the reading questions through annotation (taking care to respond to others' annotations as well) - see the instructions below. Remember to tag your annotations with 'hist3814o' so that we can find them on the course Hypothes.is group . Remember to annotate using our HIST3814o group . Do the exercises for this module, pushing yourself as far as you can. Annotate the instructions where they might be unclear or confusing; see if others have annotated them as well, and respond to them with help if you can. Keep an eye on our Slack channel - you can always offer help or seek out help there. Write a blog post describing what happened as you went through the exercises (your successes, your failures, the help you may have found/received), and link to your 'faillog' (ie. the notes you upload to your GitHub account - for more on that, see the exercises!). Submit your work to the course submission form .","title":"What you need to do this week"},{"location":"module-1/Open-Access-Research/#readings","text":"As you read the posts linked to above (Brennan, Fitzgerald, Guldi and Armitage, McDaniel, Milligan, Moravec, Owens) click through to their 'about' or 'portfolio' pages. How are these scholars portraying themselves? How do they approach the idea of 'openness'? How is your own work 'generous'? Please annotate their work with your observations and questions; please also respond to someone else's annotation with a substantive observation of your own. Remember to annotate using our HIST3814o group . Then, make an entry on your blog that contrasts this picture of 'open access research' with what you may have learned about doing history in your other courses. Where are the dangers and where are the opportunities? What does 'open access' mean for you as a student?","title":"Readings"},{"location":"module-2/Exercises/","text":"Module 2 Exercises All five exercises are on this page. Don't forget to scroll. If you have difficulties, or if the instructions need clarification, please click the 'issues' button and leave a note. Feel free to fork and improve these instructions, if you are so inclined. Remember, these exercises get progressively more difficult, and will require you to either download materials or read materials on other websites. Give yourself plenty of time. Try them all, and remember you can turn to your classmates for help. Work together! DO NOT suffer in silence as you try these exercises! Annotate, ask for help, set up an appointment, or find me in person. Background Where do we go to find data? Part of that problem is solved by knowing what question you are asking, and what kinds of data would help solve that question. Let's assume that you have a pretty good question you want an answer to say, something concerning social and household history in early Ottawa, like what was the role of 'corner' stores (if such things exist?) in fostering a sense of neighbourhood and begin thinking about how you'd find data to explore that question. The exercises in this module cover: The Dream Case Wget Writing a program to extract data from a webpage Encoding transcribed text Collecting data from Twitter Coverting images to text with Tesseract There is so much data available; with these methods, we can gather enormous amounts that will let us see large-scale macroscopic patterns. At the same time, it allows us to dive into the details with comparative ease. The thing is, not all digital data are created equally. Google has spent millions digitizing everything ; newspapers have digitized their own collections. Genealogists and local historical societies upload yoinks of digitized photographs, wills, local tax records, you-name-it , every day . But, consider what Milligan has to say about 'illusionary order' : [...] poor and misunderstood use of online newspapers can skew historical research. In a conference presentation or a lecture, it\u2019s not uknown to see the familiar yellow highlighting of found searchwords on projected images: indicative of how the original primary material was obtained. But this historical approach generally usually remains unspoken, without a critical methodological reflection. As I hope I\u2019ll show here, using Pages of the Past uncritically for historical research is akin to using a volume of the Canadian Historical Review with 10% or so of the pages ripped out. Historians, journalists, policy researchers, genealogists, and amateur researchers need to at least have a basic understanding of what goes on behind the black box. Ask yourself: what are some of the key dangers? Reflect: how have you used digitized resources uncritically in the past? Remember: To digitize doesn't or shouldn't mean uploading a photograph of a document. There's a lot more going on than that. We'll get to that in a moment. Exercise 1: The Dream Case In the dream case, your data are not just images, but are actually sorted and structured into some kind of pre-existing database. There are choices made in the creation of the database, but a good database, a good project, will lay out for you their decision making, their corpora, and how they've dealt with ambiguity and so on. You search using a robust interface, and you get a well-formed spreadsheet of data in return. Two examples of 'dream case' data: Epigraphic Database Heidelberg Commwealth War Graves Commission, Find War Dead Explore both databases. Perform a search of interest to you. In the case of the epigraphic database, if you've done any Latin, try searching for terms related to occupations; or you could search Figlina . In the CWGC database, search your own surname. Download your results. You now have data that you can explore! Using the Nano text editor in your DH Box, make a record (or records) of what you searched, the URL for your search and its results, and where you're keeping your data. Lodge a copy of this record in your repository. Exercise 2: Wget You've already encountered wget in the introduction to this workbook, when you were setting up your DH Box to use Pandoc. In this exercise, I want you to do Ian Milligan's wget tutorial at the Programming Historian to learn more about the power of this command, and how to wield that power properly. Skip ahead to step 2, since your DH Box already has wget installed. (If you want to continue to use wget after this course is over, you will have to install it on your own machine, obviously.) Once you've completed Milligan's tutorial, remember to put your history into a new Markdown file, and to lodge a copy of it in your repository. Now that you're au fait with wget, we will do part of Kellen Kurschinski's wget tutorial at the Programming Historian . I want you to use wget to download the Library and Archives Canada 14th Canadian General Hospital war diaries in a responsible and respectful manner. Otherwise, you will look like a bot attacking their site. The URLs of this diary go from http://data2.archives.ca/e/e061/e001518029.jpg to http://data2.archives.ca/e/e061/e001518109.jpg . This is a total of 80 pages note the last part of the URL goes from e001518029 to e001518109 for a total of 80 images. Make a new directory: $ mkdir war-diary and then cd into it: $ cd war-diary . Make sure you're in the directory by typing $ pwd . NB Make sure you are in your parent directory ~ . To get there directly from any subdirectory, type $ cd ~ . If you want to check your file structure quickly, go to the File Manager. We will use a simple Python script to gather all the URLs of the diary images from the Library and Archives. Python is a general purpose programming language. Type $ nano urls.py to open a new Python file called urls . Paste the script below. This script grabs the URLs from e001518029 to e001518110 and puts them in a file called urls.txt : urls = ''; f=open('urls.txt','w') for x in range(8029, 8110): urls = 'http://data2.collectionscanada.ca/e/e061/e00151%d.jpg\\n' % (x) f.write(urls) f.close Hit ctrl+x, Y, enter to save and exit Nano. Type $ python urls.py to run the Python script. Type $ ls and notice the urls.txt file. Type $ nano urls.txt to examine the file. Exit Nano. Type $ wget -i urls.txt -r --no-parent -nd -w 2 --limit-rate=100k to download all the URLs from the urls.txt file. Type $ ls to verify the files were downloaded. Add your command to your history file, and lodge it in your repository. For reference, visit Module 1, Exercise 2 . In Exercise 6 , we will open some of the text files in Nano to judge the 'object character recognition'. Part of the point of working with these files is to show that even with horrible 'digitization', we can still extract useful insights. Digitization is more than simply throwing automatically generated files online. Good digitization requires scholarly work and effort! We will learn how to do this properly in the next exercise. There's no one right way to do things, digitally. There are many paths. The crucial thing is that you find a way that makes sense for your own workflow, and that doesn't make you a drain on someone else's resources. Exercise 3: TEI Digitization requires human intervention. This can be as straightforward as correcting errors or adjusting the scanner settings when we do OCR, or it can be the rather more involved work of adding a layer of semantic information to the text. When we mark up a text with the semantic hooks and signs that explain we are talking about London, Ontario rather than London, UK , we've made the text a whole lot more useful for other scholars or tools. In this exercise, you will do some basic marking up of a text using standards from the Text Encoding Initiative . (Some of the earliest digital history work was along these lines). The TEI exercise requires careful attention to detail. Read through it before you try it. In this exercise, you'll transcribe a page from an abolitionist's pamphlet. You'll also think about ways of transforming the resulting XML into other formats. Make notes in a file to upload to your repository, and upload your XML and your XSL file to your own repository as well. (As an added optional challenge, create a gh-pages branch and figure out the direct URL to your XML file, and email that URL to me). For this exercise, do the following: The TEI exercise found in our supporting materials . I will note that a perfectly fine option for the Capstone Exercise for HIST3814 might be to use this exercise as a model to markup the war diary and suggest ways this material might be explored. Remember to make (and lodge in your repository) a file detailing the work you've done and any issues you've run into. Exercise 4: APIs Sometimes, a website will have what is called an Application Programming Interface or API. In essence, this lets a program on your computer talk to the computer serving the website you're interested in, such that the website gives you the data that you're looking for. That is, instead of you punching in the search terms, and copying and pasting the results, you get the computer to do it. More or less. The thing is, the results come back to you in a machine-readable format often, JSON, which is a kind of text format. It looks like the following: This exercises uses an O-Date Binder . Launch the jupyter binder . Open the 'Chronicling America API' notebook. Run through its various steps so that you end up with a json file of results. Imagine that you are writing a paper on the public reception of archaeology in the 19th century in the United States. Alter the notebook so that you can find primary source material for your study. Going further: Find another API for historical newspapers somewhere else in the world. Duplicate the notebook, and alter it to search this other API so that you can have material for a cross-cultural comparison. Open the 'Open Context API'. Notice how similar it is to the first notebook! Run through the steps so that you can see it work. Study the Open Context API documentation . Modify the search to return materials from a particular project or site. The final notebook, 'Open Context Measurements', is a much more complicated series of calls to the Open Context API (courtesy of Eric Kansa). In this notebook, we are searching for zoological data held in Open Context, using standardised vocabularies from that field that described faunal remains. Examine the code carefully - do you see a series of nested 'if' statements? Remember that data is often described using JSON attribute:value pairs. These can be nested within one another, like Russian dolls. This series of 'if' statements parses the data into these nested levels, looking for the faunal information. Open Context is using an ontology or formal description of categorization of the data (which you can visit on the Open Context website ) that enables inter-operability with various Linked Open Data schemes. Run each section of the code. Do you see the section that defines how to make a plot? This code is called on later in the notebook, enabling us to plot the counts of the different kinds of faunal data. Try plotting different categories. The notebooks above were written in Python. We can also interact with APIs using the R statistical programming language. The Portable Antiquities Scheme database also has an API. Launch this binder and open the 'Retrieving Data from the Portable Antiquities Scheme Database' notebook (courtesy of Daniel Pett). This notebook is in two parts. The first frames a query and then writes the result to a csv file for you. Work out how to make the query search for medieval materials, and write a csv to keep more of the data fields. The second part of the notebook that interacts with the Portable Antiquities Scheme database uses that csv file to determine where the images for each item are located on the Scheme's servers, and to download them. Going further - the Programming Historian has a lesson on creating a web API . Follow that lesson and build a web api that serves some archaeological data that you've created or have access to. One idea might be to extend the Digital Atlas of Egyptian Archaeology , a gazetteer created by Anthropology undergraduates at Michigan State University . The source data may be found on the MSU Anthropology GitHub page . Exercise 5: Mining Twitter This exercise mines Twitter for tweets, photos, etc, connected to bad archaeology. You will require the following on your machine: Twitter account Python installed on your machine However, we will use a virtual computer that already has Python and the Twarc (TWitter ARChiver) package already installed. (Mac users: you do have python already on your machine; Windows users, you don't. So, to make life easier, I built you all a virtual computer. Read on, this will make sense.) Twarc was created by Ed Summers, who is heading a project called Documenting the Now to create the tools necessary to keep track of all the ways our media are disseminating (and distorting!) the truth. It's a rapid-response suite of tools to capture events as they happen. What we're going to do is set up some credentials with Twitter that give you access to the underlying data of the tweets. Then, the Twarc packages has a bunch of functions built in that lets you search and archive tweets. Setting up your Twitter Credentials WARNING Update as of October 12, 2018: It seems that Twitter now requires some kind of developer authentication process now. So the instructions below might have extra steps. If that's the case, no problem email Dr Graham for his consumer secret/key. READ THROUGH FIRST, THEN START If you do not want to set up a Twitter account/developer app page, skip to Part Two: Firing Up Your Virtual Computer First of all, you need to set up a Twitter account. If you do not have a Twitter account , sign-up, but make sure to minimize any personal information that is exposed. For instance, do not make your handle the same as your real name. Turn off geolocation. Do not give your actual location in the profile. View the settings, and make sure all of the privacy settings are dialed down. For the time being, you do have to associate a cell phone number with your account. You can delete that once you've done the next step. Go to the Twitter apps page - https://apps.twitter.com/ and click on 'New App'. On the New Application page, just give your app a name like my-twarc or similar. For the portion labelled 'Website', use my Crafting Digital History site URL, site.craftingdigitalhistory.ca (although for our purposes any website will do). You don\u2019t need to fill in any of the rest of the fields. Continue on to the next page (tick off the box saying you\u2019ve read the developer code of behaviour). This next page shows you all the details about your new application. Click on the tab labelled Keys and Access Tokens . Copy the 'Consumer Key (API Key)' (the consumer secret) to a text file. Click on the button labelled Create Access Tokens at the bottom of the page. This generates an access token and an access secret. Copy those to your text file and save it. Do not put this file in your repo or leave it online anywhere. Otherwise, a person can impersonate you! Part Two: Firing Up Your Virtual Computer Right-click the following Binder link and select Open in new tab : Wait. It is setting up your new virtual computer. When it's loaded up, it will resembled the following image: Click the 'New' button, and select 'Terminal'. You should see something resembling the following image: WARNING. You have to interact with the virtual computer type a command, click on something within ten minutes or it shuts down and you lose everything . Pro tip: you can always type $ pwd into the terminal as a way of keeping the thing alive. This command Prints the Working Directory (ex. tells you what folder you're in). Type $ twarc configure into the terminal. And in the next screenshot, read the annotations from top to bottom: But where is this 'Displayed pin'? It's in the window in your browser where the 'Authenticate Twitter?' dialogue was displayed. Instead of the twitter page, the URL you gave way up in part one step 3 has now been loaded and the information you need is indicated with oauth_token= . You need the bit after the = sign. It will resemble something like -bBafl42aabB... etc., a long string of numbers and letters. Copy the entire string. Then paste it in: And you're ready to data mine twitter! Part Three: Find some stuff On the Twarc README page there are examples of how to use Twarc to search for information. If you typed in $ twarc search electricarchaeo and hit return, the terminal window would fill with tweet metadata and data; it'd take about one or two minutes to run as I'm pretty active on twitter (watch a video of Twarc in action ). But this is only the last week or so of information! You can save this information to a file instead of your terminal window by typing the following command into the terminal: $ twarc search electricarchaeo electricarchaeo.jsonl (that's a lower case 'L' at the end of .json ) If you then right-click on the 'jupyter' logo at the top of the screen and open the link in a new window, you'll be back at the File Explorer. One of the files listed will be the new electricarchaeo.jsonl file you made. So what can we do with this information? Quite a lot, but for now, let's make a basic word cloud of the text of the tweets. We're going to need some utilities that Ed Summer has written, some extra python programs that can work with this information. Navigate to the terminal window and type the command $ git clone https://github.com/DocNow/twarc . This tells the git program that's already installed to go to GitHub and get the source code for Twarc. One of the folders it is going to grab is called utils and that has the utility program for making a word cloud that we will use. We will now type in the location and name of the program we want to use, followed by the data to operate on, followed by the name of the output file we want to create. Type $ twarc/utils/wordcloud.py electricarchaeo.jsonl wordcloud.html into the terminal. Notice that when you press enter, nothing seems to happen. Go back to the list of files (right-click the jupyter logo and select Open in new tab ) and one of the files listed will be wordcloud.html . Click on that file. (If you reload the page, the word-cloud will regenerate, different colours, positions) Download the data you collected. Once this virtual computer shuts down, all of that data you collected will be lost. Click on the jupyter logo, select the files you want to keep, and hit download. Depending on your browser, you might get a warning, asking if you're sure you want to download. Click Yes/OK. Other things you could do Descriptions of what the other utilities do can be found on the Twarc GitHub repository . Go, play! Once you've got that jsonl file, you could convert it to csv and then import it into a spreadsheet tool like Excel of Google Sheets. The JSON to CSV website can also do that conversion for you. Warning: there is a 1MB limit for using the JSON to CSV tool. If your data is too big, we'll find something else perhaps a bit of code or a utility that we can use on the command line (ex. read Gabriel Pires' Medium article on using Python to convert JSON to CSV ). With a bit of effort, you can make a network graph of who responds to who. Or explore tweeting by gender. Or map tweets. Or... or... or. Probably the easiest thing you could do, once you've converted to csv (again, the JSON to CSV website can do this ), is to copy the column of tweet text itself into a tool like Voyant . Why not give that a try? What you do really depends on what kinds of questions you're hoping to answer. Now that you have access to the firehose of data that is Twitter, what kinds of things might you like to know? How do people talk about archaeology? How do they talk about Atlantis? What can you do with this data? Examine the Twarc repository, especially its utilities. You could extract the geolocated ones and map them. You could examine the difference between 'male' and 'female' tweeters (and how problematic might that be?). In your CSV, save the text of the posts to a new file and upload it to something like Voyant Tools to visualize trends over time. Google for analysis of Twitter data to get some ideas. Exercise 6: Using Tesseract to turn an image into text We've all used image files like JPGs and PNGs. Images always look the same on whatever machine they are displayed on, because they contain within themselves the complete description of what the 'page' should look like. You're likley familiar with the fact that you cannot select text within an image. When we digitize documents, the image that results only contains the image layer, not the text. To turn that image into text, we have to do what's called 'object character recognition', or OCR. An OCR algorithm looks at the pattern of pixels in the image, and maps these against the shapes it 'knows' to be an A, or an a, or a B, or a , and so on. Cleaner, sharper printing gives better results as do high resolution images free from noise. People who have a lot of material to OCR use some very powerful tools to identify blocks of text within the newspaper page, and then train the machine to identify these, a process beyond us just now (but visit this Tesseract q a on stackoverflow if you're interested). In this exercise, you'll: Install the Tesseract OCR engine into your DH Box Install and use ImageMagick to convert the JPG into TIFF image format Use Tesseract to OCR the resulting pages. Use Tesseract in R to OCR the resulting pages. Compare the resulting OCRd texts. Converting images in the command line Begin by making a new directory for this exercise: $ mkdir ocr-test . Type $ cd ocr-test to change directories into ocr-test. Type $ sudo apt-get install tesseract-ocr to grab the latest version of Tesseract and install it into your DH Box. Enter your password when the computer asks for it. Type $ sudo apt-get install imagemagick to install ImageMagick. Let's convert the first file to TIFF with ImageMagick's convert command $ convert -density 300 ~/war-diary/e001518087.jpg -depth 8 -strip -background white -alpha off e001518087.tiff You want a high density image, which is what the -density and the -depth flags do; the rest of the command formats the image in a way that Tesseract expects to encounter text. This command might take a while. Just wait, be patient. Extract text with $ tesseract e001518087.tiff output.txt . This might also take some time. Download the output.txt file to your own machine via DH Box's filemanager. Open the file with a text editor. Converting images in R Now we will convert the file using R. Navigate to RStudio in the DH Box. On the upper left side, click the green plus button R Script to open a new blank script file. Paste in the following script and save it as ocr in RStudio. install.packages('magick') install.packages('magrittr') install.packages('pdftools') install.packages('tesseract') library(magick) library(magrittr) library(pdftools) library(tesseract) text - image_read(\"~/war-diary/e001518087.jpg\") % % image_resize(\"2000\") % % image_convert(colorspace = 'gray') % % image_trim() % % image_ocr() write.table(text, \"~/ocr-test/R.txt\") The above script first installs three packages to RStudio, Magick, Magrittr, and Tesseract. Magick processes the image as high quality; Magrittr uses the symbols % % as a pipe that forces the values of an expression into the next function (allowing our script to perform it's conversion in steps); Tesseract is the actual OCR engine that converts our image to text. Then the script loads each package. Lastly, the script processes the image, OCRs the text, and writes it to a txt file. Before installing any packages in the DH Box RStudio, we need to install some dependencies in the command line. The reason is that since DH Box runs an older version of RStudio, not everything installs as planned compared to the desktop RStudio. Type $ sudo apt-get install libcurl4-gnutls-dev in the command line to install the libcurl library (this installs RCurl). Type $ sudo apt-get install libmagick++-dev in the command line to install the libmagick library. Type $ sudo apt-get install libtesseract-dev in the command line to install the libtesseract library. Type $ sudo apt-get install libleptonica-dev in the command line to install the libleptonic library. Type $ sudo apt-get install tesseract-ocr-eng in the command line to install the English Tesseract library. Type $ sudo apt-get install libpoppler-cpp-dev in the command line to install the poppler cpp library. Navigate to RStudio and run each install.packages line in our script. This will take some time. Run each library() line to load the libraries. Run each line up to image_ocr() . This may take some time to complete. Run the last line write.table() to export the OCR to a text file with the same name. Navigate to your file manager and download both the output.txt file and the R.txt file. Compare the two text files in your desktop. How is the OCR in the command line versus within R? Note that they both use Tesseract just with different settings and in different environments. Progressively converting our files with Tesseract Now take a screen shot of both text files (just the text area) and name them output_1.png and R_1.png respectively. Upload both files into DH Box via the File Manager. In the command line, type $ tesseract output_1.png output_1.txt . In RStudio, change the file paths in your script to the following: text - image_read(\"~/ocr-test/R_1.png\") % % image_resize(\"2000\") % % image_convert(colorspace = 'gray') % % image_trim() % % image_ocr() write.table(text, \"~/ocr-test/R_1.txt\") Run each script line again. Except this time DO NOT run the install.packages() lines since we already installed them. Simply load the libraries again and run each line. Navigate to the File Manager and download ouput_1.txt and R_1.txt . Compare these two files. Did the OCR conversion get progressively worse? How do they compare to each other, to the first attempt at conversion, and then to the originals? Choose either the command line or the R method to convert more of the war diary files to text. Save these files into a new directory called war-diary-text . We will use these text files for future work in topic modeling and text analysis. How might your decision on which method to use change the results you would get in, say, a topic modeling tool? Hint: Check below for a way to automate the conversion process . Think about how these conversions can change based on the image being run through Tesseract. Does Tesseract have an easier time converting computer text even though it's in an image format? How might OCR conversions affect the way historians work on batch files? How does the context of the text change how historians analyse it? Look up the Tesseract wiki . What other options could you use with the Tesseract command to improve the results? When you decide to download Tesseract to you own computer, use the following two guides to automating bulk OCR (multiple files) with Tesseract: Peirson's and Schmidt's . Batch converting image files Now that you've learned to convert image files to text individually, you should know that there is a quicker way to do this. The following script takes your Canadian war diary jpg image files and OCRs them using the same process as above. However this time, the function(i) ( i stands for iterate), goes through the folder for each jpg file and converts them to a png file into a text file with the suffix -ocr.txt until it notes there are no more image files to convert. The converted files will output to the same folder, in our case war-diary . library(magick) library(magrittr) library(pdftools) library(tesseract) dest - \"/war-diary\" myfiles - list.files(path = dest, pattern = \"jpg\", full.names = TRUE) # improve the images # ocr 'em # write the output to text file lapply(myfiles, function(i){ text - image_read(i) % % image_resize(\"3000x\") % % image_convert(type = 'Grayscale') % % image_trim(fuzz = 40) % % image_write(format = 'png', density = '300x300') % % tesseract::ocr() outfile - paste(i,\"-ocr.txt\",sep=\"\") cat(text, file=outfile, sep=\"\\n\") }) You can probably understand now why this method works much better than the previous R script: we automate our process (ie. we run the script just once and it iterates through the folder for us), append a suffix to the txt file name noting the OCRd text, and output it all to the same folder. Keep these files somewhere handy on your computer! We will use them throughout the course. By iterating through the folder, we have created a loop. Loops can be a very powerful tool for Digital Historians, as we can begin to automate processes that would take much longer by hand. Read more about 'for loops' on the R-blogger website . NB The above script processes dozens of images and may take quite a bit of time to complete. Reference Part of this tutorial was adapted from The Programming Historian released under the CC-BY license, including: Ian Milligan, \"Automated Downloading with Wget,\" The Programming Historian 1 (2012), https://programminghistorian.org/lessons/automated-downloading-with-wget . Kellen Kurschinski, \"Applied Archival Downloading with Wget,\" The Programming Historian 2 (2013), https://programminghistorian.org/lessons/applied-archival-downloading-with-wget . Twitter user 'superboreen' on R OCR .","title":"Exercises"},{"location":"module-2/Exercises/#module-2-exercises","text":"All five exercises are on this page. Don't forget to scroll. If you have difficulties, or if the instructions need clarification, please click the 'issues' button and leave a note. Feel free to fork and improve these instructions, if you are so inclined. Remember, these exercises get progressively more difficult, and will require you to either download materials or read materials on other websites. Give yourself plenty of time. Try them all, and remember you can turn to your classmates for help. Work together! DO NOT suffer in silence as you try these exercises! Annotate, ask for help, set up an appointment, or find me in person.","title":"Module 2 Exercises"},{"location":"module-2/Exercises/#background","text":"Where do we go to find data? Part of that problem is solved by knowing what question you are asking, and what kinds of data would help solve that question. Let's assume that you have a pretty good question you want an answer to say, something concerning social and household history in early Ottawa, like what was the role of 'corner' stores (if such things exist?) in fostering a sense of neighbourhood and begin thinking about how you'd find data to explore that question. The exercises in this module cover: The Dream Case Wget Writing a program to extract data from a webpage Encoding transcribed text Collecting data from Twitter Coverting images to text with Tesseract There is so much data available; with these methods, we can gather enormous amounts that will let us see large-scale macroscopic patterns. At the same time, it allows us to dive into the details with comparative ease. The thing is, not all digital data are created equally. Google has spent millions digitizing everything ; newspapers have digitized their own collections. Genealogists and local historical societies upload yoinks of digitized photographs, wills, local tax records, you-name-it , every day . But, consider what Milligan has to say about 'illusionary order' : [...] poor and misunderstood use of online newspapers can skew historical research. In a conference presentation or a lecture, it\u2019s not uknown to see the familiar yellow highlighting of found searchwords on projected images: indicative of how the original primary material was obtained. But this historical approach generally usually remains unspoken, without a critical methodological reflection. As I hope I\u2019ll show here, using Pages of the Past uncritically for historical research is akin to using a volume of the Canadian Historical Review with 10% or so of the pages ripped out. Historians, journalists, policy researchers, genealogists, and amateur researchers need to at least have a basic understanding of what goes on behind the black box. Ask yourself: what are some of the key dangers? Reflect: how have you used digitized resources uncritically in the past? Remember: To digitize doesn't or shouldn't mean uploading a photograph of a document. There's a lot more going on than that. We'll get to that in a moment.","title":"Background"},{"location":"module-2/Exercises/#exercise-1-the-dream-case","text":"In the dream case, your data are not just images, but are actually sorted and structured into some kind of pre-existing database. There are choices made in the creation of the database, but a good database, a good project, will lay out for you their decision making, their corpora, and how they've dealt with ambiguity and so on. You search using a robust interface, and you get a well-formed spreadsheet of data in return. Two examples of 'dream case' data: Epigraphic Database Heidelberg Commwealth War Graves Commission, Find War Dead Explore both databases. Perform a search of interest to you. In the case of the epigraphic database, if you've done any Latin, try searching for terms related to occupations; or you could search Figlina . In the CWGC database, search your own surname. Download your results. You now have data that you can explore! Using the Nano text editor in your DH Box, make a record (or records) of what you searched, the URL for your search and its results, and where you're keeping your data. Lodge a copy of this record in your repository.","title":"Exercise 1: The Dream Case"},{"location":"module-2/Exercises/#exercise-2-wget","text":"You've already encountered wget in the introduction to this workbook, when you were setting up your DH Box to use Pandoc. In this exercise, I want you to do Ian Milligan's wget tutorial at the Programming Historian to learn more about the power of this command, and how to wield that power properly. Skip ahead to step 2, since your DH Box already has wget installed. (If you want to continue to use wget after this course is over, you will have to install it on your own machine, obviously.) Once you've completed Milligan's tutorial, remember to put your history into a new Markdown file, and to lodge a copy of it in your repository. Now that you're au fait with wget, we will do part of Kellen Kurschinski's wget tutorial at the Programming Historian . I want you to use wget to download the Library and Archives Canada 14th Canadian General Hospital war diaries in a responsible and respectful manner. Otherwise, you will look like a bot attacking their site. The URLs of this diary go from http://data2.archives.ca/e/e061/e001518029.jpg to http://data2.archives.ca/e/e061/e001518109.jpg . This is a total of 80 pages note the last part of the URL goes from e001518029 to e001518109 for a total of 80 images. Make a new directory: $ mkdir war-diary and then cd into it: $ cd war-diary . Make sure you're in the directory by typing $ pwd . NB Make sure you are in your parent directory ~ . To get there directly from any subdirectory, type $ cd ~ . If you want to check your file structure quickly, go to the File Manager. We will use a simple Python script to gather all the URLs of the diary images from the Library and Archives. Python is a general purpose programming language. Type $ nano urls.py to open a new Python file called urls . Paste the script below. This script grabs the URLs from e001518029 to e001518110 and puts them in a file called urls.txt : urls = ''; f=open('urls.txt','w') for x in range(8029, 8110): urls = 'http://data2.collectionscanada.ca/e/e061/e00151%d.jpg\\n' % (x) f.write(urls) f.close Hit ctrl+x, Y, enter to save and exit Nano. Type $ python urls.py to run the Python script. Type $ ls and notice the urls.txt file. Type $ nano urls.txt to examine the file. Exit Nano. Type $ wget -i urls.txt -r --no-parent -nd -w 2 --limit-rate=100k to download all the URLs from the urls.txt file. Type $ ls to verify the files were downloaded. Add your command to your history file, and lodge it in your repository. For reference, visit Module 1, Exercise 2 . In Exercise 6 , we will open some of the text files in Nano to judge the 'object character recognition'. Part of the point of working with these files is to show that even with horrible 'digitization', we can still extract useful insights. Digitization is more than simply throwing automatically generated files online. Good digitization requires scholarly work and effort! We will learn how to do this properly in the next exercise. There's no one right way to do things, digitally. There are many paths. The crucial thing is that you find a way that makes sense for your own workflow, and that doesn't make you a drain on someone else's resources.","title":"Exercise 2: Wget"},{"location":"module-2/Exercises/#exercise-3-tei","text":"Digitization requires human intervention. This can be as straightforward as correcting errors or adjusting the scanner settings when we do OCR, or it can be the rather more involved work of adding a layer of semantic information to the text. When we mark up a text with the semantic hooks and signs that explain we are talking about London, Ontario rather than London, UK , we've made the text a whole lot more useful for other scholars or tools. In this exercise, you will do some basic marking up of a text using standards from the Text Encoding Initiative . (Some of the earliest digital history work was along these lines). The TEI exercise requires careful attention to detail. Read through it before you try it. In this exercise, you'll transcribe a page from an abolitionist's pamphlet. You'll also think about ways of transforming the resulting XML into other formats. Make notes in a file to upload to your repository, and upload your XML and your XSL file to your own repository as well. (As an added optional challenge, create a gh-pages branch and figure out the direct URL to your XML file, and email that URL to me). For this exercise, do the following: The TEI exercise found in our supporting materials . I will note that a perfectly fine option for the Capstone Exercise for HIST3814 might be to use this exercise as a model to markup the war diary and suggest ways this material might be explored. Remember to make (and lodge in your repository) a file detailing the work you've done and any issues you've run into.","title":"Exercise 3: TEI"},{"location":"module-2/Exercises/#exercise-4-apis","text":"Sometimes, a website will have what is called an Application Programming Interface or API. In essence, this lets a program on your computer talk to the computer serving the website you're interested in, such that the website gives you the data that you're looking for. That is, instead of you punching in the search terms, and copying and pasting the results, you get the computer to do it. More or less. The thing is, the results come back to you in a machine-readable format often, JSON, which is a kind of text format. It looks like the following: This exercises uses an O-Date Binder . Launch the jupyter binder . Open the 'Chronicling America API' notebook. Run through its various steps so that you end up with a json file of results. Imagine that you are writing a paper on the public reception of archaeology in the 19th century in the United States. Alter the notebook so that you can find primary source material for your study. Going further: Find another API for historical newspapers somewhere else in the world. Duplicate the notebook, and alter it to search this other API so that you can have material for a cross-cultural comparison. Open the 'Open Context API'. Notice how similar it is to the first notebook! Run through the steps so that you can see it work. Study the Open Context API documentation . Modify the search to return materials from a particular project or site. The final notebook, 'Open Context Measurements', is a much more complicated series of calls to the Open Context API (courtesy of Eric Kansa). In this notebook, we are searching for zoological data held in Open Context, using standardised vocabularies from that field that described faunal remains. Examine the code carefully - do you see a series of nested 'if' statements? Remember that data is often described using JSON attribute:value pairs. These can be nested within one another, like Russian dolls. This series of 'if' statements parses the data into these nested levels, looking for the faunal information. Open Context is using an ontology or formal description of categorization of the data (which you can visit on the Open Context website ) that enables inter-operability with various Linked Open Data schemes. Run each section of the code. Do you see the section that defines how to make a plot? This code is called on later in the notebook, enabling us to plot the counts of the different kinds of faunal data. Try plotting different categories. The notebooks above were written in Python. We can also interact with APIs using the R statistical programming language. The Portable Antiquities Scheme database also has an API. Launch this binder and open the 'Retrieving Data from the Portable Antiquities Scheme Database' notebook (courtesy of Daniel Pett). This notebook is in two parts. The first frames a query and then writes the result to a csv file for you. Work out how to make the query search for medieval materials, and write a csv to keep more of the data fields. The second part of the notebook that interacts with the Portable Antiquities Scheme database uses that csv file to determine where the images for each item are located on the Scheme's servers, and to download them. Going further - the Programming Historian has a lesson on creating a web API . Follow that lesson and build a web api that serves some archaeological data that you've created or have access to. One idea might be to extend the Digital Atlas of Egyptian Archaeology , a gazetteer created by Anthropology undergraduates at Michigan State University . The source data may be found on the MSU Anthropology GitHub page .","title":"Exercise 4: APIs"},{"location":"module-2/Exercises/#exercise-5-mining-twitter","text":"This exercise mines Twitter for tweets, photos, etc, connected to bad archaeology. You will require the following on your machine: Twitter account Python installed on your machine However, we will use a virtual computer that already has Python and the Twarc (TWitter ARChiver) package already installed. (Mac users: you do have python already on your machine; Windows users, you don't. So, to make life easier, I built you all a virtual computer. Read on, this will make sense.) Twarc was created by Ed Summers, who is heading a project called Documenting the Now to create the tools necessary to keep track of all the ways our media are disseminating (and distorting!) the truth. It's a rapid-response suite of tools to capture events as they happen. What we're going to do is set up some credentials with Twitter that give you access to the underlying data of the tweets. Then, the Twarc packages has a bunch of functions built in that lets you search and archive tweets.","title":"Exercise 5: Mining Twitter"},{"location":"module-2/Exercises/#setting-up-your-twitter-credentials","text":"WARNING Update as of October 12, 2018: It seems that Twitter now requires some kind of developer authentication process now. So the instructions below might have extra steps. If that's the case, no problem email Dr Graham for his consumer secret/key. READ THROUGH FIRST, THEN START If you do not want to set up a Twitter account/developer app page, skip to Part Two: Firing Up Your Virtual Computer First of all, you need to set up a Twitter account. If you do not have a Twitter account , sign-up, but make sure to minimize any personal information that is exposed. For instance, do not make your handle the same as your real name. Turn off geolocation. Do not give your actual location in the profile. View the settings, and make sure all of the privacy settings are dialed down. For the time being, you do have to associate a cell phone number with your account. You can delete that once you've done the next step. Go to the Twitter apps page - https://apps.twitter.com/ and click on 'New App'. On the New Application page, just give your app a name like my-twarc or similar. For the portion labelled 'Website', use my Crafting Digital History site URL, site.craftingdigitalhistory.ca (although for our purposes any website will do). You don\u2019t need to fill in any of the rest of the fields. Continue on to the next page (tick off the box saying you\u2019ve read the developer code of behaviour). This next page shows you all the details about your new application. Click on the tab labelled Keys and Access Tokens . Copy the 'Consumer Key (API Key)' (the consumer secret) to a text file. Click on the button labelled Create Access Tokens at the bottom of the page. This generates an access token and an access secret. Copy those to your text file and save it. Do not put this file in your repo or leave it online anywhere. Otherwise, a person can impersonate you!","title":"Setting up your Twitter Credentials"},{"location":"module-2/Exercises/#part-two-firing-up-your-virtual-computer","text":"Right-click the following Binder link and select Open in new tab : Wait. It is setting up your new virtual computer. When it's loaded up, it will resembled the following image: Click the 'New' button, and select 'Terminal'. You should see something resembling the following image: WARNING. You have to interact with the virtual computer type a command, click on something within ten minutes or it shuts down and you lose everything . Pro tip: you can always type $ pwd into the terminal as a way of keeping the thing alive. This command Prints the Working Directory (ex. tells you what folder you're in). Type $ twarc configure into the terminal. And in the next screenshot, read the annotations from top to bottom: But where is this 'Displayed pin'? It's in the window in your browser where the 'Authenticate Twitter?' dialogue was displayed. Instead of the twitter page, the URL you gave way up in part one step 3 has now been loaded and the information you need is indicated with oauth_token= . You need the bit after the = sign. It will resemble something like -bBafl42aabB... etc., a long string of numbers and letters. Copy the entire string. Then paste it in: And you're ready to data mine twitter!","title":"Part Two: Firing Up Your Virtual Computer"},{"location":"module-2/Exercises/#part-three-find-some-stuff","text":"On the Twarc README page there are examples of how to use Twarc to search for information. If you typed in $ twarc search electricarchaeo and hit return, the terminal window would fill with tweet metadata and data; it'd take about one or two minutes to run as I'm pretty active on twitter (watch a video of Twarc in action ). But this is only the last week or so of information! You can save this information to a file instead of your terminal window by typing the following command into the terminal: $ twarc search electricarchaeo electricarchaeo.jsonl (that's a lower case 'L' at the end of .json ) If you then right-click on the 'jupyter' logo at the top of the screen and open the link in a new window, you'll be back at the File Explorer. One of the files listed will be the new electricarchaeo.jsonl file you made. So what can we do with this information? Quite a lot, but for now, let's make a basic word cloud of the text of the tweets. We're going to need some utilities that Ed Summer has written, some extra python programs that can work with this information. Navigate to the terminal window and type the command $ git clone https://github.com/DocNow/twarc . This tells the git program that's already installed to go to GitHub and get the source code for Twarc. One of the folders it is going to grab is called utils and that has the utility program for making a word cloud that we will use. We will now type in the location and name of the program we want to use, followed by the data to operate on, followed by the name of the output file we want to create. Type $ twarc/utils/wordcloud.py electricarchaeo.jsonl wordcloud.html into the terminal. Notice that when you press enter, nothing seems to happen. Go back to the list of files (right-click the jupyter logo and select Open in new tab ) and one of the files listed will be wordcloud.html . Click on that file. (If you reload the page, the word-cloud will regenerate, different colours, positions)","title":"Part Three: Find some stuff"},{"location":"module-2/Exercises/#download-the-data-you-collected","text":"Once this virtual computer shuts down, all of that data you collected will be lost. Click on the jupyter logo, select the files you want to keep, and hit download. Depending on your browser, you might get a warning, asking if you're sure you want to download. Click Yes/OK.","title":"Download the data you collected."},{"location":"module-2/Exercises/#other-things-you-could-do","text":"Descriptions of what the other utilities do can be found on the Twarc GitHub repository . Go, play! Once you've got that jsonl file, you could convert it to csv and then import it into a spreadsheet tool like Excel of Google Sheets. The JSON to CSV website can also do that conversion for you. Warning: there is a 1MB limit for using the JSON to CSV tool. If your data is too big, we'll find something else perhaps a bit of code or a utility that we can use on the command line (ex. read Gabriel Pires' Medium article on using Python to convert JSON to CSV ). With a bit of effort, you can make a network graph of who responds to who. Or explore tweeting by gender. Or map tweets. Or... or... or. Probably the easiest thing you could do, once you've converted to csv (again, the JSON to CSV website can do this ), is to copy the column of tweet text itself into a tool like Voyant . Why not give that a try? What you do really depends on what kinds of questions you're hoping to answer. Now that you have access to the firehose of data that is Twitter, what kinds of things might you like to know? How do people talk about archaeology? How do they talk about Atlantis?","title":"Other things you could do"},{"location":"module-2/Exercises/#what-can-you-do-with-this-data","text":"Examine the Twarc repository, especially its utilities. You could extract the geolocated ones and map them. You could examine the difference between 'male' and 'female' tweeters (and how problematic might that be?). In your CSV, save the text of the posts to a new file and upload it to something like Voyant Tools to visualize trends over time. Google for analysis of Twitter data to get some ideas.","title":"What can you do with this data?"},{"location":"module-2/Exercises/#exercise-6-using-tesseract-to-turn-an-image-into-text","text":"We've all used image files like JPGs and PNGs. Images always look the same on whatever machine they are displayed on, because they contain within themselves the complete description of what the 'page' should look like. You're likley familiar with the fact that you cannot select text within an image. When we digitize documents, the image that results only contains the image layer, not the text. To turn that image into text, we have to do what's called 'object character recognition', or OCR. An OCR algorithm looks at the pattern of pixels in the image, and maps these against the shapes it 'knows' to be an A, or an a, or a B, or a , and so on. Cleaner, sharper printing gives better results as do high resolution images free from noise. People who have a lot of material to OCR use some very powerful tools to identify blocks of text within the newspaper page, and then train the machine to identify these, a process beyond us just now (but visit this Tesseract q a on stackoverflow if you're interested). In this exercise, you'll: Install the Tesseract OCR engine into your DH Box Install and use ImageMagick to convert the JPG into TIFF image format Use Tesseract to OCR the resulting pages. Use Tesseract in R to OCR the resulting pages. Compare the resulting OCRd texts.","title":"Exercise 6: Using Tesseract to turn an image into text"},{"location":"module-2/Exercises/#converting-images-in-the-command-line","text":"Begin by making a new directory for this exercise: $ mkdir ocr-test . Type $ cd ocr-test to change directories into ocr-test. Type $ sudo apt-get install tesseract-ocr to grab the latest version of Tesseract and install it into your DH Box. Enter your password when the computer asks for it. Type $ sudo apt-get install imagemagick to install ImageMagick. Let's convert the first file to TIFF with ImageMagick's convert command $ convert -density 300 ~/war-diary/e001518087.jpg -depth 8 -strip -background white -alpha off e001518087.tiff You want a high density image, which is what the -density and the -depth flags do; the rest of the command formats the image in a way that Tesseract expects to encounter text. This command might take a while. Just wait, be patient. Extract text with $ tesseract e001518087.tiff output.txt . This might also take some time. Download the output.txt file to your own machine via DH Box's filemanager. Open the file with a text editor.","title":"Converting images in the command line"},{"location":"module-2/Exercises/#converting-images-in-r","text":"Now we will convert the file using R. Navigate to RStudio in the DH Box. On the upper left side, click the green plus button R Script to open a new blank script file. Paste in the following script and save it as ocr in RStudio. install.packages('magick') install.packages('magrittr') install.packages('pdftools') install.packages('tesseract') library(magick) library(magrittr) library(pdftools) library(tesseract) text - image_read(\"~/war-diary/e001518087.jpg\") % % image_resize(\"2000\") % % image_convert(colorspace = 'gray') % % image_trim() % % image_ocr() write.table(text, \"~/ocr-test/R.txt\") The above script first installs three packages to RStudio, Magick, Magrittr, and Tesseract. Magick processes the image as high quality; Magrittr uses the symbols % % as a pipe that forces the values of an expression into the next function (allowing our script to perform it's conversion in steps); Tesseract is the actual OCR engine that converts our image to text. Then the script loads each package. Lastly, the script processes the image, OCRs the text, and writes it to a txt file. Before installing any packages in the DH Box RStudio, we need to install some dependencies in the command line. The reason is that since DH Box runs an older version of RStudio, not everything installs as planned compared to the desktop RStudio. Type $ sudo apt-get install libcurl4-gnutls-dev in the command line to install the libcurl library (this installs RCurl). Type $ sudo apt-get install libmagick++-dev in the command line to install the libmagick library. Type $ sudo apt-get install libtesseract-dev in the command line to install the libtesseract library. Type $ sudo apt-get install libleptonica-dev in the command line to install the libleptonic library. Type $ sudo apt-get install tesseract-ocr-eng in the command line to install the English Tesseract library. Type $ sudo apt-get install libpoppler-cpp-dev in the command line to install the poppler cpp library. Navigate to RStudio and run each install.packages line in our script. This will take some time. Run each library() line to load the libraries. Run each line up to image_ocr() . This may take some time to complete. Run the last line write.table() to export the OCR to a text file with the same name. Navigate to your file manager and download both the output.txt file and the R.txt file. Compare the two text files in your desktop. How is the OCR in the command line versus within R? Note that they both use Tesseract just with different settings and in different environments.","title":"Converting images in R"},{"location":"module-2/Exercises/#progressively-converting-our-files-with-tesseract","text":"Now take a screen shot of both text files (just the text area) and name them output_1.png and R_1.png respectively. Upload both files into DH Box via the File Manager. In the command line, type $ tesseract output_1.png output_1.txt . In RStudio, change the file paths in your script to the following: text - image_read(\"~/ocr-test/R_1.png\") % % image_resize(\"2000\") % % image_convert(colorspace = 'gray') % % image_trim() % % image_ocr() write.table(text, \"~/ocr-test/R_1.txt\") Run each script line again. Except this time DO NOT run the install.packages() lines since we already installed them. Simply load the libraries again and run each line. Navigate to the File Manager and download ouput_1.txt and R_1.txt . Compare these two files. Did the OCR conversion get progressively worse? How do they compare to each other, to the first attempt at conversion, and then to the originals? Choose either the command line or the R method to convert more of the war diary files to text. Save these files into a new directory called war-diary-text . We will use these text files for future work in topic modeling and text analysis. How might your decision on which method to use change the results you would get in, say, a topic modeling tool? Hint: Check below for a way to automate the conversion process . Think about how these conversions can change based on the image being run through Tesseract. Does Tesseract have an easier time converting computer text even though it's in an image format? How might OCR conversions affect the way historians work on batch files? How does the context of the text change how historians analyse it? Look up the Tesseract wiki . What other options could you use with the Tesseract command to improve the results? When you decide to download Tesseract to you own computer, use the following two guides to automating bulk OCR (multiple files) with Tesseract: Peirson's and Schmidt's .","title":"Progressively converting our files with Tesseract"},{"location":"module-2/Exercises/#batch-converting-image-files","text":"Now that you've learned to convert image files to text individually, you should know that there is a quicker way to do this. The following script takes your Canadian war diary jpg image files and OCRs them using the same process as above. However this time, the function(i) ( i stands for iterate), goes through the folder for each jpg file and converts them to a png file into a text file with the suffix -ocr.txt until it notes there are no more image files to convert. The converted files will output to the same folder, in our case war-diary . library(magick) library(magrittr) library(pdftools) library(tesseract) dest - \"/war-diary\" myfiles - list.files(path = dest, pattern = \"jpg\", full.names = TRUE) # improve the images # ocr 'em # write the output to text file lapply(myfiles, function(i){ text - image_read(i) % % image_resize(\"3000x\") % % image_convert(type = 'Grayscale') % % image_trim(fuzz = 40) % % image_write(format = 'png', density = '300x300') % % tesseract::ocr() outfile - paste(i,\"-ocr.txt\",sep=\"\") cat(text, file=outfile, sep=\"\\n\") }) You can probably understand now why this method works much better than the previous R script: we automate our process (ie. we run the script just once and it iterates through the folder for us), append a suffix to the txt file name noting the OCRd text, and output it all to the same folder. Keep these files somewhere handy on your computer! We will use them throughout the course. By iterating through the folder, we have created a loop. Loops can be a very powerful tool for Digital Historians, as we can begin to automate processes that would take much longer by hand. Read more about 'for loops' on the R-blogger website . NB The above script processes dozens of images and may take quite a bit of time to complete.","title":"Batch converting image files"},{"location":"module-2/Exercises/#reference","text":"Part of this tutorial was adapted from The Programming Historian released under the CC-BY license, including: Ian Milligan, \"Automated Downloading with Wget,\" The Programming Historian 1 (2012), https://programminghistorian.org/lessons/automated-downloading-with-wget . Kellen Kurschinski, \"Applied Archival Downloading with Wget,\" The Programming Historian 2 (2013), https://programminghistorian.org/lessons/applied-archival-downloading-with-wget . Twitter user 'superboreen' on R OCR .","title":"Reference"},{"location":"module-2/Finding Data/","text":"How do we find data, anyway? May 21-28, 2018 Concepts Something given. That's a nice way of thinking about it. Of course, much of the data that we are 'given' wasn't really given willingly . When we topic model Martha Ballard's diary , did she give this to us? Of course, she couldn't have imagined what we might try to do to it. Other kinds of data - census data, for instance - were compelled: folks had to answer the questions, on pain of punishment. This is all to suggest that there is a moral dimension to what we do with big data in history. Stop for a moment and read The Joys of Big Data (if you haven't already) and then The Third Wave of Computational History . Digitized data is not value-neutral; we need to think about, and talk about, what it means to collect, transform, analyze and visualize it. Who has the power here? (and you might also reflect on 'the most profitable obsolete technology' ) Finally, you might also think about recent history - listen to Ian Milligan discuss how Yahoo's closure of Geocities represented a terrible blow to social history . Accepting that historical 'big' data is out there, that there's more material than one person can usefully digest and understand, and that a big-picture, macroscopic point of view is a useful perspective, means also thinking about the digital milieu that makes this possible. But see this piece by Tim Sherratt on Seams and edges: Dreams of aggregation, access discovery in a broken world . We interact with the data we find, and in the process, we alter both it and ourselves! As you do your projects and work through this workbook, think about the ethical, moral, and legal dimensions to what you are doing. Always keep track of your thoughts in your notebook. Remember to put them up in your GitHub repo. Finding big data So how can we find big data? The exercises in this module will teach you how some historical materials get online, and the work involved in doing that. They will show you how to use wget on the command line to grab webpages; and they will introduce you to the concept of APIs and what you might achieve with them as a historian. Additional exercises show you how to use some existing free and commercial tools for webscraping; and we will also learn how to grab social media data as well. For future references consult this list of historical data sources . You should also perhaps dip into the 'Data Fundamentals' part of Data + Design (PDF opens in new window) ; think of it as another excellent textbook to help you when you need another perspective on the materials in this course. And don't forget serendipity Follow researchers and institutions in your field of study. Once on Twitter I saw something that struck me as an excellent find. Penn Libraries tweeted, and I retweeted, a link to a traveller's diary from the 19th century - a woman who sailed from the US to Europe and thence the Nile, which she ascended and explored. Tweeting about it led to a flurry of activity amongst scholars, and even now, the transcription has begun. Indeed, I made an Android-only game out of it . But first... let's set a bit of framework. If we're going to find data, we need to be able to access the power of our machines, to get them to do what we want. It's worth thinking about what Corey Doctorow has called the war on general purpose computing as we begin... ...and then thinking about what 'search' actually means. Check out Ted Underwood's piece on 'Theorizing Research Practices We Forgot to Theorize Twenty Years Ago' . Finally, Cameron Blevins has some thoughts on the 'perpetual sunrise of methodology' . What you need to do this week Respond to the readings and the reading questions through annotation (taking care to respond to others' annotations as well) - see the instructions below. Remember to tag your annotations with 'hist3814o' so that we can find them on the course Hypothes.is group . Remember to annotate using our HIST3814o group . Do the exercises for this module, pushing yourself as far as you can. Annotate the instructions where they might be unclear or confusing; see if others have annotated them as well, and respond to them with help if you can. Keep an eye on our Slack channel - you can always offer help or seek out help there. Write a blog post describing what happened as you went through the exercises (your successes, your failures, the help you may have found/received), and link to your 'faillog' (ie. the notes you upload to your GitHub account - for more on that, see the exercises!). Submit your work to the course submission form . Readings This week, I want you to choose just one of the articles linked to above to do your 'official' annotations on OR annotate one of these two articles regarding the 'Transcribing Bentham' project: Causer Wallace, Building A Volunteer Community: Results and Findings from Transcribe Bentham DHQ 6.2, 2012 Causer, Tonra, Wallace Transcription maximized; expense minimized? Crowdsourcing and editing The Collected Works of Jeremy Bentham LLC 27.2, 2012 Again, I also want you to respond to at least one substantive annotation made by your peers. Remember, with Hypothes.is you can annotate pdfs that you have opened in your browser from a website. Reading questions : Have you ever sat down with one of the librarians to get help finding something? Consider the knowledge and labour involved not just with finding materials, but in making materials findable in the first place. Make an entry in your blog that reflects on these questions in the light of your annotations.","title":"How do we find data?"},{"location":"module-2/Finding Data/#how-do-we-find-data-anyway-may-21-28-2018","text":"","title":"How do we find data, anyway? &mdash; May 21-28, 2018"},{"location":"module-2/Finding Data/#concepts","text":"Something given. That's a nice way of thinking about it. Of course, much of the data that we are 'given' wasn't really given willingly . When we topic model Martha Ballard's diary , did she give this to us? Of course, she couldn't have imagined what we might try to do to it. Other kinds of data - census data, for instance - were compelled: folks had to answer the questions, on pain of punishment. This is all to suggest that there is a moral dimension to what we do with big data in history. Stop for a moment and read The Joys of Big Data (if you haven't already) and then The Third Wave of Computational History . Digitized data is not value-neutral; we need to think about, and talk about, what it means to collect, transform, analyze and visualize it. Who has the power here? (and you might also reflect on 'the most profitable obsolete technology' ) Finally, you might also think about recent history - listen to Ian Milligan discuss how Yahoo's closure of Geocities represented a terrible blow to social history . Accepting that historical 'big' data is out there, that there's more material than one person can usefully digest and understand, and that a big-picture, macroscopic point of view is a useful perspective, means also thinking about the digital milieu that makes this possible. But see this piece by Tim Sherratt on Seams and edges: Dreams of aggregation, access discovery in a broken world . We interact with the data we find, and in the process, we alter both it and ourselves! As you do your projects and work through this workbook, think about the ethical, moral, and legal dimensions to what you are doing. Always keep track of your thoughts in your notebook. Remember to put them up in your GitHub repo.","title":"Concepts"},{"location":"module-2/Finding Data/#finding-big-data","text":"So how can we find big data? The exercises in this module will teach you how some historical materials get online, and the work involved in doing that. They will show you how to use wget on the command line to grab webpages; and they will introduce you to the concept of APIs and what you might achieve with them as a historian. Additional exercises show you how to use some existing free and commercial tools for webscraping; and we will also learn how to grab social media data as well. For future references consult this list of historical data sources . You should also perhaps dip into the 'Data Fundamentals' part of Data + Design (PDF opens in new window) ; think of it as another excellent textbook to help you when you need another perspective on the materials in this course.","title":"Finding big data"},{"location":"module-2/Finding Data/#and-dont-forget-serendipity","text":"Follow researchers and institutions in your field of study. Once on Twitter I saw something that struck me as an excellent find. Penn Libraries tweeted, and I retweeted, a link to a traveller's diary from the 19th century - a woman who sailed from the US to Europe and thence the Nile, which she ascended and explored. Tweeting about it led to a flurry of activity amongst scholars, and even now, the transcription has begun. Indeed, I made an Android-only game out of it .","title":"And don't forget serendipity"},{"location":"module-2/Finding Data/#but-first-lets-set-a-bit-of-framework","text":"If we're going to find data, we need to be able to access the power of our machines, to get them to do what we want. It's worth thinking about what Corey Doctorow has called the war on general purpose computing as we begin... ...and then thinking about what 'search' actually means. Check out Ted Underwood's piece on 'Theorizing Research Practices We Forgot to Theorize Twenty Years Ago' . Finally, Cameron Blevins has some thoughts on the 'perpetual sunrise of methodology' .","title":"But first... let's set a bit of framework."},{"location":"module-2/Finding Data/#what-you-need-to-do-this-week","text":"Respond to the readings and the reading questions through annotation (taking care to respond to others' annotations as well) - see the instructions below. Remember to tag your annotations with 'hist3814o' so that we can find them on the course Hypothes.is group . Remember to annotate using our HIST3814o group . Do the exercises for this module, pushing yourself as far as you can. Annotate the instructions where they might be unclear or confusing; see if others have annotated them as well, and respond to them with help if you can. Keep an eye on our Slack channel - you can always offer help or seek out help there. Write a blog post describing what happened as you went through the exercises (your successes, your failures, the help you may have found/received), and link to your 'faillog' (ie. the notes you upload to your GitHub account - for more on that, see the exercises!). Submit your work to the course submission form .","title":"What you need to do this week"},{"location":"module-2/Finding Data/#readings","text":"This week, I want you to choose just one of the articles linked to above to do your 'official' annotations on OR annotate one of these two articles regarding the 'Transcribing Bentham' project: Causer Wallace, Building A Volunteer Community: Results and Findings from Transcribe Bentham DHQ 6.2, 2012 Causer, Tonra, Wallace Transcription maximized; expense minimized? Crowdsourcing and editing The Collected Works of Jeremy Bentham LLC 27.2, 2012 Again, I also want you to respond to at least one substantive annotation made by your peers. Remember, with Hypothes.is you can annotate pdfs that you have opened in your browser from a website. Reading questions : Have you ever sat down with one of the librarians to get help finding something? Consider the knowledge and labour involved not just with finding materials, but in making materials findable in the first place. Make an entry in your blog that reflects on these questions in the light of your annotations.","title":"Readings"},{"location":"module-3/Exercises/","text":"Module 3 Exercises The exercises in this module cover: Cleaning text with regular expressions Cleaning files with Open Refine Exercise 1: Regular Expressions When we have text that has been marked up, we can do interesting things with it. In the previous module, you saw that XSL can be used to add styles to your XML (which a browser can interpret and display). You can see how having those tags makes for easier searching for information as well, right? Sometimes things are not well marked up. Sometimes, it's all a real mess. In this exercise, we'll explore 'regular expressions', (AKA 'Regex') or ways of asking the computer to search for patterns rather than matching exact text. This exercise follows a tutorial written for The Macroscope . Another good reference is the regular expressions website . You should open RegeXr , an interactive tutorial that shows us what various regular expressions can do, in a browser window so you can test your regular expressions out before applying them to your data! We will use the power of regular expressions to search for particular patterns in a published volume of the correspondence of the Republic of Texas. We'll use regex to massage the information into a format that we can then use to generate a social network of letter writers over time. (You might also want to try the Programming Historian tutorial Understanding Regular Expressions .) What if you had a lot of documents that you needed to clean up? One way of doing it would be to write a Python program that applied your regex automatically, across all the files in a folder. Optional advanced exercise : Cleaning OCR'd Text with Regular Expressions . For this exercise, do the following: Start with a gentle introduction to regex . Then begin the regex exercise . Remember to copy your history file and any other information, observations, or thoughts to your GitHub repo. Exercise 2: Open Refine Open Refine is the final tool we'll explore in this module. This engine allows us to clean up our messy data. We will feed it the results from Exercise 2 in order to consolidate individuals (ie. 'Shawn' and 'S4awn' are probably the same person, so Open Refine will consolidate that information for us). This exercise also follows a tutorial originally written for The Macroscope . Open Refine does not run in DH Box, so use the File Manager in DH Box to move your cleaned-correspondence file to somewhere safe on your computer. For this exercise, do the following: The Open Refine exercise in our supporting materials . For more advanced usage of Open Refine, as an optional exercise you can also try The Programming Historian's Tutorial on Open Refine . Remember to copy your notes and any other information/observations/thoughts to your GitHub repo. Capstone Exercise You can use what you've been learning here to do some clean-up on the Canadian war diary files (or a subset of them, of course, as suits your interests you have explored them, haven't you?). Google for sed patterns and see if you can combine what you find with what you've learned in this module in order to clean up the text. For instance, a common error in OCR is to confuse the letter i with the letter l and the numeral 1 . Shawville becomes Shawvllle. You could use grep to see if that error is present, and then sed to correct it. The file names are long, and there are several hundred; you might give this kind of thing a try too: $ find . -type f -exec sed -i.bak \"s/foo/bar/g\" {} \\; This command finds all files in a folder and creates a backup for each one in turn before searching for foo and replacing it with bar . The command $ grep \"[b-df-hj-np-tv-xz]\\{5\\}\" filename will find all instances of strings of five consonants or more, which can be useful to give you an idea of what kinds of sed patterns to write. Maybe, if you inspect the PDF and the txt files together, you can figure out patterns that set off interesting things in say the classified ads or the editorials and then write some grep and sed to create new files with just that information. Then you could use Open Refine to further clean things up. Maybe the messiness of the data is exactly the point ( and my workup on bad OCR ) you want to explore. Nevertheless: Cleaning data is 80% of the work in digital history. Remember to copy your notes and any other information/observations/thoughts to your GitHub repo. Going Further See some of the suggestions at the end of the Open Refine exercise . The Stanford Named Entity Recognizer is a program that enables you to automatically tag words in your corpus according to whether or not they are place names, individuals, and so on. The output can then be subsequently extracted and visualized. Try the NER exercise in our supporting materials . Counting and Mining Data with Unix","title":"Exercises"},{"location":"module-3/Exercises/#module-3-exercises","text":"The exercises in this module cover: Cleaning text with regular expressions Cleaning files with Open Refine","title":"Module 3 Exercises"},{"location":"module-3/Exercises/#exercise-1-regular-expressions","text":"When we have text that has been marked up, we can do interesting things with it. In the previous module, you saw that XSL can be used to add styles to your XML (which a browser can interpret and display). You can see how having those tags makes for easier searching for information as well, right? Sometimes things are not well marked up. Sometimes, it's all a real mess. In this exercise, we'll explore 'regular expressions', (AKA 'Regex') or ways of asking the computer to search for patterns rather than matching exact text. This exercise follows a tutorial written for The Macroscope . Another good reference is the regular expressions website . You should open RegeXr , an interactive tutorial that shows us what various regular expressions can do, in a browser window so you can test your regular expressions out before applying them to your data! We will use the power of regular expressions to search for particular patterns in a published volume of the correspondence of the Republic of Texas. We'll use regex to massage the information into a format that we can then use to generate a social network of letter writers over time. (You might also want to try the Programming Historian tutorial Understanding Regular Expressions .) What if you had a lot of documents that you needed to clean up? One way of doing it would be to write a Python program that applied your regex automatically, across all the files in a folder. Optional advanced exercise : Cleaning OCR'd Text with Regular Expressions . For this exercise, do the following: Start with a gentle introduction to regex . Then begin the regex exercise . Remember to copy your history file and any other information, observations, or thoughts to your GitHub repo.","title":"Exercise 1: Regular Expressions"},{"location":"module-3/Exercises/#exercise-2-open-refine","text":"Open Refine is the final tool we'll explore in this module. This engine allows us to clean up our messy data. We will feed it the results from Exercise 2 in order to consolidate individuals (ie. 'Shawn' and 'S4awn' are probably the same person, so Open Refine will consolidate that information for us). This exercise also follows a tutorial originally written for The Macroscope . Open Refine does not run in DH Box, so use the File Manager in DH Box to move your cleaned-correspondence file to somewhere safe on your computer. For this exercise, do the following: The Open Refine exercise in our supporting materials . For more advanced usage of Open Refine, as an optional exercise you can also try The Programming Historian's Tutorial on Open Refine . Remember to copy your notes and any other information/observations/thoughts to your GitHub repo.","title":"Exercise 2: Open Refine"},{"location":"module-3/Exercises/#capstone-exercise","text":"You can use what you've been learning here to do some clean-up on the Canadian war diary files (or a subset of them, of course, as suits your interests you have explored them, haven't you?). Google for sed patterns and see if you can combine what you find with what you've learned in this module in order to clean up the text. For instance, a common error in OCR is to confuse the letter i with the letter l and the numeral 1 . Shawville becomes Shawvllle. You could use grep to see if that error is present, and then sed to correct it. The file names are long, and there are several hundred; you might give this kind of thing a try too: $ find . -type f -exec sed -i.bak \"s/foo/bar/g\" {} \\; This command finds all files in a folder and creates a backup for each one in turn before searching for foo and replacing it with bar . The command $ grep \"[b-df-hj-np-tv-xz]\\{5\\}\" filename will find all instances of strings of five consonants or more, which can be useful to give you an idea of what kinds of sed patterns to write. Maybe, if you inspect the PDF and the txt files together, you can figure out patterns that set off interesting things in say the classified ads or the editorials and then write some grep and sed to create new files with just that information. Then you could use Open Refine to further clean things up. Maybe the messiness of the data is exactly the point ( and my workup on bad OCR ) you want to explore. Nevertheless: Cleaning data is 80% of the work in digital history. Remember to copy your notes and any other information/observations/thoughts to your GitHub repo.","title":"Capstone Exercise"},{"location":"module-3/Exercises/#going-further","text":"See some of the suggestions at the end of the Open Refine exercise . The Stanford Named Entity Recognizer is a program that enables you to automatically tag words in your corpus according to whether or not they are place names, individuals, and so on. The output can then be subsequently extracted and visualized. Try the NER exercise in our supporting materials . Counting and Mining Data with Unix","title":"Going Further"},{"location":"module-3/Wrangling Data/","text":"Wrangling Data May 28 - June 4, 2018 Concepts In the previous module, we successfully grabbed a lot of data from various online repositories. Some of it was already in well-structured tables; much of it was not. All of it was text though. Initially, it (or most of it) was just scanned images of documents. At some point, object character recognition was used to identify the black dots from the white dots in those images, to recognize the patterns that make up letters, numbers, and punctuation. There are commercial products that can do this (and we have some installed in the Underhill Research Room that you can use), and there are free products that you can install on your computer to do it yourself. It all looks so neat and tidy. Ian Milligan discusses this 'illusionary order' and its implications for historians (the previous link will not work unless you are connected to Carleton's VPN): In this article, I make two arguments. Firstly, online historical databases have profoundly shaped Canadian historiography. In a shift that is rarely \u2013 if ever \u2013 made explicit, Canadian historians have profoundly reacted to the availability of online databases. Secondly, historians need to understand how OCR works, in order to bring a level of methodological rigor to their work that use these sources. Just as we saw with Ted Underwood's article on theorizing search , these 'simple' steps in the research process are anything but. They are also profoundly theoretical in how they change what it is we can know. In archaeology, every step of the method, every stage in the process, has a profound impact on the stories we eventually tell about the past. Decisions we make destroy data, and create new data. Historians aren't used to thinking about these kinds of issues! There are also manual ways of doing the same thing as OCR does - we call these things 'humans', and we organize their work through 'crowdsourcing'. We break the process up into wee manageable steps, and make these available over the net. Sometimes we gamify these steps, to make them more 'fun'. If several people all work on the same piece of text, the thinking is that errors will cancel each other out: a proper transcription will emerge from the work of the crowd. While transcriptions might've provided the earliest examples of crowdsourcing research (but read also The HeritageCrowd Project and the subsequent 'How I Lost the Crowd' ), other tasks are now finding their way into the crowdsourced world - see the archaeological applications within the MicroPasts platform. These include things like 'masking' artefact photographs in order to develop 3d photogrammetric models. But often, we don't have a whole crowd. We're just one person, alone, with a computer, at the archive. Or working with someone else's digitized image that we found online . How do we wrangle that data? Let's start with M. H. Beal's account of how she 'xml'd her way to data management' and then consider a few more of the nuts and bolts of her work in OA TEI-XML DH on the WWW; or, My Guide to Acronymic Success . This kind of work is extraordinarily important! You already had a taste of it in the TEI exercise in the last module. (Now, if we had a seriously big project where we were transcribing lots of text, we'd invest in a dedicated XML editor like Oxygen - there are plugins available and frameworks for doing historical transcription on this platform. There is a 30 day free trial license if you want to give it a try. But for now, Notepad++, Textwrangler, Komodo Edit, Sublime text, or any of a number of good text editors will do all that we need to do). Also, check out the TEI . Take 15 minutes and read through What is XML and Why Should Humanists Care? by David Birnbaum. Annotate! If you didn't do the TEI exercise in Module 2, you could pause right now and give that a try - or at least, read it over and talk to some of your peers who DID do it. In this module we're going to do some other kinds of wrangling. Exercises In the exercises for this week we are going to focus on some bare-bones wrangling of data. First, we are going to do some activities and exercises to get in the right frame of mind. Then, we'll switch gears and we'll use regular expressions to search and extract information from the Diplomatic Correspondence of the Republic of Texas, which you'll find at the Internet Archive. We'll conclude by using 'Open Refine' to tidy up the information we extracted from the Texan correspondence. Things you will learn in this module: The power of regular expressions. Search and Replace in Word just won't cut it any more for you! (Another reason why you should write in Markdown in the first place and then convert to Word for the final typesetting.) Open Refine as a powerful engine for tidying up the messiness that is OCR'd text. What you need to do this week Respond to the readings and the reading questions through annotation (taking care to respond to others' annotations as well) - see the instructions below. Remember to tag your annotations with 'hist3814o' so that we can find them on the course Hypothes.is group . Remember to annotate using our HIST3814o group . Do the exercises for this module, pushing yourself as far as you can. Annotate the instructions where they might be unclear or confusing; see if others have annotated them as well, and respond to them with help if you can. Keep an eye on our Slack channel - you can always offer help or seek out help there. Write a blog post describing what happened as you went through the exercises (your successes, your failures, the help you may have found/received), and link to your 'faillog' (ie. the notes you upload to your GitHub account - for more on that, see the exercises!). Submit your work to the course submission form . Readings Select one of the articles behind the links above OR select one of the articles below to annotate. Blevins, Mining and Mapping the Production of Space A View of the World from Houston Blevins, Space, Nation, and the Triumph of Region: A View of the World from Houston Ian Milligan on Imageplot and images in web archives Ryan Cordell, Qitjb-the-raven Reading questions : On your blog, reflect on any data cleaning you've had to do in other classes. Why don't historians discuss this kind of work? What gets hidden, what gets lost, how is the ultimate argument weaker as a result? Or does it matter? Make reference (or link to) key annotations, whether by you or one of your peers, to support your points.","title":"Data is messy"},{"location":"module-3/Wrangling Data/#wrangling-data-may-28-june-4-2018","text":"","title":"Wrangling Data &mdash; May 28 - June 4, 2018"},{"location":"module-3/Wrangling Data/#concepts","text":"In the previous module, we successfully grabbed a lot of data from various online repositories. Some of it was already in well-structured tables; much of it was not. All of it was text though. Initially, it (or most of it) was just scanned images of documents. At some point, object character recognition was used to identify the black dots from the white dots in those images, to recognize the patterns that make up letters, numbers, and punctuation. There are commercial products that can do this (and we have some installed in the Underhill Research Room that you can use), and there are free products that you can install on your computer to do it yourself. It all looks so neat and tidy. Ian Milligan discusses this 'illusionary order' and its implications for historians (the previous link will not work unless you are connected to Carleton's VPN): In this article, I make two arguments. Firstly, online historical databases have profoundly shaped Canadian historiography. In a shift that is rarely \u2013 if ever \u2013 made explicit, Canadian historians have profoundly reacted to the availability of online databases. Secondly, historians need to understand how OCR works, in order to bring a level of methodological rigor to their work that use these sources. Just as we saw with Ted Underwood's article on theorizing search , these 'simple' steps in the research process are anything but. They are also profoundly theoretical in how they change what it is we can know. In archaeology, every step of the method, every stage in the process, has a profound impact on the stories we eventually tell about the past. Decisions we make destroy data, and create new data. Historians aren't used to thinking about these kinds of issues! There are also manual ways of doing the same thing as OCR does - we call these things 'humans', and we organize their work through 'crowdsourcing'. We break the process up into wee manageable steps, and make these available over the net. Sometimes we gamify these steps, to make them more 'fun'. If several people all work on the same piece of text, the thinking is that errors will cancel each other out: a proper transcription will emerge from the work of the crowd. While transcriptions might've provided the earliest examples of crowdsourcing research (but read also The HeritageCrowd Project and the subsequent 'How I Lost the Crowd' ), other tasks are now finding their way into the crowdsourced world - see the archaeological applications within the MicroPasts platform. These include things like 'masking' artefact photographs in order to develop 3d photogrammetric models. But often, we don't have a whole crowd. We're just one person, alone, with a computer, at the archive. Or working with someone else's digitized image that we found online . How do we wrangle that data? Let's start with M. H. Beal's account of how she 'xml'd her way to data management' and then consider a few more of the nuts and bolts of her work in OA TEI-XML DH on the WWW; or, My Guide to Acronymic Success . This kind of work is extraordinarily important! You already had a taste of it in the TEI exercise in the last module. (Now, if we had a seriously big project where we were transcribing lots of text, we'd invest in a dedicated XML editor like Oxygen - there are plugins available and frameworks for doing historical transcription on this platform. There is a 30 day free trial license if you want to give it a try. But for now, Notepad++, Textwrangler, Komodo Edit, Sublime text, or any of a number of good text editors will do all that we need to do). Also, check out the TEI . Take 15 minutes and read through What is XML and Why Should Humanists Care? by David Birnbaum. Annotate! If you didn't do the TEI exercise in Module 2, you could pause right now and give that a try - or at least, read it over and talk to some of your peers who DID do it. In this module we're going to do some other kinds of wrangling.","title":"Concepts"},{"location":"module-3/Wrangling Data/#exercises","text":"In the exercises for this week we are going to focus on some bare-bones wrangling of data. First, we are going to do some activities and exercises to get in the right frame of mind. Then, we'll switch gears and we'll use regular expressions to search and extract information from the Diplomatic Correspondence of the Republic of Texas, which you'll find at the Internet Archive. We'll conclude by using 'Open Refine' to tidy up the information we extracted from the Texan correspondence.","title":"Exercises"},{"location":"module-3/Wrangling Data/#things-you-will-learn-in-this-module","text":"The power of regular expressions. Search and Replace in Word just won't cut it any more for you! (Another reason why you should write in Markdown in the first place and then convert to Word for the final typesetting.) Open Refine as a powerful engine for tidying up the messiness that is OCR'd text.","title":"Things you will learn in this module:"},{"location":"module-3/Wrangling Data/#what-you-need-to-do-this-week","text":"Respond to the readings and the reading questions through annotation (taking care to respond to others' annotations as well) - see the instructions below. Remember to tag your annotations with 'hist3814o' so that we can find them on the course Hypothes.is group . Remember to annotate using our HIST3814o group . Do the exercises for this module, pushing yourself as far as you can. Annotate the instructions where they might be unclear or confusing; see if others have annotated them as well, and respond to them with help if you can. Keep an eye on our Slack channel - you can always offer help or seek out help there. Write a blog post describing what happened as you went through the exercises (your successes, your failures, the help you may have found/received), and link to your 'faillog' (ie. the notes you upload to your GitHub account - for more on that, see the exercises!). Submit your work to the course submission form .","title":"What you need to do this week"},{"location":"module-3/Wrangling Data/#readings","text":"Select one of the articles behind the links above OR select one of the articles below to annotate. Blevins, Mining and Mapping the Production of Space A View of the World from Houston Blevins, Space, Nation, and the Triumph of Region: A View of the World from Houston Ian Milligan on Imageplot and images in web archives Ryan Cordell, Qitjb-the-raven Reading questions : On your blog, reflect on any data cleaning you've had to do in other classes. Why don't historians discuss this kind of work? What gets hidden, what gets lost, how is the ultimate argument weaker as a result? Or does it matter? Make reference (or link to) key annotations, whether by you or one of your peers, to support your points.","title":"Readings"},{"location":"module-4/Exercises/","text":"Module 4 Exercises There are many different tools and approaches you could use to visualize your data, both as a preliminary pass to spot the holes and also for more formal analysis. In which case, for this module, I would like you to select two of these exercises which seem most relevant to your capstone exercise. You are welcome to work through more of them, of course, but I want the exercises to move your own research forward. Some of these I wrote and some are adapted from The Macroscope ; others are adapted or used holus-bolus from scholars like Miriam Posner , Fred Gibbs , and Heather Froehlich (and I'm grateful that they shared their materials!). Finally, you are welcome to explore the lessons and tutorials at The Programming Historian if they seem appropriate to what you want to do for the Capstone Exercise. But what if I haven't any idea of what to do for the capstone exercise? Then read through the various tutorials for inspiration. Find something that strikes you as interesting, and then talk to me about how you might employ the ideas or concepts with regard to the Canadian war diary data. Things to install? Many of these involve having to install more software on your own machine. In those exercises that involve using R and RStudio, you are welcome to install RStudio on your own machine OR to use it in DH Box. Please read this quick introduction to R and Rstudio carefully . If you decide to install R and RStudio on your own machine, I would suggest you read the introductory bits from Lincoln Mullen's book-in-progress, Computational Historical Thinking , especially the 'setup' part under 'getting started' (pay attention to the bit on installing packages and dependencies). If you spot any exercises in Mullen's book that seem relevant to the Capstone Exercise, you may do those as an alternative to the ones here. Alternatively , go to Swirl and learn the basics of R within Swirl . DHNow links to a new Basic Text Mining in R tutorial which is worth checking out as well. NB It is always very important to record in your own notebooks what version of R you used for your analysis, what version of any R packages you installed and used, and so on because packages can go out of date. In the table below I've gathered the exercises together under the headings of Text , Networks , Maps , and Charts . I've also added some entries that I am categorizing under Art . The first is a series of exercises on the sonification of data and the second is a guide to making Twitterbots ; the third is about glitching digital imagery. These approaches can provide surprising and novel insights into history, as they move from representing history digitally to performing it. Visit, for instance, the final project in an undergraduate digital history class at the University of Saskatchewan by Daniel Ruten . Daniel translated simple word clouds of a First World War diary into a profound auditory performance. I would be very interested indeed to see if any capstone exercises in HIST3814o gave sonification or Twitterbots or glitching a try. The exercises in this module are covered in the chart below: Texts Networks Maps Charts Art Topic Modeling Tool Network analysis in Gephi Simple mapping georectifying Quick charts using RAW Sonification Topic Modeling in R Network Analysis in R QGIS (tutorials by Fred Gibbs) Twitterbots Text Analysis with OverviewProject Network Analysis in Cytoscape Geoparsing with Python Glitching Photos Corpus Linguistics with AntConc Choose your own adventure Palladio with Posner Text Analysis with Voyant Using igraph to visualize network data Leaflet.js Maps Identifying Similar Images with TensorFlow Exercise 1: Network Visualization This exercise uses the open source program Gephi which you install on your own computer. If you'd rather not install anything, please see Network Analysis in R instead. Recall that the index of the collected letters of the Republic of Texas was just a list of letters from so-and-so to so-and-so. We haven't looked at the content of those letters, but the shape of network the meta data of that correspondence can be revealing (remember Paul Revere! ) When we stitch that together into a network of people connected because they exchanged letters, we end up with a shard of their social network. Networks can be queried for things like power, position, and role, and so used judiciously, we can begin to suss something of the social structures in which their history took place. I would recommend that you also take a long look at Scott Weingart's series, Networks Demystified . Finally, heed our warning . For this exercise, do the following: Transform your Texan Correspondence data into a network, which you will then visualize with the open source program Gephi . The detailed instructions are in our supporting materials . Going Further The data you get from topic modeling is in many ways just the first step to making it meaningful. Read through Miram Posner's article, Very basic strategies for interpreting results from the Topic Modeling Tool . In this article, Posner explores the results of your data, what the different files mean, and how you can interpret that data. Posner also shows in another article how you can visualize your results from the Topic Modeling Tool . While visualization is just one method to interpret your data, it is a powerful way to begin to make some meaningful insights into the data. Exercise 2: Topic Modeling Tool In this exercise you will use the Topic Modeling Tool to create a simple topic model and a webpage that allows you to browse the results. Download the Topic Modeling Tool from GitHub . Make sure you have some content on your own machine; the Colonial Newspaper Database is a handy corpus. (Created by Melodee Beals, it's a series of late 18th, early 19th century cleanly transcribed newspaper articles from Scotland and Northern England; You can grab my copy from GitHub ). Or perhaps you might move your copy of the Canadian war diary out of DH Box onto your computer. At the command prompt in DH Box, type $ ls to make sure you can see your Canadian war diary text folder (ie. you can't zip a folder from the command line if you are in that folder, so $ cd out of it if necessary). Assuming your files are in war-diary-text , zip the folder up with this command, $ zip -r wardiaryfiles.zip war-diary-text . Use the File Manager to download the zip file. Unzip the folder on your machine. Double-click on the file you downloaded in Step 1. This will open a java-based graphical user interface with one of the most common topic modeling approaches, 'Latent Dirichlet Allocation'. Set the input to be the Colonial Newspaper Database or the Canadian war diary. Set the output to be somewhere neat and tidy on your computer. Set the number of topics you'd like to model. Click 'train topics' to run the algorithm. When it finishes, go to the folder you selected for output, and find the file all_topics.html in the output_html folder. Click on all_topics.html . You now have a browser-based way of navigating your topics and documents. In the output_csv folder created, you will find the same information as CSV, which you could then input into a spreadsheet for other kinds of visualizations (which we'll talk about in class). Make a note in your open notebook about your process and your observations. How does reading this material in this way change/challenge/or focus your understanding of the material? Exercise 3: Topic Modeling in R Exercise 2 was quite a simple way to do topic modeling. In this exercise, we are going to use a package for the R statistical language called 'Mallet' to do our topic modeling. One way isn't necessarily better than the other, although doing our analysis within R allows the potential for extending the analysis or combining it with other data. First, read this introduction to R so what follows isn't a complete shock! For this exercise, do ONE of the following: Guidance for doing this in RStudio in the DH Box Guidance for doing this in RStudio installed on your own computer Exercise 4: Text Analysis with Overview In this exercise, we're going to look at the Colonial Newspaper Database again, but this time using a tool called 'Overview'. Overview uses a different approach than the topic models we've been discussing. In essence, it looks at word frequencies and their distributions within a document, and within a corpus, to organize the documents into folders of progressively similar word use. You can download Overview to run on your own machine, but for our purposes, the hosted version on the Overview Docs website is sufficient. Go to that page, watch the video, create an account, and then log in ( you must create an account to use Overview ). (More help about how Overview works may be found on their blog , including helpful videos.) Once you're inside, click 'import from a CSV file', and upload the CND.csv (which you can download and save to your own machine from my GitHub - right-click and save as the previous link). On the 'UPLOAD A CSV FILE' page in Overview click 'browse' and select the CND.csv . It will give you a preview. There are a number of options here you can tell Overview which words to ignore, and which words to give added importance to. What words will you select? Make a note in your notebook. Hit 'upload'. A new page appears, called YOUR DOCUMENT SETS . Click on the one you just uploaded. A file folder tree showing documents of progressively greater similarity will open; on the right side will be the list of documents within each box (the box in question will be greyed out when you click on it, so you know where you are). You can search for words in your document, and Overview will tell you where they are; you can tag documents that you find interesting. The Overview system allows you to jump between a distant, macroscopic view and a close, document level view. Jump back and forth, see what you can find. For suggestions about how to use Overview effectively, try their blog . Make notes about what you observe in your notebook. Also, you can export your tagged document set from Overview, so that you could visualize the patterns of tagging in a spreadsheet (for instance). Going further: Do you see how you could upload your documents that you collected during Module 2? Exercise 5: Corpus Linguistics with AntConc Heather Froelich has put together an excellent step-by-step with using AntConc for exploring textual patterns within, and across, corpora of texts. Work your way through her tutorial . Can you get our example materials (from the Colonial Newspaper Database) into AntConc? Our instructions in The Macroscope might help you to split the CSV into individual txt files. Alternatively, do you have any materials of your own, already collected? Feed them into AntConc. What patterns do you see? What if you compare your materials against other corpora of texts? For your information, CoRD has a collection of corpora that you can explore . Exercise 6: Text Analysis with Voyant In Module 2 , if you recall, we worked through how to transform XML using stylesheets. Melodee Beals used a stylesheet to transform her database into a series of individual txt files. In the exercises above, a transformer was used to make the database into a single CSV file. In this exercise, we are going to use Voyant Tools to visualize patterns of word use in the database. Voyant can read either a CSV or text files. The advantage of uploading a folder of text files is that, if the files are in chronological order, Voyant's default visualizations will also be arranged in chronological order and thus we can see change over time. Go to Voyant Tools . Paste the following URL to the CSV of the CND database: https://raw.githubusercontent.com/shawngraham/exercise/gh-pages/CND.csv . Now, open a new browser window, and go to the colonial newspaper file on Voyant tools . Do you see the difference? In the latter window, the individual articles have been uploaded individually, and thus are treated as individual documents in chronological order. Explore the corpus, comparing terms over time, looking at keywords in context, and using the RezoViz tool to create a graph where people, places, and organizations that appear in the same documents (and across documents) are connected (you can find 'rezoviz' under the cogwheel icon at the top right of the panel). Google these terms and tools for what they mean and how others have used them. You can embed any of the tools in your blogs by using the 'save' icon and getting the iframe or embed code. You can apply stopwords by clicking on the cogwheel in any of the different tools, and selecting stopwords. Apply the stopwords globally, and you'll only have to do this once! What patterns do you see? What do different tools highlight? Which ones are useful? What patterns do you see that strike you as interesting? Note this all down. Going further: Upload materials you collected in Module 2 and explore them. Exercise 7: RAW Quick Charts Using RAW A quick chart can be a handy thing to have. Google spreadsheets, Microsoft Excel, and a host of other programs can make excellent charts quickly with their wizard functions. Never hesitate to turn to these. However, they are not always good with non-numeric data. In Module 3 , you used the Stanford Named Entity Recognizer to extract place names from a text. After some further munging with regex, you might have ended up with a CSV that looks like the Texas CSV . Can we do a quick visualization of this information? One useful tool is RAW . Open RAW in a new window. Copy the table of data of places mentioned in the Texan correspondence, and paste it into the data input box at the top of the RAW screen. Oh no, an error! A quick data munge You should get an error message, to the effect that you need to check 'line 2'. What's gone wrong? RAW has checked the number of values you have in that row, and compared it to the number of columns in row 1 (which contains all the column names). It sees that the two don't match. What we need to do is add a default null value in those cells. Go to Google Sheets . Click the 'Go to Google Sheets' button, and then click on the big green plus sign to start a new sheet. Paste the following into the top-left cell (cell A1): =IMPORTDATA(\"https://raw.githubusercontent.com/hist3907b-winter2015/module4-holes/master/texas.csv\") Pretty neat, eh? Now, here's the thing: even though your sheet looks like it is filled with information, it's not (at least, as far as the script we are about to run is concerned). That is to say, the sheet itself only has one cell of data, and that one cell is grabbing info from elsewhere on the web and dynamically filling the sheet. The script we're going to run works only on static values (more or less). Place your cursor in cell B1. On a Mac, hit shift+cmnd+downarrow . On a Windows machine, hit shift+ctrl+downarrow . Then on Mac shift+cmnd+rightarrow , on Windows shift+ctrl+rightarrow . Copy all of that data ( cmnd+c or ctrl+c ). Under 'Edit' select 'paste special' - 'paste VALUES only'. The formula you put in cell A1 now says #REF! . You can delete this now. This mucking about is necessary so that the add on script we are about to run will work. We now need to fill those empty values. In the tool bar, click add ons - get add ons . Search for blanks . You want to add Blank Detector . Now, click somewhere in your data. On Mac, hit cmnd+a . On Windows, hit ctrl+a . This highlights all of your data. Click Add ons - blank detector - detect cells . A dialogue panel will open on the right side of your screen. Click the button beside set value and type in null . Hit run . All of the blank cells will fill with the word null . Delete column A (which formerly had record numbers, but is now just filled with the word null . We don't need it). If you get the error, 'run exceeded maximum time' just hit the run button again. This script might take a few minutes. You can now copy and paste your table of data into the data input box in RAW, and you should get the green thumbs up saying 'x records have been successfully parsed!' NB The Blank Detector add on may take a long time. Try it out on the CSV. You can copy the parsed data from my Google Sheets file . Playing with RAW RAW takes your data, and depending on your choices, passes it into chart templates built on the D3.js code library. D3.js is a powerful library for making all sorts of charts (including interactive ones). If this sort of thing interests you, you can follow the tutorials in Elijah Meeks' excellent new book . With your data pasted in, you can now experiment with a number of different visualizations that are all built on the D3.js code library. Try the \u2018alluvial\u2019 diagram. Pick place1 and place2 as your dimensions you click and drag the green boxes under 'map your data' into the 'steps' box. Leave the 'size' box empty. Under 'customize your visualization' you can click inside the 'width' box to make the diagram wider and more legible. Does anything jump out? Try place3 and place 4. Try place1, place2, place3, and place4 in a single alluvial diagram. When we look at the original letters, we see that the writer often identified the town in which he was writing, and the town of the addressee. Why choose the third and fourth places? Perhaps it makes sense, for a given research question, to assume that with the pleasantries out of the way the writers will discuss the places important to their message. Experiment! This is one of the joys of working with data, experimenting to see how you can deform your materials to see them in a new light. You can export your visualization under the 'download' box at the bottom of the RAW page your choices are as a simple raster image (PNG), a vector image (SVG) or a data representation (json). Exercise 8: Simple Mapping and Georectifying In this exercise, you will find a historical map online, upload a copy to a map warper service, georectify it, and then display the map online, via a hosted service like CartoDB, and also through a map you will build yourself using Leaflet.js. Finally, we will also convert CSV to geojson using Mapbox's geojson converter , and we'll map that as a GitHub gist. We'll also grab a geojson file hosted on GitHub gist and import it into cartodb. Georectifying Georectifying is the process of taking an image (whether it is of a historical map, chart, airphoto, or whatever) and manipulating its geometry so that it matches a geographic projection. Think of it like this: you take your handdrawn map, and use pushpins to pin down known locations on your map to a globe. As you pin, your image stretches and warps. Traditionally, this has not been an easy thing to do, if you are new to GIS. In recent years, the curve has flattened significantly. In this exercise, we'll grab an image, upload it to the Map Warper website, and then export it as a tileset which can be used in other mapping programs. Get a historical map. I like the Fire Insurance plans from the Gatineau Valley Historical Society ; I'm sure you can find others to suit your interests. Right-click and save as to grab a copy. Save it somewhere easily accessible. Go to Map Warper and sign up for an account. Then login. Go to the upload screen: Fill in as much of the metadata as you can. Then select your map from your computer, and upload it. On the next page, click 'Rectify'. Pan and zoom both maps until you're sure you're looking at the same area in both. Double click in a map, select the location icon, and click on a point (location) you are sure you can match in the other window. Click on the other map window, select the location icon, and then click on the same point. Note the 'Add control point' button below and between both maps will light up. Click on this to confirm that this is a control point you want. Do this at least three times; the more times you can do it, the better the map warp. Having selected your control points, click on 'Warp image'. You can now click on the 'Export' panel, and get the URL for your georectified image in a few different formats. If you clicked on the KML option, a new Google Map window will open. For many web mapping applications, the Tiles (Google/OSM scheme): Tiles Based URL is what you want. You'll get a URL like this: http://mapwarper.net/maps/tile/27421/{z}/{x}/{y}.png . Save that info. You'll need it later. You have now georectified a map. Let's use that map as a base layer in Palladio . Go to Palladio . Hit 'start'. You will see 'Load .csv or spreadsheet'. In the box, paste in some data. You can progress to the next step without having any real data: just paste or type something in and hit enter so you have two lines. Obviously, you won't have any points on your map, but if you were having trouble with that step, this allows you to bypass it to continue on with this tutorial. Click the 'Map' tab at the top of the screen. Click 'New layer' on the right side menu. Click the Tiles tab in the right side menu. Select Custom Tiles. Paste your Map Warper URL into the Tileset URL field. Your URL will look like http://mapwarper.net/maps/tile/27421/{z}/{x}/{y}.png . Click 'Add layer'. Congratulations! You've georectified a map, and used it as a base layer for a visualization of some point data. Reference these notes on using a georectified map with the CartoDB service . Exercise 9: Network Analysis in R Earlier, we took the index from the Texan Correspondence, a list of letters from so-and-so to so-and-so. When we stitch that together into a network of people connected because they exchanged letters, we end up with a shard of their social network. Networks can be queried for things like power, position, and role, and so used judiciously, we can begin to suss something of the social structures in which their history took place. Before you go any further, make sure you also take a long look at Scott Weingart's series, Networks Demystified . Finally, heed our warning . This exercise uses the R language to do our analysis, which in DH Box we access via RStudio, a programming environment. For this exercise, do the following: Please read the introduction to R in our supporting materials . Then progress to the netviz exercise . Exercise 10: QGIS There are many excellent tutorials around concerning how to get started with GIS. Our own MacOdrum Library's MADGIC centre has tremendous resources and I would encourage you to speak with the map librarians before embarking on any serious mapping projects. In the short term, the historian Fred Gibbs has an excellent series on using the open source GIS platform QGIS to make and map historical data. For this exercise, do the following: Try Gibbs' first tutorial, 'Making a map with QGIS' . Installing QGIS Downloading geographic data Displaying data in QGIS Next, try georectifying a historical map and adding it to your GIS following Gibbs' other tutorial 'Using Historical maps with QGIS' . Exercise 11: Identifying Similar Images with TensorFlow This exercise uses uses a jupyter notebook to run the code. You can launch this jupyter notebook in an executable environment by clicking on the button below. This exercise is based on Douglas Duhaime's tutorial identifying similar images with tensorflow which is very clearly laid out and explained. In this notebook, I've compressed the steps. The 'images' directory only has 25 images in it for demo purposes. In Duhaime's post, the images folder (that you grab with wget has about 2000 images in it). Launch the jupyter notebook . This may take up to a minute to finish loading. Open the find-similar-images.ipynb file. You'll notice that the notebook is already pre-populated with commands. For each section, click the button labelled | Run . This will either skip to the next paragraph if it is text/information or run the command if it is a command. Each time the run finishes, check the main notebook with the file listings. The notebook will update and create new files as different commands run. Click | Run through each command/section. Open the Affinity Propagation.ipynb file and for each section, click the button labelled | Run . Notice the clusters that are created. Going Further Now that you understand the basics of the TensorFlow exercise, try running your own jupyter instance. Navigate to Dr. Graham's TensorFlow GitHub respository . Fork the repository so you have your own version. Drop more images into the image folder. Navigate to the Binder website . Paste the URL of your forked GitHub repository into the section labelled GitHub repository name or URL . Click the button labelled launch to create your own instance of the jupyter notebook. When the Binder finishes building, click the text labelled Copy the text below, then paste into your README to show a binder badge: launch|Binder . Copy the Markdown link to your Binder instance. Navigate to your forked repository on GitHub. Open the README.md file and click the pen/pencil icon to edit the file. Use the button text you copied in step 8 to change the link for the button labelled launch|Binder from the link to Dr. Graham's Binder instance to your new Binder URL, otherwise clicking the button in your GitHub repository will just open Dr. Graham's original repository. Going Further There are many tutorials at The Programming Historian that are appropriate here. Try some under the 'Data Manipulation' or 'Distant Reading' headings.","title":"Exercises"},{"location":"module-4/Exercises/#module-4-exercises","text":"There are many different tools and approaches you could use to visualize your data, both as a preliminary pass to spot the holes and also for more formal analysis. In which case, for this module, I would like you to select two of these exercises which seem most relevant to your capstone exercise. You are welcome to work through more of them, of course, but I want the exercises to move your own research forward. Some of these I wrote and some are adapted from The Macroscope ; others are adapted or used holus-bolus from scholars like Miriam Posner , Fred Gibbs , and Heather Froehlich (and I'm grateful that they shared their materials!). Finally, you are welcome to explore the lessons and tutorials at The Programming Historian if they seem appropriate to what you want to do for the Capstone Exercise. But what if I haven't any idea of what to do for the capstone exercise? Then read through the various tutorials for inspiration. Find something that strikes you as interesting, and then talk to me about how you might employ the ideas or concepts with regard to the Canadian war diary data.","title":"Module 4 Exercises"},{"location":"module-4/Exercises/#things-to-install","text":"Many of these involve having to install more software on your own machine. In those exercises that involve using R and RStudio, you are welcome to install RStudio on your own machine OR to use it in DH Box. Please read this quick introduction to R and Rstudio carefully . If you decide to install R and RStudio on your own machine, I would suggest you read the introductory bits from Lincoln Mullen's book-in-progress, Computational Historical Thinking , especially the 'setup' part under 'getting started' (pay attention to the bit on installing packages and dependencies). If you spot any exercises in Mullen's book that seem relevant to the Capstone Exercise, you may do those as an alternative to the ones here. Alternatively , go to Swirl and learn the basics of R within Swirl . DHNow links to a new Basic Text Mining in R tutorial which is worth checking out as well. NB It is always very important to record in your own notebooks what version of R you used for your analysis, what version of any R packages you installed and used, and so on because packages can go out of date. In the table below I've gathered the exercises together under the headings of Text , Networks , Maps , and Charts . I've also added some entries that I am categorizing under Art . The first is a series of exercises on the sonification of data and the second is a guide to making Twitterbots ; the third is about glitching digital imagery. These approaches can provide surprising and novel insights into history, as they move from representing history digitally to performing it. Visit, for instance, the final project in an undergraduate digital history class at the University of Saskatchewan by Daniel Ruten . Daniel translated simple word clouds of a First World War diary into a profound auditory performance. I would be very interested indeed to see if any capstone exercises in HIST3814o gave sonification or Twitterbots or glitching a try. The exercises in this module are covered in the chart below: Texts Networks Maps Charts Art Topic Modeling Tool Network analysis in Gephi Simple mapping georectifying Quick charts using RAW Sonification Topic Modeling in R Network Analysis in R QGIS (tutorials by Fred Gibbs) Twitterbots Text Analysis with OverviewProject Network Analysis in Cytoscape Geoparsing with Python Glitching Photos Corpus Linguistics with AntConc Choose your own adventure Palladio with Posner Text Analysis with Voyant Using igraph to visualize network data Leaflet.js Maps Identifying Similar Images with TensorFlow","title":"Things to install?"},{"location":"module-4/Exercises/#exercise-1-network-visualization","text":"This exercise uses the open source program Gephi which you install on your own computer. If you'd rather not install anything, please see Network Analysis in R instead. Recall that the index of the collected letters of the Republic of Texas was just a list of letters from so-and-so to so-and-so. We haven't looked at the content of those letters, but the shape of network the meta data of that correspondence can be revealing (remember Paul Revere! ) When we stitch that together into a network of people connected because they exchanged letters, we end up with a shard of their social network. Networks can be queried for things like power, position, and role, and so used judiciously, we can begin to suss something of the social structures in which their history took place. I would recommend that you also take a long look at Scott Weingart's series, Networks Demystified . Finally, heed our warning . For this exercise, do the following: Transform your Texan Correspondence data into a network, which you will then visualize with the open source program Gephi . The detailed instructions are in our supporting materials .","title":"Exercise 1: Network Visualization"},{"location":"module-4/Exercises/#going-further","text":"The data you get from topic modeling is in many ways just the first step to making it meaningful. Read through Miram Posner's article, Very basic strategies for interpreting results from the Topic Modeling Tool . In this article, Posner explores the results of your data, what the different files mean, and how you can interpret that data. Posner also shows in another article how you can visualize your results from the Topic Modeling Tool . While visualization is just one method to interpret your data, it is a powerful way to begin to make some meaningful insights into the data.","title":"Going Further"},{"location":"module-4/Exercises/#exercise-2-topic-modeling-tool","text":"In this exercise you will use the Topic Modeling Tool to create a simple topic model and a webpage that allows you to browse the results. Download the Topic Modeling Tool from GitHub . Make sure you have some content on your own machine; the Colonial Newspaper Database is a handy corpus. (Created by Melodee Beals, it's a series of late 18th, early 19th century cleanly transcribed newspaper articles from Scotland and Northern England; You can grab my copy from GitHub ). Or perhaps you might move your copy of the Canadian war diary out of DH Box onto your computer. At the command prompt in DH Box, type $ ls to make sure you can see your Canadian war diary text folder (ie. you can't zip a folder from the command line if you are in that folder, so $ cd out of it if necessary). Assuming your files are in war-diary-text , zip the folder up with this command, $ zip -r wardiaryfiles.zip war-diary-text . Use the File Manager to download the zip file. Unzip the folder on your machine. Double-click on the file you downloaded in Step 1. This will open a java-based graphical user interface with one of the most common topic modeling approaches, 'Latent Dirichlet Allocation'. Set the input to be the Colonial Newspaper Database or the Canadian war diary. Set the output to be somewhere neat and tidy on your computer. Set the number of topics you'd like to model. Click 'train topics' to run the algorithm. When it finishes, go to the folder you selected for output, and find the file all_topics.html in the output_html folder. Click on all_topics.html . You now have a browser-based way of navigating your topics and documents. In the output_csv folder created, you will find the same information as CSV, which you could then input into a spreadsheet for other kinds of visualizations (which we'll talk about in class). Make a note in your open notebook about your process and your observations. How does reading this material in this way change/challenge/or focus your understanding of the material?","title":"Exercise 2: Topic Modeling Tool"},{"location":"module-4/Exercises/#exercise-3-topic-modeling-in-r","text":"Exercise 2 was quite a simple way to do topic modeling. In this exercise, we are going to use a package for the R statistical language called 'Mallet' to do our topic modeling. One way isn't necessarily better than the other, although doing our analysis within R allows the potential for extending the analysis or combining it with other data. First, read this introduction to R so what follows isn't a complete shock! For this exercise, do ONE of the following: Guidance for doing this in RStudio in the DH Box Guidance for doing this in RStudio installed on your own computer","title":"Exercise 3: Topic Modeling in R"},{"location":"module-4/Exercises/#exercise-4-text-analysis-with-overview","text":"In this exercise, we're going to look at the Colonial Newspaper Database again, but this time using a tool called 'Overview'. Overview uses a different approach than the topic models we've been discussing. In essence, it looks at word frequencies and their distributions within a document, and within a corpus, to organize the documents into folders of progressively similar word use. You can download Overview to run on your own machine, but for our purposes, the hosted version on the Overview Docs website is sufficient. Go to that page, watch the video, create an account, and then log in ( you must create an account to use Overview ). (More help about how Overview works may be found on their blog , including helpful videos.) Once you're inside, click 'import from a CSV file', and upload the CND.csv (which you can download and save to your own machine from my GitHub - right-click and save as the previous link). On the 'UPLOAD A CSV FILE' page in Overview click 'browse' and select the CND.csv . It will give you a preview. There are a number of options here you can tell Overview which words to ignore, and which words to give added importance to. What words will you select? Make a note in your notebook. Hit 'upload'. A new page appears, called YOUR DOCUMENT SETS . Click on the one you just uploaded. A file folder tree showing documents of progressively greater similarity will open; on the right side will be the list of documents within each box (the box in question will be greyed out when you click on it, so you know where you are). You can search for words in your document, and Overview will tell you where they are; you can tag documents that you find interesting. The Overview system allows you to jump between a distant, macroscopic view and a close, document level view. Jump back and forth, see what you can find. For suggestions about how to use Overview effectively, try their blog . Make notes about what you observe in your notebook. Also, you can export your tagged document set from Overview, so that you could visualize the patterns of tagging in a spreadsheet (for instance). Going further: Do you see how you could upload your documents that you collected during Module 2?","title":"Exercise 4: Text Analysis with Overview"},{"location":"module-4/Exercises/#exercise-5-corpus-linguistics-with-antconc","text":"Heather Froelich has put together an excellent step-by-step with using AntConc for exploring textual patterns within, and across, corpora of texts. Work your way through her tutorial . Can you get our example materials (from the Colonial Newspaper Database) into AntConc? Our instructions in The Macroscope might help you to split the CSV into individual txt files. Alternatively, do you have any materials of your own, already collected? Feed them into AntConc. What patterns do you see? What if you compare your materials against other corpora of texts? For your information, CoRD has a collection of corpora that you can explore .","title":"Exercise 5: Corpus Linguistics with AntConc"},{"location":"module-4/Exercises/#exercise-6-text-analysis-with-voyant","text":"In Module 2 , if you recall, we worked through how to transform XML using stylesheets. Melodee Beals used a stylesheet to transform her database into a series of individual txt files. In the exercises above, a transformer was used to make the database into a single CSV file. In this exercise, we are going to use Voyant Tools to visualize patterns of word use in the database. Voyant can read either a CSV or text files. The advantage of uploading a folder of text files is that, if the files are in chronological order, Voyant's default visualizations will also be arranged in chronological order and thus we can see change over time. Go to Voyant Tools . Paste the following URL to the CSV of the CND database: https://raw.githubusercontent.com/shawngraham/exercise/gh-pages/CND.csv . Now, open a new browser window, and go to the colonial newspaper file on Voyant tools . Do you see the difference? In the latter window, the individual articles have been uploaded individually, and thus are treated as individual documents in chronological order. Explore the corpus, comparing terms over time, looking at keywords in context, and using the RezoViz tool to create a graph where people, places, and organizations that appear in the same documents (and across documents) are connected (you can find 'rezoviz' under the cogwheel icon at the top right of the panel). Google these terms and tools for what they mean and how others have used them. You can embed any of the tools in your blogs by using the 'save' icon and getting the iframe or embed code. You can apply stopwords by clicking on the cogwheel in any of the different tools, and selecting stopwords. Apply the stopwords globally, and you'll only have to do this once! What patterns do you see? What do different tools highlight? Which ones are useful? What patterns do you see that strike you as interesting? Note this all down. Going further: Upload materials you collected in Module 2 and explore them.","title":"Exercise 6: Text Analysis with Voyant"},{"location":"module-4/Exercises/#exercise-7-raw","text":"","title":"Exercise 7: RAW"},{"location":"module-4/Exercises/#quick-charts-using-raw","text":"A quick chart can be a handy thing to have. Google spreadsheets, Microsoft Excel, and a host of other programs can make excellent charts quickly with their wizard functions. Never hesitate to turn to these. However, they are not always good with non-numeric data. In Module 3 , you used the Stanford Named Entity Recognizer to extract place names from a text. After some further munging with regex, you might have ended up with a CSV that looks like the Texas CSV . Can we do a quick visualization of this information? One useful tool is RAW . Open RAW in a new window. Copy the table of data of places mentioned in the Texan correspondence, and paste it into the data input box at the top of the RAW screen. Oh no, an error!","title":"Quick Charts Using RAW"},{"location":"module-4/Exercises/#a-quick-data-munge","text":"You should get an error message, to the effect that you need to check 'line 2'. What's gone wrong? RAW has checked the number of values you have in that row, and compared it to the number of columns in row 1 (which contains all the column names). It sees that the two don't match. What we need to do is add a default null value in those cells. Go to Google Sheets . Click the 'Go to Google Sheets' button, and then click on the big green plus sign to start a new sheet. Paste the following into the top-left cell (cell A1): =IMPORTDATA(\"https://raw.githubusercontent.com/hist3907b-winter2015/module4-holes/master/texas.csv\") Pretty neat, eh? Now, here's the thing: even though your sheet looks like it is filled with information, it's not (at least, as far as the script we are about to run is concerned). That is to say, the sheet itself only has one cell of data, and that one cell is grabbing info from elsewhere on the web and dynamically filling the sheet. The script we're going to run works only on static values (more or less). Place your cursor in cell B1. On a Mac, hit shift+cmnd+downarrow . On a Windows machine, hit shift+ctrl+downarrow . Then on Mac shift+cmnd+rightarrow , on Windows shift+ctrl+rightarrow . Copy all of that data ( cmnd+c or ctrl+c ). Under 'Edit' select 'paste special' - 'paste VALUES only'. The formula you put in cell A1 now says #REF! . You can delete this now. This mucking about is necessary so that the add on script we are about to run will work. We now need to fill those empty values. In the tool bar, click add ons - get add ons . Search for blanks . You want to add Blank Detector . Now, click somewhere in your data. On Mac, hit cmnd+a . On Windows, hit ctrl+a . This highlights all of your data. Click Add ons - blank detector - detect cells . A dialogue panel will open on the right side of your screen. Click the button beside set value and type in null . Hit run . All of the blank cells will fill with the word null . Delete column A (which formerly had record numbers, but is now just filled with the word null . We don't need it). If you get the error, 'run exceeded maximum time' just hit the run button again. This script might take a few minutes. You can now copy and paste your table of data into the data input box in RAW, and you should get the green thumbs up saying 'x records have been successfully parsed!' NB The Blank Detector add on may take a long time. Try it out on the CSV. You can copy the parsed data from my Google Sheets file .","title":"A quick data munge"},{"location":"module-4/Exercises/#playing-with-raw","text":"RAW takes your data, and depending on your choices, passes it into chart templates built on the D3.js code library. D3.js is a powerful library for making all sorts of charts (including interactive ones). If this sort of thing interests you, you can follow the tutorials in Elijah Meeks' excellent new book . With your data pasted in, you can now experiment with a number of different visualizations that are all built on the D3.js code library. Try the \u2018alluvial\u2019 diagram. Pick place1 and place2 as your dimensions you click and drag the green boxes under 'map your data' into the 'steps' box. Leave the 'size' box empty. Under 'customize your visualization' you can click inside the 'width' box to make the diagram wider and more legible. Does anything jump out? Try place3 and place 4. Try place1, place2, place3, and place4 in a single alluvial diagram. When we look at the original letters, we see that the writer often identified the town in which he was writing, and the town of the addressee. Why choose the third and fourth places? Perhaps it makes sense, for a given research question, to assume that with the pleasantries out of the way the writers will discuss the places important to their message. Experiment! This is one of the joys of working with data, experimenting to see how you can deform your materials to see them in a new light. You can export your visualization under the 'download' box at the bottom of the RAW page your choices are as a simple raster image (PNG), a vector image (SVG) or a data representation (json).","title":"Playing with RAW"},{"location":"module-4/Exercises/#exercise-8-simple-mapping-and-georectifying","text":"In this exercise, you will find a historical map online, upload a copy to a map warper service, georectify it, and then display the map online, via a hosted service like CartoDB, and also through a map you will build yourself using Leaflet.js. Finally, we will also convert CSV to geojson using Mapbox's geojson converter , and we'll map that as a GitHub gist. We'll also grab a geojson file hosted on GitHub gist and import it into cartodb.","title":"Exercise 8: Simple Mapping and Georectifying"},{"location":"module-4/Exercises/#georectifying","text":"Georectifying is the process of taking an image (whether it is of a historical map, chart, airphoto, or whatever) and manipulating its geometry so that it matches a geographic projection. Think of it like this: you take your handdrawn map, and use pushpins to pin down known locations on your map to a globe. As you pin, your image stretches and warps. Traditionally, this has not been an easy thing to do, if you are new to GIS. In recent years, the curve has flattened significantly. In this exercise, we'll grab an image, upload it to the Map Warper website, and then export it as a tileset which can be used in other mapping programs. Get a historical map. I like the Fire Insurance plans from the Gatineau Valley Historical Society ; I'm sure you can find others to suit your interests. Right-click and save as to grab a copy. Save it somewhere easily accessible. Go to Map Warper and sign up for an account. Then login. Go to the upload screen: Fill in as much of the metadata as you can. Then select your map from your computer, and upload it. On the next page, click 'Rectify'. Pan and zoom both maps until you're sure you're looking at the same area in both. Double click in a map, select the location icon, and click on a point (location) you are sure you can match in the other window. Click on the other map window, select the location icon, and then click on the same point. Note the 'Add control point' button below and between both maps will light up. Click on this to confirm that this is a control point you want. Do this at least three times; the more times you can do it, the better the map warp. Having selected your control points, click on 'Warp image'. You can now click on the 'Export' panel, and get the URL for your georectified image in a few different formats. If you clicked on the KML option, a new Google Map window will open. For many web mapping applications, the Tiles (Google/OSM scheme): Tiles Based URL is what you want. You'll get a URL like this: http://mapwarper.net/maps/tile/27421/{z}/{x}/{y}.png . Save that info. You'll need it later. You have now georectified a map. Let's use that map as a base layer in Palladio . Go to Palladio . Hit 'start'. You will see 'Load .csv or spreadsheet'. In the box, paste in some data. You can progress to the next step without having any real data: just paste or type something in and hit enter so you have two lines. Obviously, you won't have any points on your map, but if you were having trouble with that step, this allows you to bypass it to continue on with this tutorial. Click the 'Map' tab at the top of the screen. Click 'New layer' on the right side menu. Click the Tiles tab in the right side menu. Select Custom Tiles. Paste your Map Warper URL into the Tileset URL field. Your URL will look like http://mapwarper.net/maps/tile/27421/{z}/{x}/{y}.png . Click 'Add layer'. Congratulations! You've georectified a map, and used it as a base layer for a visualization of some point data. Reference these notes on using a georectified map with the CartoDB service .","title":"Georectifying"},{"location":"module-4/Exercises/#exercise-9-network-analysis-in-r","text":"Earlier, we took the index from the Texan Correspondence, a list of letters from so-and-so to so-and-so. When we stitch that together into a network of people connected because they exchanged letters, we end up with a shard of their social network. Networks can be queried for things like power, position, and role, and so used judiciously, we can begin to suss something of the social structures in which their history took place. Before you go any further, make sure you also take a long look at Scott Weingart's series, Networks Demystified . Finally, heed our warning . This exercise uses the R language to do our analysis, which in DH Box we access via RStudio, a programming environment. For this exercise, do the following: Please read the introduction to R in our supporting materials . Then progress to the netviz exercise .","title":"Exercise 9: Network Analysis in R"},{"location":"module-4/Exercises/#exercise-10-qgis","text":"There are many excellent tutorials around concerning how to get started with GIS. Our own MacOdrum Library's MADGIC centre has tremendous resources and I would encourage you to speak with the map librarians before embarking on any serious mapping projects. In the short term, the historian Fred Gibbs has an excellent series on using the open source GIS platform QGIS to make and map historical data. For this exercise, do the following: Try Gibbs' first tutorial, 'Making a map with QGIS' . Installing QGIS Downloading geographic data Displaying data in QGIS Next, try georectifying a historical map and adding it to your GIS following Gibbs' other tutorial 'Using Historical maps with QGIS' .","title":"Exercise 10: QGIS"},{"location":"module-4/Exercises/#exercise-11-identifying-similar-images-with-tensorflow","text":"This exercise uses uses a jupyter notebook to run the code. You can launch this jupyter notebook in an executable environment by clicking on the button below. This exercise is based on Douglas Duhaime's tutorial identifying similar images with tensorflow which is very clearly laid out and explained. In this notebook, I've compressed the steps. The 'images' directory only has 25 images in it for demo purposes. In Duhaime's post, the images folder (that you grab with wget has about 2000 images in it). Launch the jupyter notebook . This may take up to a minute to finish loading. Open the find-similar-images.ipynb file. You'll notice that the notebook is already pre-populated with commands. For each section, click the button labelled | Run . This will either skip to the next paragraph if it is text/information or run the command if it is a command. Each time the run finishes, check the main notebook with the file listings. The notebook will update and create new files as different commands run. Click | Run through each command/section. Open the Affinity Propagation.ipynb file and for each section, click the button labelled | Run . Notice the clusters that are created.","title":"Exercise 11: Identifying Similar Images with TensorFlow"},{"location":"module-4/Exercises/#going-further_1","text":"Now that you understand the basics of the TensorFlow exercise, try running your own jupyter instance. Navigate to Dr. Graham's TensorFlow GitHub respository . Fork the repository so you have your own version. Drop more images into the image folder. Navigate to the Binder website . Paste the URL of your forked GitHub repository into the section labelled GitHub repository name or URL . Click the button labelled launch to create your own instance of the jupyter notebook. When the Binder finishes building, click the text labelled Copy the text below, then paste into your README to show a binder badge: launch|Binder . Copy the Markdown link to your Binder instance. Navigate to your forked repository on GitHub. Open the README.md file and click the pen/pencil icon to edit the file. Use the button text you copied in step 8 to change the link for the button labelled launch|Binder from the link to Dr. Graham's Binder instance to your new Binder URL, otherwise clicking the button in your GitHub repository will just open Dr. Graham's original repository.","title":"Going Further"},{"location":"module-4/Exercises/#going-further_2","text":"There are many tutorials at The Programming Historian that are appropriate here. Try some under the 'Data Manipulation' or 'Distant Reading' headings.","title":"Going Further"},{"location":"module-4/Seeing Patterns/","text":"Seeing Patterns June 4-11, 2018 Concepts In the previous module, we collected and cleaned vast amounts of historical data. Let's see what patterns emerge. Part exploration, part reflection, and part argument, these two stages are not linear but feed into one another. Exploration and preliminary visualizations are often about finding the holes. Sometimes, a quick visualization helps you understand what is missing, or what is a glitch in your method, or what is just plain mistaken in your assumptions. I once spent quite a lot of time staring at a network graph before I realized that I'd neglected to import the entire file - I was trying to interpret just a subset of it! Back to my wrangling I went, and in the process of cleaning that data, I realized there were patterns that would be better visualized as simple line plots. I visualized these - and found other errors. Back to wrangling. In the end, a combination of network graph and simple line plots allowed me to identify a story about influence and control of landholding (I was working with archaeological data). In this module, let us begin by looking at a couple of pieces that explore this cyclical dynamic between data wrangling and exploration. In our Slack space for this module or in your notebooks, I want you to explicitly discuss the work of the following scholars. Make explicit connections with things you've read/explored/studied/developed in other history classes! In essence: what would your last essay last term have looked like, if you'd been thinking along these lines? Begin by visiting the Mapping Texts project (PDF downloads in new tab) . Look at the work of Michelle Moravec who is using corpus linguistics tools to understand the history of women's suffrage (and also the visualizing gender ). Listen to Micki Kaufmann on Quantifying Kissinger . (Her methods are detailed on her blog .) Talk about network analysis . Keep an eye on Paul Revere . Talk about topic modeling (and also this blog post on Mallet ). Talk about principles of visualization . You should probably talk about Edward Tufte , too. Talk. What could history be like if more of our materials went through these mills? Things you will learn in this module Importing, querying, and visualizing networks with Gephi - igraph, R Topic modeling with the GUI topic modeling tool, and MALLET at the command line, and in R Simple maps with CartoDB (which may include georectifying and displaying historical maps as base layers) Corpus linguistics with AntConc TF-IDF with Overview Quick visualizations using RAW, an open source data visualization framework We will be busy in this module. Do not be afraid to ask myself, your peers, or the wider DH community for help and advice! It's the only way we grow. And finally... Just because there is a package, or a routine, or an approach to doing x with your data does not mean that you park your critical apparatus at the door. Consider Matthew Jockers who has done some amazing work putting together a package for the R statistical language that helps one analyze plot arcs in thousands of novels at once . Jockers describes how it works on his blog . Annie Swafford explores this package in a comprehensive blog post. She highlights important issues in the way the package deals with the complexity of the world, and how that might have an impact on results and conclusions drawn from using the package. The interplay of the work that Jockers has done, and the dialogue that Swafford has started with Jockers, is an important feature (and perhaps, a defining feature) of digital humanities scholarship. Jocker builds; and Swafford approaches the code from a humanistic perspective; and in the dialectic of their exchange, the code and our ability to understand the code's limitations and potentials will improve. And so we understand the world better. When you choose your exploratory method, you need to consider that the method has its own agenda! What you need to do this week Respond to the readings and the reading questions through annotation (taking care to respond to others' annotations as well) - see the instructions below. Remember to tag your annotations with 'hist3814o' so that we can find them on the course Hypothes.is group . Remember to annotate using our HIST3814o group . Do the exercises for this module, pushing yourself as far as you can. Annotate the instructions where they might be unclear or confusing; see if others have annotated them as well, and respond to them with help if you can. Keep an eye on our Slack channel - you can always offer help or seek out help there. The exercises in this module will pick up where we left off, with the Texan Correspondence. You'll represent the exchange of letters as a network. We'll try to extract place names from it. We'll topic model the letters themselves. We'll explore various ways of visualizing those results. Write a blog post describing what happened as you went through the exercises (your successes, your failures, the help you may have found/received), and link to your 'faillog' (ie. the notes you upload to your GitHub account - for more on that, see the exercises!). Submit your work to the course submission form . Readings Select one of the articles behind the links above (and/or in the exercises) to annotate. Ask yourself: who benefits from this? Who is hurt from this? Make an entry in your blog on this theme.","title":"Seeing Patterns"},{"location":"module-4/Seeing Patterns/#seeing-patterns-june-4-11-2018","text":"","title":"Seeing Patterns &mdash; June 4-11, 2018"},{"location":"module-4/Seeing Patterns/#concepts","text":"In the previous module, we collected and cleaned vast amounts of historical data. Let's see what patterns emerge. Part exploration, part reflection, and part argument, these two stages are not linear but feed into one another. Exploration and preliminary visualizations are often about finding the holes. Sometimes, a quick visualization helps you understand what is missing, or what is a glitch in your method, or what is just plain mistaken in your assumptions. I once spent quite a lot of time staring at a network graph before I realized that I'd neglected to import the entire file - I was trying to interpret just a subset of it! Back to my wrangling I went, and in the process of cleaning that data, I realized there were patterns that would be better visualized as simple line plots. I visualized these - and found other errors. Back to wrangling. In the end, a combination of network graph and simple line plots allowed me to identify a story about influence and control of landholding (I was working with archaeological data). In this module, let us begin by looking at a couple of pieces that explore this cyclical dynamic between data wrangling and exploration. In our Slack space for this module or in your notebooks, I want you to explicitly discuss the work of the following scholars. Make explicit connections with things you've read/explored/studied/developed in other history classes! In essence: what would your last essay last term have looked like, if you'd been thinking along these lines? Begin by visiting the Mapping Texts project (PDF downloads in new tab) . Look at the work of Michelle Moravec who is using corpus linguistics tools to understand the history of women's suffrage (and also the visualizing gender ). Listen to Micki Kaufmann on Quantifying Kissinger . (Her methods are detailed on her blog .) Talk about network analysis . Keep an eye on Paul Revere . Talk about topic modeling (and also this blog post on Mallet ). Talk about principles of visualization . You should probably talk about Edward Tufte , too. Talk. What could history be like if more of our materials went through these mills?","title":"Concepts"},{"location":"module-4/Seeing Patterns/#things-you-will-learn-in-this-module","text":"Importing, querying, and visualizing networks with Gephi - igraph, R Topic modeling with the GUI topic modeling tool, and MALLET at the command line, and in R Simple maps with CartoDB (which may include georectifying and displaying historical maps as base layers) Corpus linguistics with AntConc TF-IDF with Overview Quick visualizations using RAW, an open source data visualization framework We will be busy in this module. Do not be afraid to ask myself, your peers, or the wider DH community for help and advice! It's the only way we grow.","title":"Things you will learn in this module"},{"location":"module-4/Seeing Patterns/#and-finally","text":"Just because there is a package, or a routine, or an approach to doing x with your data does not mean that you park your critical apparatus at the door. Consider Matthew Jockers who has done some amazing work putting together a package for the R statistical language that helps one analyze plot arcs in thousands of novels at once . Jockers describes how it works on his blog . Annie Swafford explores this package in a comprehensive blog post. She highlights important issues in the way the package deals with the complexity of the world, and how that might have an impact on results and conclusions drawn from using the package. The interplay of the work that Jockers has done, and the dialogue that Swafford has started with Jockers, is an important feature (and perhaps, a defining feature) of digital humanities scholarship. Jocker builds; and Swafford approaches the code from a humanistic perspective; and in the dialectic of their exchange, the code and our ability to understand the code's limitations and potentials will improve. And so we understand the world better. When you choose your exploratory method, you need to consider that the method has its own agenda!","title":"And finally..."},{"location":"module-4/Seeing Patterns/#what-you-need-to-do-this-week","text":"Respond to the readings and the reading questions through annotation (taking care to respond to others' annotations as well) - see the instructions below. Remember to tag your annotations with 'hist3814o' so that we can find them on the course Hypothes.is group . Remember to annotate using our HIST3814o group . Do the exercises for this module, pushing yourself as far as you can. Annotate the instructions where they might be unclear or confusing; see if others have annotated them as well, and respond to them with help if you can. Keep an eye on our Slack channel - you can always offer help or seek out help there. The exercises in this module will pick up where we left off, with the Texan Correspondence. You'll represent the exchange of letters as a network. We'll try to extract place names from it. We'll topic model the letters themselves. We'll explore various ways of visualizing those results. Write a blog post describing what happened as you went through the exercises (your successes, your failures, the help you may have found/received), and link to your 'faillog' (ie. the notes you upload to your GitHub account - for more on that, see the exercises!). Submit your work to the course submission form .","title":"What you need to do this week"},{"location":"module-4/Seeing Patterns/#readings","text":"Select one of the articles behind the links above (and/or in the exercises) to annotate. Ask yourself: who benefits from this? Who is hurt from this? Make an entry in your blog on this theme.","title":"Readings"},{"location":"module-5/Exercises/","text":"Capstone Exercise For the Capstone Exercise, you have the opportunity to create a domain (website + complete access to the webserver that powers it, so that you can install other platforms/services) of your own. A typical setup will be something along the lines of: GitHub: yourusername.github.io . Reclaim Hosting: your-domain-name.org or your-domain-name.ca or your-domain-name.com . On your blog/narrative, you may want an 'About' page where you can signal the kinds of history you're interested in, and the preferred way for people to get in touch with you. You do not have to use your real name. Remember the real names policy . This section begins with several options you may wish to pursue for the Capstone Exercise. Following these options are resources, tutorials, and tips/hints you can reference to support the Capstone Exercise you choose. Depending on how you decided to communicate findings, this section contains resources which introduce you to the following concepts: Layout Manipulating graphics Design with colour and font Creating digital exhibits I do not want to leave you thinking that all digital history work is necessarily text based. To that end, if you are looking for a challenge or for exposure to something rather different, I suggest you at least bookmark my series of tutorials on augmented reality, games, and 3d models for history and archaeology . Let's get started. 'Accidental Renaissance: the photos that look like Italian paintings' . Option 1: GitHub Pages with a Static Site Generator Static site generators are software packages that allow you to quickly build and easily update a website via the terminal. Most static site generators use human readable programming languages like YAML / YML (Yet Another Markdown Language) that contain the settings for your website. You create a basic text file (i.e. Markdown), run a build command in the terminal that will convert your text files into web pages, and push the pages to your server. GitHub offers each user their own unique domain attached to their account - this service is called GitHub Pages, or gh-pages . Each user domain resembles yourusername.github.io , where yourusername is your GitHub account username. If you went to yourusername.github.io right now, it will show a 404 / 'site not found' error. This is because you have not yet set-up your gh-pages website. Like the master branch of your GitHub repository, gh-pages is another branch option. Essentially, you create a GitHub respository, populate the gh-pages branch with web files, and your yourusername.github.io will become a functioning website. By deafault, GitHub Pages uses Jekyll, a static site generator . However, Jekyll is not the only option. Hugo is another static site generator. This workbook was built using the static site generator MKDocs . While this option shows you how to use GitHub Pages powered by Jekyll to populate your GitHub domain, the instructions are generally similar for other static site generators. You make a few changes and run the website using a GitHub repository. Login to your GitHub account. Click the button labelled + at the top left of the page next to your account. Select New repository . Under Repository name * , enter yourusername.github.io (ensure you change 'yourusername' to your GitHub username). Click the button labelled Create repository . Navigate to your DH Box account and open the terminal (you can also set this up on your own machine, in which case you open the Terminal (Mac) or Command Prompt (Windows) or GitHub desktop client). Type $ git clone https://github.com/username/username.github.io into the terminal (changing the URL to your GitHub repository path). Type $ cd username.github.io into the terminal (ensure you change 'username' to your GitHub username). Type $ echo \"Hello World\" index.html into the terminal. This will pipe \"Hello World\" into the home file index.html of your new site. Type $ git add --all into the terminal. Type $ git commit -m \"Initial commit\" into the terminal. Type $ git push -u origin master into the terminal. Navigate to your domain at yourusername.github.io . You will see the text \"Hello World\". Customize your website The instructions above showed you how to get you GitHub Pages domain up and running. GitHub provides further instruction on customizing your Jekyll site in their documentation . GitHub Pages makes it easy to set-up a theme. Navigate to your GitHub Pages repository. Click the tab labelled Settings . Scroll down to the section labelled GitHub Pages . Click the button labelled Change theme . Choose a new theme from the provided options. Open the _config.yml file and and click the pen/pencil icon to edit the file. Your configuration file will show only the theme information. Add the following YAML configuration settings to you _config.yml file: github: [metadata] encoding: UTF-8 kramdown: input: GFM hard_wrap: false future: true jailed: false gfm_quirks: paragraph_end Open the index.html file and and click the pen/pencil icon to edit the file. Change the name of the file to index.md . Remove the \"Hello World\" message and replace it with the following Markdown header: --- title: Home page layout: default --- Here is my home page. Add a commit message and click the green button labelled Commit changes . Navigate to your repository home page and create a new file called 2019-04-07-first-post.md (change the 2019-04-07 to today's date). Copy and paste the following text into your new Markdown file: --- title: This is my title layout: post --- Here is my page. Add a commit message and click the green button labelled Commit changes . Navigate to your yourusername.github.io domain. If the site has not updated yet, wait a minute and/or clear your browser's cache. Going Further The instructions above show you how you can begin customizing you Jekyll website. You will probably want to change the theme and structure of the website. For instance, your blog posts should go in a folder called _posts . For more information, read Barry Clark's article on customizing your Jekyll site . You will also want to read through the Jekyll documentation . Most static site generators use similar layouts, structures, and YAML configurations. While the documentation for each generator will differ, the general idea between all of them remains the same. You may also want to build a simple website through GitHub Pages using the MkDocs static site generator and DH Box. Navigate to the static site generator exercise using GitHub Pages in the supporting materials . Option 2: Jekyll Now Jekyll is a static site generator widely used to create a website quickly that you can run from your GitHub account. Jekyll takes static Markdown files and converts them to a functioning website. While Jekyll requires some editing via the terminal, Jekyll Now is a project from Barry Clark that allows you to leverage the blogging power and simplicity of Jekyll without touching the terminal. NB If you already have a gh-pages branch set up at yourusername.github.io , this may produce some unexpected results. Login to your GitHub account. Navigate to the Jekyll Now repository . For further instruction, follow the Jekyll Now Quick Start guide on GitHub . Fork the Jekyll Now repository. In your fork of the respository, click Settings . Under Repository name , change the name from jekyll-now to yourusername.github.io (ensure you change 'yourusername' to your GitHub username). By setting the name to your unique GitHub domain (i.e. gh-pages), GitHub knows to make your gh-pages domain draw from these files. Click the button labelled Rename . Your site is now available at yourusername.github.io . NB If you navigate to your domain URL right away, it will not be there and will show a '404' error. You first need to upload a post in the format YYYY-MM-DD-some-text.md , ensuring YYYY-MM-DD is always the prefix to your posts file name. Navigate back to your forked repository page. Open the _config.yml file. Click the pen/pencil icon to edit the file. Change the relevant fields (i.e. name , description , avatar , footer-links , etc.). For avatar , navigate to your GitHub profile, right click your profile picture/avatar, and select View image . This will open your avatar in a new tab. Copy the URL for your avatar and paste it into the _config.yml avatar section. Write a commit message and click the green button labelled Commit changes . Navigate to the _posts folder and open the 2014-3-3-Hello-World.md file. Click the pen/pencil icon to edit the file. Change the file name from 2014-3-3-Hello-World.md to something relevant, ensuring to change the prefix to today's date in the YYYY-MM-DD format. Edit the file to describe your new site. Write a commit message and click the green button labelled Commit changes . Navigate to yourusername.github.io ( remember , ensure you change 'yourusername' to your GitHub username). You should now see your udpated site. If your site still shows a 404 error, try clearing your browser's cache and trying again. Also, ensure your post files use the proper naming convention with the YYYY-MM-DD date prefix. To add new posts, navigate to the _posts folder and click the button labelled Create new file . Remember to use the proper naming convention with the YYYY-MM-DD date prefix. Going Further After following the instructions in the previous section, you now have a functioning website that runs entirely through GitHub. You can, however, further customize your site. Read through Barry Clark's article on customizing your Jekyll site . Incorporate design and functionality in your website from the resources on this page. Option 3: Reclaim Hosting In the course space for cuLearn, I gave you a code to use to pay for a domain of your own. I have already purchased domain space for you from an academic web hosting service, Reclaim Hosting . This space will last for one year, at which point you have the option of paying to renew it or letting it die. Previous students in this course have used their domain to help with their applications for jobs and graduate school. Because you have complete access and control over your domain, you can install other services as well. For instance, maybe you use Dropbox or Google Drive to sync your files across machines, or to function as a backup? You can install a service called 'OwnCloud' on your own domain that does the same thing, so that you have control over all your own materials. You will be asked for the name you want to have for your space. You need to be thinking of branding here. Think of a professional name that conveys something of your personality and approach to history. I for instance own the domain, 'Electric Archaeology', which I chose to convey that I'm interested in digital archaeology, but also, that I move fast and cover a lot of breaking develops in the field (Hey. It's my blog. I like the name). Please choose wisely. Some combination of your first and last name is often the best idea, since your own name is your own best calling card ('shawn graham' is such a generic name, that it was already long gone by the time I started doing digital history). Type in a name, select a top-level domain (ie. .com, .ca, .org, etc. I'll suggest .ca), and click on the 'check availability' button. If the pop-up says 'Congratulations, domain available!' then click on the continue button. (You may be offered free id protection, where Reclaim Hosting will hide your details is someone does a 'who-is' search on the domain name. If it does, then tick off the check box to confirm that you want this, and hit continue). On the next screen, (the billing screen) fill in all of the information. The balance should be $0. At the bottom left, it will also say that you\u2019ve used a one-time promotional code. Hit the green button at the bottom to complete the purchase (which is not costing you anything). Congratulations! You now own your very own domain. It might take a bit of time for your domain to appear live on the web . During this time, you can log into your cPanel and install Wordpress and so on - see the next section below. Giving you a space of your own is my political act of protest against the centralization of learning inside learning management systems. Learning isn't 'managed', it's cultivated. Unlike cuLearn, I cannot monitor your activity inside your own domain. I can only see what you choose to make public. Unlike Facebook or Snapchat or Instagram, I am not trying to monetize your personality on the web. Wordpress for your blog Wordpress is probably the best option for a platform for writing the narrative bits of your digital history work. Click the 'Client area'. It will tell you that you have one active account (with Reclaim Hosting) and one active domain. When the time comes to renew your account or to close it down, this is where you do it. Note also that there is a link to 'Support', which will put you in touch with Reclaim Hosting's help desk. They are extremely fast and good at providing support; always treat any help request you make with them as if you were writing a formal email to me. Be polite and considerate, and thank them. The owners of the business often are the ones who provide the help! Without them, we couldn't do this class. Go to 'cPanel' - this is where you can install all sorts of different kinds of software on your account. Search for and select 'Web applications' Click on Wordpress. Then click on 'Install this application'. The next screen presents you with a number of options. Leave these set to their defaults. For 'location', leave this blank (you want to leave the directory option blank). That tells the installatron to put Wordpress at your-domain.ca . (When/if you install other pieces of software, you'd change this variable so that the new software doesn't overwrite this software!) Further down the page, under 'Settings', you need to change 'Administrator username', 'Administrator password', 'Website title', 'Website tagline'. This is the username and password to actually do things with your blog, and the name of your blog itself . Leave everything else alone. Click Install! Once it's done, the installer will remind you of the URL to your site, and the URL to the blog's dashboard (where you go to write posts, make changes to the look and feel of the site). Open these in a new browser window, and bookmark them. To login to your blog, remember to go to the dashboard URL (eg. http://your-domain.ca/wp-admin ), enter your blog administrator username and password. You can close the cPanel webpage now (log out first). Customising your blog If you look at the dashboard for your blog, you'll see a lot of options down the left side of your screen. If you want to change the look of your blog, click on 'Appearance' then click on 'Themes'. A number of themes are pre-installed. You can click on the different themes and select 'Preview' to see how your blog might look. When you find one you like, select 'activate'. Go to the other browser window where you have your-domain.ca open. Reload the page to see the changes take effect! If you're the sort of person who likes to sketch ideas out on paper, Lauren Heywood of the Disruptive Media Learning Lab has designed a paper-based exercise to prototype ideas. Why not give it a try? Print this PDF of Wordpress design (downloads in a new window) and follow the instructions. To write new content, know that there is a difference between a Post and a Page . A page is a new link on your site, while posts appear in chronological order on a page, with the most recent on top. Most themes show the most recent post by default, and pages appear in any menus on the site. When you are logged into your blog any post or page will have an 'Edit' button at the bottom that you can click. You'll then be presented with an editing box with the standard toolbar across the top (allowing you to change the font, insert images, change the alignment of the text and so on). At the top right will be a button to save or publish/update the post/page. Your blog will have a default 'about' page. Change that default text now to reflect something about who you are and why you are taking this course. To create new pages, you click on the 'Pages' link in your dashboard and select 'Add new'. To create new posts, you click on the 'Posts' link in your dashboard and select 'Add new'. Explore the options for your blog; customize and make the space your own. Password protected posts: If for any reason you feel that you don't want a post to be viewable by the web-at-large, you can hide it behind a password. At the top right where the 'Publish' button hides, click on 'Visibility' and select 'Password protected'. Remember though: you'll have to share the password with me for grading purposes. For more information about controlling visibility of your posts and so on, visit the Wordpress content visibility help page . Collecting your own annotations on your blog Hypothes.is has an API that allows you to do some neat things. 'API' stands for 'Application Programming Interface', which is just a fancy way of saying, 'you can write a program that interacts with this web service'. Kris Shaffer , a professor at the University of Mary Washington, has written a plugin for Wordpress that allows you to automatically collect annotations you make across the web and to display them all on a single page on your blog. So, we'll go get that plugin and install it on your blog: Open Kris Shaffer's Hypothes.is aggregator in a new browser window. Click 'Clone or Download' (the green button at the top right). In the pop-up, click 'Download ZIP' Go over to the dashboard for your blog (if you closed this, you can get there again by opening a new browser window and going to your-domain.ca/wp-admin ). In the dashboard, click on Plugins Add New. Click 'Upload Plugin'. It will open a window asking you to find and select the zip file you downloaded. This is probably in your 'Downloads' folder. Select and click Ok. Once it\u2019s uploaded and installed, click 'Activate'. In the dashboard, click on 'pages' then add a new page. Call it 'Web Notes' or 'Annotations' or something similar. Shaffer has created a 'shortcode' that tells Wordpress to go over to Hypothes.is and grab the latest information. So, in the text of your new page (in the editor window, make sure to click the 'text' button or else this won't work), enter the shortcode that will grab all of your public annotations: [hypothesis user = 'kris.shaffer'] where you remove the kris.shaffer and put in your own Hypothes.is username. Hit the 'Publish' button. Wordpress will report that the page has been published and give you a link to it; click on this link to see your new always-updating list of annotations! Of course, if you haven't made any public annotations yet, nothing will display. Go annotate something, then come back and reload the page, but remember to annotate using our HIST3814o group . Layout 'Layout' can mean different things in different contexts. A general overview on issues in layout is covered by 'What makes a design great', Lynda.com (requires sign-up for free trial) and 'Exploring principles of layout and composition' (requires sign-up for free trial) . The Slideshare on effective layouts gives you a sense of things to watch out for as well. For academic posters in particular, consider these poster design suggestions from the APA . In essence, good layout makes your argument legible, intuitive, and reinforces the rhetorical points you are trying to make. You should take into account 'principles of universal design' consider design issues with PowerPoint and websites (although some of the technical solutions proposed in those two documents are a bit out of date, the general principles hold!). If you decided to communicate findings via a poster, following are some hints/tips for designing a poster or modifying an existing poster. Inkscape View the Inkscape exercise in our supporting materials for a gentle introduction to the software . Download one of the scientific poster templates from Ugo Sangiorgi (These are developed from Felix Breuer's blog post ; note his discussion on design). Open it in Inkscape. Make notes in your open notebook from the point of view of layout: what elements of the design work? What aren't working? How would you repurpose this poster to fit the requirements of the assessment exercise ( remember, the details in the syllabus )? Visit Inkscape's website for help with the basics of Inkscape . Modify the poster, and upload the SVG, PDF, or PNG version to your repository. PowerPoint There's a lot of information out there on making posters with PowerPoint. Read parts 1, 2, and 3 of the Make Signs tutorial and then consider Colin Purrington's advice . Once you've read and digested the material, pick a poster from Pimp My Poster that sticks out to you. Make notes in your open notebook: from the point of view of layout what elements of the design work? What aren't working? How would you repurpose this poster to fit the requirements of the assessment exercises ( remember to visit GitHub for the details )? Grab a template from from Colin Purrington's website , and use it to prototype a poster that works. Upload the poster as a PDF to your repository. If you want to explore layout in the context of webpage creation, I would point you to the the roster of lessons at Codeacademy . Same instructions: find an existing site that you will consider from the point of view of what works, what doesn't, and use that reflection to guide the construction of your own. Typography Typography plays an important role in visual communication. It can do everything from reduce eyestrain (do a search for 'easiest fonts to read on screen') to communicate power and authority. Is it any wonder that strong bold capitals come to be known as 'Roman'? But first: are you a comic sans criminal? NB Serif fonts are sometimes not as accessible as sans-serif fonts. Older screens and monitors can have a difficult time rendering serif fonts. Sans-serif fonts can be more readable across different screens. However , this does not mean you should throw away serif fonts many serif fonts can be quite accessible and add to your design goals if used properly. For more information on accessible fonts, check out WebAIM's article on the topic . If you decided to communicate findings via a poster, website, or anything involving design, below are some hints/tips for the proper use of typography: I want you to read and understand the section on font choices from the Owl at Purdue . Then, play some rounds of Typeconnection . Pay attention to why or why not various pairings work. Then, I want you to consider what typographic pair would work best for the Capstone Exercise? Finally, the following tutorial shows how you can make a webpage that uses your paired fonts and explains why they work. The first part of this section then is to find a pair of fonts and to understand why they work best for you. Read the materials above, and once you're done with the Typeconnection site , go to Google Fonts and search for a pair that are suitable for your purpose . When you find a font you like, click the 'Add to collection' button. At the bottom of the screen, you'll see a link for 'use'. Click on this Google will ask you if you want to use any of the variants of the font. Tick off accordingly. Do you see the embed code that Google provides, and the code to integrate the font into your CSS (stylesheet)? Leave this window open we're going to use it in a moment. Make a new repository on GitHub. In your repository, click the button beside 'branch'. In the search box that opens, type in gh-pages . This will create a version of your repository that can be served as a website. You're now in the gh-pages branch. Click on the + sign (plus sign) beside the repository name. This will create a new file in your gh-branch of your repository. Call it myfontchoice.html (the .html file name is important to specify; otherwise your browser will not know how to render your page). You now need to put some HTML in there. I've written a simple webpage that will use two fonts from Google Fonts, and then applies the font to my text depending on whether or not it is a header, which you specify like this: h1 this is a header in between header tags /h1 or a paragraph, which you specify like this: p blah blah blah a paragraph of text blah blah blah /p . Right-click and open in a new tab my webpage on GitHub . Copy the HTML into your new HTML document. Commit your changes (ie. save). Let's see what we've got. To see the website version of your gh-pages branch, you go to yourusername .github.io/ reponame /myfontchoice.html (ie. the final part of the URL is the name of the document in your repo). Do that now. You should see a simple webpage, with two very distinctive fonts. Now let's slide your font choices into the HTML. Go to your HTML page in your gh-pages repo (ie. not the github.io version, but the github.com/ yourusername / repo /myfontchoice.html version). Hit the edit button. Look closely at line 6. Do you see my two fonts? Do you see the pipe character (the | symbol) between them? That tells Google you want both fonts. Navigate to the Google Fonts page again to grab the exact name for your fonts (uppercase letters make a difference!) and paste them into line 5 appropriately. Lines 8 and 14 specify which font to use for headers, and which font to use for body text. Change the lines appropriately. Change my silly text for a paragraph that explains what the fonts are, and why they work for this purpose. Commit your changes. Go to the github.io version of your repository (if you forget the address, you can find it under the 'Settings' tab on your normal repository page when you're in the gh-pages branch). Reload the page several times to clear the older version you've already looked at and to replace it with your version. Ta da! Not only have you thought carefully about typography and fonts, you've also learned how to serve a webpage from a GitHub repository. Hint: For basic HTML, W3schools has a really good guide to keep around . Going further with GitHub Pages Now that you have learned the basics of creating a simple website using GitHub, you have the ability to go further. Using the DH Box, you can build a simple website through GitHub Pages using the MkDocs static site generator. Navigate to the static site generator exercise using GitHub Pages in the supporting materials . Colour There's a lot of bumpf out there on the 'pyschology of colour'. Google it to see what I mean ( Youth Designer has a good example ). A lot of what you see here isn't so much psychology as it is associations (and indeed, western associations, at that). Associations are important of course; you don't want to undermine your message by making some unfortunate connections. If you decided to communicate findings via a website, below are some hints/tips for the proper use of colour: One where the colours support the broader message of the page. And the other where the colours undermine that broader message. Explain, in both cases, how your colour choices enhance and/or detract. Alternatively, you can make a one page slide in PowerPoint doing the same thing. Resources Below is a graphic and a video tutorial from Lynda.com (requires sign-up for free trial) to help with the theoretical side of things: Visit the tutorial Understanding the rules of color, Lynda.com (requires sign-up for free trial) To learn how to style your webpage appropriately, you can follow this tutorial on CSS from codeacademy.com . Below is a graphic that presents how colours are perceived in different cultures: Creating an online exhibit with Omeka Omeka is an open source Content Management System (CMS) for sharing digital collections and creating media-rich online exhibits. A CMS is a web program that manages web content including graphics, text, videos, and more. A CMS allows users to upload their content via a Graphical User Interface (GUI) and organizes it into appropriate folder structures. Traditional websites are programmed from the ground up everything from structure, to styling, to creating folders and specifiying where multi-media content is located. The main advantage of using a CMS is that it requires little to no actual background-level programming. That being said, anyone can edit a CMS's source code files, add their own, or modify them to fit their needs. Most CMSs are built with PHP , a web scripting language used to dynamically create content. However, most CMSs are also accompanied by massive native and third party plugin libraries that allow users to modify their website without touching any source code. Examples of CMSs include Wordpress, Drupal, Grav, Known, Joomla, and many more. Unlike many popular CMSs, Omeka is not a blogging platform. Omeka is used to create online exhibits. Today we are going to create a simple online exhibit. Our overall steps are as follows: Create a subdomain of your website where Omeka will live. This will allow you to host Omeka as an entity of its own on your website, keeping your main website separate. Install Omeka through cPanel on Reclaim Hosting. Create an online exhibit with several historical items. Change the style of our Omeka site by modifying the CSS of our theme. Creating a subdomain Login to your Reclaim Hosting account through the Reclaim Hosting login page . In the client area, select the cPanel tab. Under the Domains section, select Subdomains. Typically, a subdomain divides your overall website into distinct portions. For instance, you could have your landing page at mysite.com , your blog at blog.mysite.com , and your class work at hist3814.mysite.com . That way each subdomain could have different styles and serve different purposes, but still be connected to your overall domain. You can add as many subdomains to your site as you'd like. Subdomains are essentially just an easier way of organizing your files and showing distinct differences from your main domain. You could technically also do mysite.com/blog or mysite.com/hist3814 . In that case, however, it seems expected they would all be styled similarly to mysite.com . Furthermore, it might cause file structure issues to have, for example, Wordpress running your mysite.com and Omeka running mysite.com/Omeka . It's best to stick with your free subdomains. Enter the name of your online exhibit in the Subdomain field. You could simply put omeka , omeka-test , etc. or get more descriptive and say gatineau-fire-maps . Choose a title relevant to your Omeka site's purpose. Leave the Domain field set to your domain. The Document Root will be automatically created (an advantage of adding a subdomain!). I would not recommend changing this, but if you want to further organize your folder structure, you are able to change the location. Click the Create button. It will take a moment, but your new subdomain will be approved. NB Since the subdomain has nothing in it, if you navigate to it now, you will see 'Index of/' showing the file structure. This is normal. Installing Omeka Navigate back to your cPanel home page. Under the Applications section, select Omeka. On the right side, click the '+ install this application' button. Under the Location section, select the subdomain you created in the previous instructions. Do not choose your main mysite.com domain. Choose your omeka.mysite.com domain or your variant of it. You can choose the https or http version. Note, however, that https is more secure. Delete cms under Directory (Optional). We want Omeka installed to the root of our subdomain, not in a folder called cms . If you were installing Omeka to your main site, then you'd want a subdirectory called cms . Under the Version section, leave the settings as they are. Make sure 'I accept the license agreement' is selected. It's also good practice to allow it to update as needed and to allow backups. Under the Settings section, either create a new Administrator Username and Administrator Password or securely record the Administrator Username and Administrator Password generated for you. (You can click Show Password to show it.) REMEMBER YOUR LOGIN INFORMATION! You NEED the username and password to make changes to your Omeka site. Add an Administrator Email. Change the Website Title to a fitting and appropriate title. Under Advanced, leave the default settings. Click the Install button. You will be redirected to 'My Applications'. The installation may take a minute to process. Select the middle link of your new Omeka application that says omeka.mysite.com/admin or your variant of the subdomain. Login using your Administrator Username and your Administrator Password. At the top of the admin page, click on Settings. Scroll down to the bottom. In the ImageMagick Directory Path field, type /usr/bin . Reclaim Hosting comes with ImageMagick preinstalled. ImageMagick resizes and generates thumbnails of your images. Click Save Changes. Uploading content to Omeka For the purposes of this option, we will use the Gatineau Valley Historical Society fire insurance maps used in Module 4, Exercise 8 . You are also allowed to use any historical sources to create an exhibit. Maybe you want to choose historical newspapers that have been digitized from the Library and Archives or the war diary from Module 2, Exercise 2 . Just make sure to note where you got your sources and, preferably, that you can download them as images. On the left side, select Collections. Click Add a Collection. My new collection will be fire insurance maps. Under Title, add Maps of the Gatineau Valley Fire Insurance Plans. Go through each metadata field and fill in as much information as you can about your collection. Under Add Collection, click Public to make your collection publicly viewable. Click Add Collection. On the left side, select Items. Click Add an item. Navigate to the Gatineau Valley Historical Society Fire insurance maps or your own source of historical files. Using the information provided with first map, fill in as much information as you can about your item. Click on the map image. Right click and save the image somewhere handy like your desktop. Select the Files tab. Click Browse and choose your image file (note you cannot upload a single image larger than 128MB). On the right side under Collection, select your Collection and make sure it is set to Public. Click Add Item. It may take a few minutes to upload. On the right side, click the middle, blue View Public Page button. Go ahead and upload several exhibit items. If you can find other items related to your collection elsewhere, that'd be great too. Go to your variant of the omeka.mysite.com subdomain to view how the page looks to the public. Styling our exhibit (and making it more accessible!) At the top of the admin page, click Appearance. The default theme is 'Thanks, Roy'. You can choose any theme you want. They are all different and each has different advantages and disadvantages for editing. For this option, we will use the 'Thanks, Roy' theme. NB If you choose a different theme to modify, you may see different options. Proceed at your own intent. Click the blue Configure Theme button. Each theme configuration allows you to change basic style and content settings. With the configuration page open in one tab, open your omeka.mysite.com to view the public page. Whenever you make an update, simply refresh the public page to view it. Notice the Browse Items and Browse Collections links on the left of your public page are a bit light and hard to read. In the configuration, copy the text colour hex code #444444 . Navigate to the Color Hex website and search #444444 . Hex codes are an internet convention where a series of numbers or numbers and letters correspond with a unique colour. Scroll down to Shades of #444444 . I think #1b1b1b looks dark enough for a solid contrast. Navigate back to the Omeka configuration page and replace the Text and Links fields hex codes of #444444 with #1b1b1b . On the right side, click Save Changes. Refresh the public page and notice the text darkens. This will help make your website more readable. Using the browser inspector to make design easier Navigate to your public Omeka page. Right click on the \"Featured Item\" text. Select Inspect Element. The browser inspection tool allows you to view the code behind a website. NB Chrome and Firefox have great inspector tools. Not all browsers have these tools, however, notably Safari. The font is not that easy to read either. We will change it, but first let's find the current font. Scroll down on the right side under the Rules tab until you see the body CSS. This rule shows the font is called PT serif, written like the following: body { font-family: \"PT Serif\", Times, serif; } Go to Google Fonts to choose a new, more readable font. Search Roboto. Roboto is a clean, easy to read font. Click the red plus sign to select the font. It will add to a queue below. Click the black queue bar. Scroll to where it says Specify in CSS. Copy the text that says font-family: 'Roboto', sans-serif; . We will add this rule to our CSS file. Navigate to your cPanel home page and select File Manager at the top of the page. On the left side click on the folder for your Omeka site. The folder should have the same name as your domain. For instance, if your site is called omeka.mysite.com , the folder will be called omeka.mysite.com unless you changed the name earlier. Double click each folder until you are in the css folder: Themes Default css Right click style.css and select Edit. Reclaim Hosting has some great editing tools within the cPanel so you don't have to make updates within your desktop text editor and then reupload the files. Search font-family by hitting ctrl-f (Windows) or cmnd-f (Mac). We ONLY want to change the rules that say font-family: \"PT Serif\", Times, serif; . Use the search to replace each instance of font-family: \"PT Serif\", Times, serif; with font-family: 'Roboto', sans-serif; . Click the blue Save Changes button at the top right of the editor. Navigate to your Omeka site and refresh the page. Notice the font changes from PT Serif to Roboto. Before (default PT Serif font) After (updated Roboto font) Use your browser's inspector to find out the style rules of any specific element on a web page. If you make any major mistakes and want to start from scratch, you can copy the original style.css file from GitHub , paste it into your file, and save the changes. This will revert it back to the original style. NB It is important to note that for our purposes, we edited the default style.css file directly. This was to introduce you to CSS and making changes to the source code. Typically, you'd want to add a new file called, for example, custom.css and specify the path to that file in our source code. That way you keep your own changes distinct from the default source code files. However, that would involve editing different PHP rules which is beyond the scope of this option. This is very important for making upgrades. If your theme ever upgrades, you will lose all your changes if you only edited the default style.css . This applies to any file type. Therefore, you may want to keep a copy of your changes somewhere handy, like on your desktop. More Resources Below are more resources, tutorials, and things to explore. Accessibility and Design While most of this applies to websites, think also about how the general principles apply to other ways and means of telling our data stories. How People with Disabilities Use the Web Web Content Accessibility and Mobile Web Accessibility in HTML5 Web Accessibility Evaluation Tool Considering the User Perspective Constructing a POUR Website Percievable, Operable, Understandable, Robust. Applies much wider than design of websites. Checking Microsoft Office-generated documents for accessibility Infographics and Storytelling Infographics The Difference between Infographics and Visualizations Design hints for infographics Piktochart Infogr.am Easel.ly Storytelling Creatavist Medium Cowbird Exposure Timeline.js Storymap ManyLines ManyLines is an application that allows you to create narratives from network graphs. In essence, you upload a network file in .gexf format (which you can export from Gephi) and it renders it on the screen. There are some layout options to make the graph more intelligible. Then, you take a series of snapshots zoomed in on the graph in different places, and add text to describe what it is that's important about these networks. The app puts a Prezi-like wrapper around your snapshots, and the whole can then be embedded in a website or be used as a standalone website. Check out my first attempt on medialab. You can also embed nearly anything in the narrative panels Youtube videos, timeline.js as long as you know how to correctly format an iframe . To give this a try, why not use the Texan Correspondence network we generated in earlier modules? Export it in .gexf format from Gephi, import to ManyLines, and go! The interface is fairly straightforward. Just follow the prompts. Caveat Utilitor I don't know how long anything made with ManyLines will live on their website. But, knowing what you know about wget and other tools, do you see how you could archive a copy on your own machine? ManyLines is available on GitHub so you can certainly use it locally. Leaflet Maps can be created through a variety of services ( TileMill , Carto and mapbox for instance). These can then be embedded in your webpages or documents. Often, that's enough. But sometimes, you'd like to take control, and keep all the data, all the map, under your own name, in your own webspace. Visit the gentle introduction to using leaflet.js in our supporting materials to make, style, and serve your maps. Also visit a template on my GitHub repository for mapping with leaflet, drawing all of your point data and ancillary information from a CSV file. Leaflet also has a list of background maps you can use . Leaflet in the field: Pembroke Soundscapes Project Visit the Pembroke Soundscapes Project for an example of an academic historical project using Leaflet. This project uses an interactive map with sound clips to explore the industrial history of Pembroke, Ontario. Designing Maps with Processing and Illustrator Nicolas Felton is a renowned designer. Watch his 90 minute workshop on Skillshare . NB I do not use Processing or Illustrator, but Processing is free and Inkscape can do many of the things that Illustrator does.","title":"Capstone Exercise"},{"location":"module-5/Exercises/#capstone-exercise","text":"For the Capstone Exercise, you have the opportunity to create a domain (website + complete access to the webserver that powers it, so that you can install other platforms/services) of your own. A typical setup will be something along the lines of: GitHub: yourusername.github.io . Reclaim Hosting: your-domain-name.org or your-domain-name.ca or your-domain-name.com . On your blog/narrative, you may want an 'About' page where you can signal the kinds of history you're interested in, and the preferred way for people to get in touch with you. You do not have to use your real name. Remember the real names policy . This section begins with several options you may wish to pursue for the Capstone Exercise. Following these options are resources, tutorials, and tips/hints you can reference to support the Capstone Exercise you choose. Depending on how you decided to communicate findings, this section contains resources which introduce you to the following concepts: Layout Manipulating graphics Design with colour and font Creating digital exhibits I do not want to leave you thinking that all digital history work is necessarily text based. To that end, if you are looking for a challenge or for exposure to something rather different, I suggest you at least bookmark my series of tutorials on augmented reality, games, and 3d models for history and archaeology . Let's get started. 'Accidental Renaissance: the photos that look like Italian paintings' .","title":"Capstone Exercise"},{"location":"module-5/Exercises/#option-1-github-pages-with-a-static-site-generator","text":"Static site generators are software packages that allow you to quickly build and easily update a website via the terminal. Most static site generators use human readable programming languages like YAML / YML (Yet Another Markdown Language) that contain the settings for your website. You create a basic text file (i.e. Markdown), run a build command in the terminal that will convert your text files into web pages, and push the pages to your server. GitHub offers each user their own unique domain attached to their account - this service is called GitHub Pages, or gh-pages . Each user domain resembles yourusername.github.io , where yourusername is your GitHub account username. If you went to yourusername.github.io right now, it will show a 404 / 'site not found' error. This is because you have not yet set-up your gh-pages website. Like the master branch of your GitHub repository, gh-pages is another branch option. Essentially, you create a GitHub respository, populate the gh-pages branch with web files, and your yourusername.github.io will become a functioning website. By deafault, GitHub Pages uses Jekyll, a static site generator . However, Jekyll is not the only option. Hugo is another static site generator. This workbook was built using the static site generator MKDocs . While this option shows you how to use GitHub Pages powered by Jekyll to populate your GitHub domain, the instructions are generally similar for other static site generators. You make a few changes and run the website using a GitHub repository. Login to your GitHub account. Click the button labelled + at the top left of the page next to your account. Select New repository . Under Repository name * , enter yourusername.github.io (ensure you change 'yourusername' to your GitHub username). Click the button labelled Create repository . Navigate to your DH Box account and open the terminal (you can also set this up on your own machine, in which case you open the Terminal (Mac) or Command Prompt (Windows) or GitHub desktop client). Type $ git clone https://github.com/username/username.github.io into the terminal (changing the URL to your GitHub repository path). Type $ cd username.github.io into the terminal (ensure you change 'username' to your GitHub username). Type $ echo \"Hello World\" index.html into the terminal. This will pipe \"Hello World\" into the home file index.html of your new site. Type $ git add --all into the terminal. Type $ git commit -m \"Initial commit\" into the terminal. Type $ git push -u origin master into the terminal. Navigate to your domain at yourusername.github.io . You will see the text \"Hello World\".","title":"Option 1: GitHub Pages with a Static Site Generator"},{"location":"module-5/Exercises/#customize-your-website","text":"The instructions above showed you how to get you GitHub Pages domain up and running. GitHub provides further instruction on customizing your Jekyll site in their documentation . GitHub Pages makes it easy to set-up a theme. Navigate to your GitHub Pages repository. Click the tab labelled Settings . Scroll down to the section labelled GitHub Pages . Click the button labelled Change theme . Choose a new theme from the provided options. Open the _config.yml file and and click the pen/pencil icon to edit the file. Your configuration file will show only the theme information. Add the following YAML configuration settings to you _config.yml file: github: [metadata] encoding: UTF-8 kramdown: input: GFM hard_wrap: false future: true jailed: false gfm_quirks: paragraph_end Open the index.html file and and click the pen/pencil icon to edit the file. Change the name of the file to index.md . Remove the \"Hello World\" message and replace it with the following Markdown header: --- title: Home page layout: default --- Here is my home page. Add a commit message and click the green button labelled Commit changes . Navigate to your repository home page and create a new file called 2019-04-07-first-post.md (change the 2019-04-07 to today's date). Copy and paste the following text into your new Markdown file: --- title: This is my title layout: post --- Here is my page. Add a commit message and click the green button labelled Commit changes . Navigate to your yourusername.github.io domain. If the site has not updated yet, wait a minute and/or clear your browser's cache.","title":"Customize your website"},{"location":"module-5/Exercises/#going-further","text":"The instructions above show you how you can begin customizing you Jekyll website. You will probably want to change the theme and structure of the website. For instance, your blog posts should go in a folder called _posts . For more information, read Barry Clark's article on customizing your Jekyll site . You will also want to read through the Jekyll documentation . Most static site generators use similar layouts, structures, and YAML configurations. While the documentation for each generator will differ, the general idea between all of them remains the same. You may also want to build a simple website through GitHub Pages using the MkDocs static site generator and DH Box. Navigate to the static site generator exercise using GitHub Pages in the supporting materials .","title":"Going Further"},{"location":"module-5/Exercises/#option-2-jekyll-now","text":"Jekyll is a static site generator widely used to create a website quickly that you can run from your GitHub account. Jekyll takes static Markdown files and converts them to a functioning website. While Jekyll requires some editing via the terminal, Jekyll Now is a project from Barry Clark that allows you to leverage the blogging power and simplicity of Jekyll without touching the terminal. NB If you already have a gh-pages branch set up at yourusername.github.io , this may produce some unexpected results. Login to your GitHub account. Navigate to the Jekyll Now repository . For further instruction, follow the Jekyll Now Quick Start guide on GitHub . Fork the Jekyll Now repository. In your fork of the respository, click Settings . Under Repository name , change the name from jekyll-now to yourusername.github.io (ensure you change 'yourusername' to your GitHub username). By setting the name to your unique GitHub domain (i.e. gh-pages), GitHub knows to make your gh-pages domain draw from these files. Click the button labelled Rename . Your site is now available at yourusername.github.io . NB If you navigate to your domain URL right away, it will not be there and will show a '404' error. You first need to upload a post in the format YYYY-MM-DD-some-text.md , ensuring YYYY-MM-DD is always the prefix to your posts file name. Navigate back to your forked repository page. Open the _config.yml file. Click the pen/pencil icon to edit the file. Change the relevant fields (i.e. name , description , avatar , footer-links , etc.). For avatar , navigate to your GitHub profile, right click your profile picture/avatar, and select View image . This will open your avatar in a new tab. Copy the URL for your avatar and paste it into the _config.yml avatar section. Write a commit message and click the green button labelled Commit changes . Navigate to the _posts folder and open the 2014-3-3-Hello-World.md file. Click the pen/pencil icon to edit the file. Change the file name from 2014-3-3-Hello-World.md to something relevant, ensuring to change the prefix to today's date in the YYYY-MM-DD format. Edit the file to describe your new site. Write a commit message and click the green button labelled Commit changes . Navigate to yourusername.github.io ( remember , ensure you change 'yourusername' to your GitHub username). You should now see your udpated site. If your site still shows a 404 error, try clearing your browser's cache and trying again. Also, ensure your post files use the proper naming convention with the YYYY-MM-DD date prefix. To add new posts, navigate to the _posts folder and click the button labelled Create new file . Remember to use the proper naming convention with the YYYY-MM-DD date prefix.","title":"Option 2: Jekyll Now"},{"location":"module-5/Exercises/#going-further_1","text":"After following the instructions in the previous section, you now have a functioning website that runs entirely through GitHub. You can, however, further customize your site. Read through Barry Clark's article on customizing your Jekyll site . Incorporate design and functionality in your website from the resources on this page.","title":"Going Further"},{"location":"module-5/Exercises/#option-3-reclaim-hosting","text":"In the course space for cuLearn, I gave you a code to use to pay for a domain of your own. I have already purchased domain space for you from an academic web hosting service, Reclaim Hosting . This space will last for one year, at which point you have the option of paying to renew it or letting it die. Previous students in this course have used their domain to help with their applications for jobs and graduate school. Because you have complete access and control over your domain, you can install other services as well. For instance, maybe you use Dropbox or Google Drive to sync your files across machines, or to function as a backup? You can install a service called 'OwnCloud' on your own domain that does the same thing, so that you have control over all your own materials. You will be asked for the name you want to have for your space. You need to be thinking of branding here. Think of a professional name that conveys something of your personality and approach to history. I for instance own the domain, 'Electric Archaeology', which I chose to convey that I'm interested in digital archaeology, but also, that I move fast and cover a lot of breaking develops in the field (Hey. It's my blog. I like the name). Please choose wisely. Some combination of your first and last name is often the best idea, since your own name is your own best calling card ('shawn graham' is such a generic name, that it was already long gone by the time I started doing digital history). Type in a name, select a top-level domain (ie. .com, .ca, .org, etc. I'll suggest .ca), and click on the 'check availability' button. If the pop-up says 'Congratulations, domain available!' then click on the continue button. (You may be offered free id protection, where Reclaim Hosting will hide your details is someone does a 'who-is' search on the domain name. If it does, then tick off the check box to confirm that you want this, and hit continue). On the next screen, (the billing screen) fill in all of the information. The balance should be $0. At the bottom left, it will also say that you\u2019ve used a one-time promotional code. Hit the green button at the bottom to complete the purchase (which is not costing you anything). Congratulations! You now own your very own domain. It might take a bit of time for your domain to appear live on the web . During this time, you can log into your cPanel and install Wordpress and so on - see the next section below. Giving you a space of your own is my political act of protest against the centralization of learning inside learning management systems. Learning isn't 'managed', it's cultivated. Unlike cuLearn, I cannot monitor your activity inside your own domain. I can only see what you choose to make public. Unlike Facebook or Snapchat or Instagram, I am not trying to monetize your personality on the web.","title":"Option 3: Reclaim Hosting"},{"location":"module-5/Exercises/#wordpress-for-your-blog","text":"Wordpress is probably the best option for a platform for writing the narrative bits of your digital history work. Click the 'Client area'. It will tell you that you have one active account (with Reclaim Hosting) and one active domain. When the time comes to renew your account or to close it down, this is where you do it. Note also that there is a link to 'Support', which will put you in touch with Reclaim Hosting's help desk. They are extremely fast and good at providing support; always treat any help request you make with them as if you were writing a formal email to me. Be polite and considerate, and thank them. The owners of the business often are the ones who provide the help! Without them, we couldn't do this class. Go to 'cPanel' - this is where you can install all sorts of different kinds of software on your account. Search for and select 'Web applications' Click on Wordpress. Then click on 'Install this application'. The next screen presents you with a number of options. Leave these set to their defaults. For 'location', leave this blank (you want to leave the directory option blank). That tells the installatron to put Wordpress at your-domain.ca . (When/if you install other pieces of software, you'd change this variable so that the new software doesn't overwrite this software!) Further down the page, under 'Settings', you need to change 'Administrator username', 'Administrator password', 'Website title', 'Website tagline'. This is the username and password to actually do things with your blog, and the name of your blog itself . Leave everything else alone. Click Install! Once it's done, the installer will remind you of the URL to your site, and the URL to the blog's dashboard (where you go to write posts, make changes to the look and feel of the site). Open these in a new browser window, and bookmark them. To login to your blog, remember to go to the dashboard URL (eg. http://your-domain.ca/wp-admin ), enter your blog administrator username and password. You can close the cPanel webpage now (log out first).","title":"Wordpress for your blog"},{"location":"module-5/Exercises/#customising-your-blog","text":"If you look at the dashboard for your blog, you'll see a lot of options down the left side of your screen. If you want to change the look of your blog, click on 'Appearance' then click on 'Themes'. A number of themes are pre-installed. You can click on the different themes and select 'Preview' to see how your blog might look. When you find one you like, select 'activate'. Go to the other browser window where you have your-domain.ca open. Reload the page to see the changes take effect! If you're the sort of person who likes to sketch ideas out on paper, Lauren Heywood of the Disruptive Media Learning Lab has designed a paper-based exercise to prototype ideas. Why not give it a try? Print this PDF of Wordpress design (downloads in a new window) and follow the instructions. To write new content, know that there is a difference between a Post and a Page . A page is a new link on your site, while posts appear in chronological order on a page, with the most recent on top. Most themes show the most recent post by default, and pages appear in any menus on the site. When you are logged into your blog any post or page will have an 'Edit' button at the bottom that you can click. You'll then be presented with an editing box with the standard toolbar across the top (allowing you to change the font, insert images, change the alignment of the text and so on). At the top right will be a button to save or publish/update the post/page. Your blog will have a default 'about' page. Change that default text now to reflect something about who you are and why you are taking this course. To create new pages, you click on the 'Pages' link in your dashboard and select 'Add new'. To create new posts, you click on the 'Posts' link in your dashboard and select 'Add new'. Explore the options for your blog; customize and make the space your own. Password protected posts: If for any reason you feel that you don't want a post to be viewable by the web-at-large, you can hide it behind a password. At the top right where the 'Publish' button hides, click on 'Visibility' and select 'Password protected'. Remember though: you'll have to share the password with me for grading purposes. For more information about controlling visibility of your posts and so on, visit the Wordpress content visibility help page .","title":"Customising your blog"},{"location":"module-5/Exercises/#collecting-your-own-annotations-on-your-blog","text":"Hypothes.is has an API that allows you to do some neat things. 'API' stands for 'Application Programming Interface', which is just a fancy way of saying, 'you can write a program that interacts with this web service'. Kris Shaffer , a professor at the University of Mary Washington, has written a plugin for Wordpress that allows you to automatically collect annotations you make across the web and to display them all on a single page on your blog. So, we'll go get that plugin and install it on your blog: Open Kris Shaffer's Hypothes.is aggregator in a new browser window. Click 'Clone or Download' (the green button at the top right). In the pop-up, click 'Download ZIP' Go over to the dashboard for your blog (if you closed this, you can get there again by opening a new browser window and going to your-domain.ca/wp-admin ). In the dashboard, click on Plugins Add New. Click 'Upload Plugin'. It will open a window asking you to find and select the zip file you downloaded. This is probably in your 'Downloads' folder. Select and click Ok. Once it\u2019s uploaded and installed, click 'Activate'. In the dashboard, click on 'pages' then add a new page. Call it 'Web Notes' or 'Annotations' or something similar. Shaffer has created a 'shortcode' that tells Wordpress to go over to Hypothes.is and grab the latest information. So, in the text of your new page (in the editor window, make sure to click the 'text' button or else this won't work), enter the shortcode that will grab all of your public annotations: [hypothesis user = 'kris.shaffer'] where you remove the kris.shaffer and put in your own Hypothes.is username. Hit the 'Publish' button. Wordpress will report that the page has been published and give you a link to it; click on this link to see your new always-updating list of annotations! Of course, if you haven't made any public annotations yet, nothing will display. Go annotate something, then come back and reload the page, but remember to annotate using our HIST3814o group .","title":"Collecting your own annotations on your blog"},{"location":"module-5/Exercises/#layout","text":"'Layout' can mean different things in different contexts. A general overview on issues in layout is covered by 'What makes a design great', Lynda.com (requires sign-up for free trial) and 'Exploring principles of layout and composition' (requires sign-up for free trial) . The Slideshare on effective layouts gives you a sense of things to watch out for as well. For academic posters in particular, consider these poster design suggestions from the APA . In essence, good layout makes your argument legible, intuitive, and reinforces the rhetorical points you are trying to make. You should take into account 'principles of universal design' consider design issues with PowerPoint and websites (although some of the technical solutions proposed in those two documents are a bit out of date, the general principles hold!). If you decided to communicate findings via a poster, following are some hints/tips for designing a poster or modifying an existing poster.","title":"Layout"},{"location":"module-5/Exercises/#inkscape","text":"View the Inkscape exercise in our supporting materials for a gentle introduction to the software . Download one of the scientific poster templates from Ugo Sangiorgi (These are developed from Felix Breuer's blog post ; note his discussion on design). Open it in Inkscape. Make notes in your open notebook from the point of view of layout: what elements of the design work? What aren't working? How would you repurpose this poster to fit the requirements of the assessment exercise ( remember, the details in the syllabus )? Visit Inkscape's website for help with the basics of Inkscape . Modify the poster, and upload the SVG, PDF, or PNG version to your repository.","title":"Inkscape"},{"location":"module-5/Exercises/#powerpoint","text":"There's a lot of information out there on making posters with PowerPoint. Read parts 1, 2, and 3 of the Make Signs tutorial and then consider Colin Purrington's advice . Once you've read and digested the material, pick a poster from Pimp My Poster that sticks out to you. Make notes in your open notebook: from the point of view of layout what elements of the design work? What aren't working? How would you repurpose this poster to fit the requirements of the assessment exercises ( remember to visit GitHub for the details )? Grab a template from from Colin Purrington's website , and use it to prototype a poster that works. Upload the poster as a PDF to your repository. If you want to explore layout in the context of webpage creation, I would point you to the the roster of lessons at Codeacademy . Same instructions: find an existing site that you will consider from the point of view of what works, what doesn't, and use that reflection to guide the construction of your own.","title":"PowerPoint"},{"location":"module-5/Exercises/#typography","text":"Typography plays an important role in visual communication. It can do everything from reduce eyestrain (do a search for 'easiest fonts to read on screen') to communicate power and authority. Is it any wonder that strong bold capitals come to be known as 'Roman'? But first: are you a comic sans criminal? NB Serif fonts are sometimes not as accessible as sans-serif fonts. Older screens and monitors can have a difficult time rendering serif fonts. Sans-serif fonts can be more readable across different screens. However , this does not mean you should throw away serif fonts many serif fonts can be quite accessible and add to your design goals if used properly. For more information on accessible fonts, check out WebAIM's article on the topic . If you decided to communicate findings via a poster, website, or anything involving design, below are some hints/tips for the proper use of typography: I want you to read and understand the section on font choices from the Owl at Purdue . Then, play some rounds of Typeconnection . Pay attention to why or why not various pairings work. Then, I want you to consider what typographic pair would work best for the Capstone Exercise? Finally, the following tutorial shows how you can make a webpage that uses your paired fonts and explains why they work. The first part of this section then is to find a pair of fonts and to understand why they work best for you. Read the materials above, and once you're done with the Typeconnection site , go to Google Fonts and search for a pair that are suitable for your purpose . When you find a font you like, click the 'Add to collection' button. At the bottom of the screen, you'll see a link for 'use'. Click on this Google will ask you if you want to use any of the variants of the font. Tick off accordingly. Do you see the embed code that Google provides, and the code to integrate the font into your CSS (stylesheet)? Leave this window open we're going to use it in a moment. Make a new repository on GitHub. In your repository, click the button beside 'branch'. In the search box that opens, type in gh-pages . This will create a version of your repository that can be served as a website. You're now in the gh-pages branch. Click on the + sign (plus sign) beside the repository name. This will create a new file in your gh-branch of your repository. Call it myfontchoice.html (the .html file name is important to specify; otherwise your browser will not know how to render your page). You now need to put some HTML in there. I've written a simple webpage that will use two fonts from Google Fonts, and then applies the font to my text depending on whether or not it is a header, which you specify like this: h1 this is a header in between header tags /h1 or a paragraph, which you specify like this: p blah blah blah a paragraph of text blah blah blah /p . Right-click and open in a new tab my webpage on GitHub . Copy the HTML into your new HTML document. Commit your changes (ie. save). Let's see what we've got. To see the website version of your gh-pages branch, you go to yourusername .github.io/ reponame /myfontchoice.html (ie. the final part of the URL is the name of the document in your repo). Do that now. You should see a simple webpage, with two very distinctive fonts. Now let's slide your font choices into the HTML. Go to your HTML page in your gh-pages repo (ie. not the github.io version, but the github.com/ yourusername / repo /myfontchoice.html version). Hit the edit button. Look closely at line 6. Do you see my two fonts? Do you see the pipe character (the | symbol) between them? That tells Google you want both fonts. Navigate to the Google Fonts page again to grab the exact name for your fonts (uppercase letters make a difference!) and paste them into line 5 appropriately. Lines 8 and 14 specify which font to use for headers, and which font to use for body text. Change the lines appropriately. Change my silly text for a paragraph that explains what the fonts are, and why they work for this purpose. Commit your changes. Go to the github.io version of your repository (if you forget the address, you can find it under the 'Settings' tab on your normal repository page when you're in the gh-pages branch). Reload the page several times to clear the older version you've already looked at and to replace it with your version. Ta da! Not only have you thought carefully about typography and fonts, you've also learned how to serve a webpage from a GitHub repository. Hint: For basic HTML, W3schools has a really good guide to keep around .","title":"Typography"},{"location":"module-5/Exercises/#going-further-with-github-pages","text":"Now that you have learned the basics of creating a simple website using GitHub, you have the ability to go further. Using the DH Box, you can build a simple website through GitHub Pages using the MkDocs static site generator. Navigate to the static site generator exercise using GitHub Pages in the supporting materials .","title":"Going further with GitHub Pages"},{"location":"module-5/Exercises/#colour","text":"There's a lot of bumpf out there on the 'pyschology of colour'. Google it to see what I mean ( Youth Designer has a good example ). A lot of what you see here isn't so much psychology as it is associations (and indeed, western associations, at that). Associations are important of course; you don't want to undermine your message by making some unfortunate connections. If you decided to communicate findings via a website, below are some hints/tips for the proper use of colour: One where the colours support the broader message of the page. And the other where the colours undermine that broader message. Explain, in both cases, how your colour choices enhance and/or detract. Alternatively, you can make a one page slide in PowerPoint doing the same thing.","title":"Colour"},{"location":"module-5/Exercises/#resources","text":"Below is a graphic and a video tutorial from Lynda.com (requires sign-up for free trial) to help with the theoretical side of things: Visit the tutorial Understanding the rules of color, Lynda.com (requires sign-up for free trial) To learn how to style your webpage appropriately, you can follow this tutorial on CSS from codeacademy.com . Below is a graphic that presents how colours are perceived in different cultures:","title":"Resources"},{"location":"module-5/Exercises/#creating-an-online-exhibit-with-omeka","text":"Omeka is an open source Content Management System (CMS) for sharing digital collections and creating media-rich online exhibits. A CMS is a web program that manages web content including graphics, text, videos, and more. A CMS allows users to upload their content via a Graphical User Interface (GUI) and organizes it into appropriate folder structures. Traditional websites are programmed from the ground up everything from structure, to styling, to creating folders and specifiying where multi-media content is located. The main advantage of using a CMS is that it requires little to no actual background-level programming. That being said, anyone can edit a CMS's source code files, add their own, or modify them to fit their needs. Most CMSs are built with PHP , a web scripting language used to dynamically create content. However, most CMSs are also accompanied by massive native and third party plugin libraries that allow users to modify their website without touching any source code. Examples of CMSs include Wordpress, Drupal, Grav, Known, Joomla, and many more. Unlike many popular CMSs, Omeka is not a blogging platform. Omeka is used to create online exhibits. Today we are going to create a simple online exhibit. Our overall steps are as follows: Create a subdomain of your website where Omeka will live. This will allow you to host Omeka as an entity of its own on your website, keeping your main website separate. Install Omeka through cPanel on Reclaim Hosting. Create an online exhibit with several historical items. Change the style of our Omeka site by modifying the CSS of our theme.","title":"Creating an online exhibit with Omeka"},{"location":"module-5/Exercises/#creating-a-subdomain","text":"Login to your Reclaim Hosting account through the Reclaim Hosting login page . In the client area, select the cPanel tab. Under the Domains section, select Subdomains. Typically, a subdomain divides your overall website into distinct portions. For instance, you could have your landing page at mysite.com , your blog at blog.mysite.com , and your class work at hist3814.mysite.com . That way each subdomain could have different styles and serve different purposes, but still be connected to your overall domain. You can add as many subdomains to your site as you'd like. Subdomains are essentially just an easier way of organizing your files and showing distinct differences from your main domain. You could technically also do mysite.com/blog or mysite.com/hist3814 . In that case, however, it seems expected they would all be styled similarly to mysite.com . Furthermore, it might cause file structure issues to have, for example, Wordpress running your mysite.com and Omeka running mysite.com/Omeka . It's best to stick with your free subdomains. Enter the name of your online exhibit in the Subdomain field. You could simply put omeka , omeka-test , etc. or get more descriptive and say gatineau-fire-maps . Choose a title relevant to your Omeka site's purpose. Leave the Domain field set to your domain. The Document Root will be automatically created (an advantage of adding a subdomain!). I would not recommend changing this, but if you want to further organize your folder structure, you are able to change the location. Click the Create button. It will take a moment, but your new subdomain will be approved. NB Since the subdomain has nothing in it, if you navigate to it now, you will see 'Index of/' showing the file structure. This is normal.","title":"Creating a subdomain"},{"location":"module-5/Exercises/#installing-omeka","text":"Navigate back to your cPanel home page. Under the Applications section, select Omeka. On the right side, click the '+ install this application' button. Under the Location section, select the subdomain you created in the previous instructions. Do not choose your main mysite.com domain. Choose your omeka.mysite.com domain or your variant of it. You can choose the https or http version. Note, however, that https is more secure. Delete cms under Directory (Optional). We want Omeka installed to the root of our subdomain, not in a folder called cms . If you were installing Omeka to your main site, then you'd want a subdirectory called cms . Under the Version section, leave the settings as they are. Make sure 'I accept the license agreement' is selected. It's also good practice to allow it to update as needed and to allow backups. Under the Settings section, either create a new Administrator Username and Administrator Password or securely record the Administrator Username and Administrator Password generated for you. (You can click Show Password to show it.) REMEMBER YOUR LOGIN INFORMATION! You NEED the username and password to make changes to your Omeka site. Add an Administrator Email. Change the Website Title to a fitting and appropriate title. Under Advanced, leave the default settings. Click the Install button. You will be redirected to 'My Applications'. The installation may take a minute to process. Select the middle link of your new Omeka application that says omeka.mysite.com/admin or your variant of the subdomain. Login using your Administrator Username and your Administrator Password. At the top of the admin page, click on Settings. Scroll down to the bottom. In the ImageMagick Directory Path field, type /usr/bin . Reclaim Hosting comes with ImageMagick preinstalled. ImageMagick resizes and generates thumbnails of your images. Click Save Changes.","title":"Installing Omeka"},{"location":"module-5/Exercises/#uploading-content-to-omeka","text":"For the purposes of this option, we will use the Gatineau Valley Historical Society fire insurance maps used in Module 4, Exercise 8 . You are also allowed to use any historical sources to create an exhibit. Maybe you want to choose historical newspapers that have been digitized from the Library and Archives or the war diary from Module 2, Exercise 2 . Just make sure to note where you got your sources and, preferably, that you can download them as images. On the left side, select Collections. Click Add a Collection. My new collection will be fire insurance maps. Under Title, add Maps of the Gatineau Valley Fire Insurance Plans. Go through each metadata field and fill in as much information as you can about your collection. Under Add Collection, click Public to make your collection publicly viewable. Click Add Collection. On the left side, select Items. Click Add an item. Navigate to the Gatineau Valley Historical Society Fire insurance maps or your own source of historical files. Using the information provided with first map, fill in as much information as you can about your item. Click on the map image. Right click and save the image somewhere handy like your desktop. Select the Files tab. Click Browse and choose your image file (note you cannot upload a single image larger than 128MB). On the right side under Collection, select your Collection and make sure it is set to Public. Click Add Item. It may take a few minutes to upload. On the right side, click the middle, blue View Public Page button. Go ahead and upload several exhibit items. If you can find other items related to your collection elsewhere, that'd be great too. Go to your variant of the omeka.mysite.com subdomain to view how the page looks to the public.","title":"Uploading content to Omeka"},{"location":"module-5/Exercises/#styling-our-exhibit-and-making-it-more-accessible","text":"At the top of the admin page, click Appearance. The default theme is 'Thanks, Roy'. You can choose any theme you want. They are all different and each has different advantages and disadvantages for editing. For this option, we will use the 'Thanks, Roy' theme. NB If you choose a different theme to modify, you may see different options. Proceed at your own intent. Click the blue Configure Theme button. Each theme configuration allows you to change basic style and content settings. With the configuration page open in one tab, open your omeka.mysite.com to view the public page. Whenever you make an update, simply refresh the public page to view it. Notice the Browse Items and Browse Collections links on the left of your public page are a bit light and hard to read. In the configuration, copy the text colour hex code #444444 . Navigate to the Color Hex website and search #444444 . Hex codes are an internet convention where a series of numbers or numbers and letters correspond with a unique colour. Scroll down to Shades of #444444 . I think #1b1b1b looks dark enough for a solid contrast. Navigate back to the Omeka configuration page and replace the Text and Links fields hex codes of #444444 with #1b1b1b . On the right side, click Save Changes. Refresh the public page and notice the text darkens. This will help make your website more readable.","title":"Styling our exhibit (and making it more accessible!)"},{"location":"module-5/Exercises/#using-the-browser-inspector-to-make-design-easier","text":"Navigate to your public Omeka page. Right click on the \"Featured Item\" text. Select Inspect Element. The browser inspection tool allows you to view the code behind a website. NB Chrome and Firefox have great inspector tools. Not all browsers have these tools, however, notably Safari. The font is not that easy to read either. We will change it, but first let's find the current font. Scroll down on the right side under the Rules tab until you see the body CSS. This rule shows the font is called PT serif, written like the following: body { font-family: \"PT Serif\", Times, serif; } Go to Google Fonts to choose a new, more readable font. Search Roboto. Roboto is a clean, easy to read font. Click the red plus sign to select the font. It will add to a queue below. Click the black queue bar. Scroll to where it says Specify in CSS. Copy the text that says font-family: 'Roboto', sans-serif; . We will add this rule to our CSS file. Navigate to your cPanel home page and select File Manager at the top of the page. On the left side click on the folder for your Omeka site. The folder should have the same name as your domain. For instance, if your site is called omeka.mysite.com , the folder will be called omeka.mysite.com unless you changed the name earlier. Double click each folder until you are in the css folder: Themes Default css Right click style.css and select Edit. Reclaim Hosting has some great editing tools within the cPanel so you don't have to make updates within your desktop text editor and then reupload the files. Search font-family by hitting ctrl-f (Windows) or cmnd-f (Mac). We ONLY want to change the rules that say font-family: \"PT Serif\", Times, serif; . Use the search to replace each instance of font-family: \"PT Serif\", Times, serif; with font-family: 'Roboto', sans-serif; . Click the blue Save Changes button at the top right of the editor. Navigate to your Omeka site and refresh the page. Notice the font changes from PT Serif to Roboto. Before (default PT Serif font) After (updated Roboto font) Use your browser's inspector to find out the style rules of any specific element on a web page. If you make any major mistakes and want to start from scratch, you can copy the original style.css file from GitHub , paste it into your file, and save the changes. This will revert it back to the original style. NB It is important to note that for our purposes, we edited the default style.css file directly. This was to introduce you to CSS and making changes to the source code. Typically, you'd want to add a new file called, for example, custom.css and specify the path to that file in our source code. That way you keep your own changes distinct from the default source code files. However, that would involve editing different PHP rules which is beyond the scope of this option. This is very important for making upgrades. If your theme ever upgrades, you will lose all your changes if you only edited the default style.css . This applies to any file type. Therefore, you may want to keep a copy of your changes somewhere handy, like on your desktop.","title":"Using the browser inspector to make design easier"},{"location":"module-5/Exercises/#more-resources","text":"Below are more resources, tutorials, and things to explore.","title":"More Resources"},{"location":"module-5/Exercises/#accessibility-and-design","text":"While most of this applies to websites, think also about how the general principles apply to other ways and means of telling our data stories. How People with Disabilities Use the Web Web Content Accessibility and Mobile Web Accessibility in HTML5 Web Accessibility Evaluation Tool Considering the User Perspective Constructing a POUR Website Percievable, Operable, Understandable, Robust. Applies much wider than design of websites. Checking Microsoft Office-generated documents for accessibility","title":"Accessibility and Design"},{"location":"module-5/Exercises/#infographics-and-storytelling","text":"","title":"Infographics and Storytelling"},{"location":"module-5/Exercises/#infographics","text":"The Difference between Infographics and Visualizations Design hints for infographics Piktochart Infogr.am Easel.ly","title":"Infographics"},{"location":"module-5/Exercises/#storytelling","text":"Creatavist Medium Cowbird Exposure Timeline.js Storymap","title":"Storytelling"},{"location":"module-5/Exercises/#manylines","text":"ManyLines is an application that allows you to create narratives from network graphs. In essence, you upload a network file in .gexf format (which you can export from Gephi) and it renders it on the screen. There are some layout options to make the graph more intelligible. Then, you take a series of snapshots zoomed in on the graph in different places, and add text to describe what it is that's important about these networks. The app puts a Prezi-like wrapper around your snapshots, and the whole can then be embedded in a website or be used as a standalone website. Check out my first attempt on medialab. You can also embed nearly anything in the narrative panels Youtube videos, timeline.js as long as you know how to correctly format an iframe . To give this a try, why not use the Texan Correspondence network we generated in earlier modules? Export it in .gexf format from Gephi, import to ManyLines, and go! The interface is fairly straightforward. Just follow the prompts. Caveat Utilitor I don't know how long anything made with ManyLines will live on their website. But, knowing what you know about wget and other tools, do you see how you could archive a copy on your own machine? ManyLines is available on GitHub so you can certainly use it locally.","title":"ManyLines"},{"location":"module-5/Exercises/#leaflet","text":"Maps can be created through a variety of services ( TileMill , Carto and mapbox for instance). These can then be embedded in your webpages or documents. Often, that's enough. But sometimes, you'd like to take control, and keep all the data, all the map, under your own name, in your own webspace. Visit the gentle introduction to using leaflet.js in our supporting materials to make, style, and serve your maps. Also visit a template on my GitHub repository for mapping with leaflet, drawing all of your point data and ancillary information from a CSV file. Leaflet also has a list of background maps you can use .","title":"Leaflet"},{"location":"module-5/Exercises/#leaflet-in-the-field-pembroke-soundscapes-project","text":"Visit the Pembroke Soundscapes Project for an example of an academic historical project using Leaflet. This project uses an interactive map with sound clips to explore the industrial history of Pembroke, Ontario.","title":"Leaflet in the field: Pembroke Soundscapes Project"},{"location":"module-5/Exercises/#designing-maps-with-processing-and-illustrator","text":"Nicolas Felton is a renowned designer. Watch his 90 minute workshop on Skillshare . NB I do not use Processing or Illustrator, but Processing is free and Inkscape can do many of the things that Illustrator does.","title":"Designing Maps with Processing and Illustrator"},{"location":"module-5/Humanities Visualization/","text":"Humanities Visualization June 11-19, 2018 In this week, I want you to focus on the Capstone Exericse. You do not need to read or annotate any pieces this week. YOU DO have to write a blog post explaining what you've been up to this week. Concepts In this module, we will be exploring the nuts and bolts of visualization. However, we will also be thinking about what it means to visualize 'data' from a humanities perspective. Following Drucker, we're going to imagine what it means to think about our data not as things received (ie. empirically observed) but rather as capta , as things taken/transformed. It means visualizing and dealing with the intepretive process that got us to this point. What's more, we need to be aware of 'screen essentialism' and how it might be blinkering us to the possibilities of what humanities visualization could be. Finally, we need to be aware of the ways our digital 'templates' that we use reproduce ways of thinking and being that are antithetical to humanities' perspectives. The following are worth reading on these issues: Drucker, J. \"Humanities approaches to graphical display\". DHQ 2011.5 http://www.digitalhumanities.org/dhq/vol/5/1/000091/000091.html Williams, G. \"Disability, Universal Design, and the Digital Humanities\" Debates in the Digital Humanities 2012. http://dhdebates.gc.cuny.edu/debates/text/44 Owens, T. \"Discovery and Justification Are Different: Notes on Science-ing the Humanities\" http://www.trevorowens.org/2012/11/discovery-and-justification-are-different-notes-on-sciencing-the-humanities/ Owens, T. \"Defining Data for Humanists: Text, Artifact, Information, or Evidence?\" Journal of Digital Humanities 2011 1.1. http://journalofdigitalhumanities.org/1-1/defining-data-for-humanists-by-trevor-owens/ Watters, Audrey. \"Men (Still) Explain Technology to Me: Gender and Education Technology\" Hackeducation I also have a number of pieces of my own archaeological work that I think provide examples of how humanistic visualization can be a driver of interpretation and understanding. For instance, one thing I am currently working on is the possibility for sound to be a better representation of humanistic data . Oh, and by the way: maps are great, but sometimes, maps of ideas are even better; check out this landscape of Last.fm Folksonomy (PDF downloads in new tab) . (If you have any facility with Python, you might like this library that allows you to generate similar self-organizing maps ). Since Python 3 is installed in your DH Box, you're all set! (By the way, I find dabapps piece on python very helpful anytime I set out to do any Python work on my own computer.) What you need to do this week Work on your project. You have until midnight August 16th to submit it. See the Final Project requirements . Remember that all supporting files need to be in their own GitHub repository (it is not necessary to share the Canadian war diary files, unless you have created some sort of dataset from them), while the final project itself has to be mounted on your own domain. Talk to me and talk to each other in Slack. Feel free to collaborate, but keep a record of who does what and how much. If you missed completing a module, now might also be a good time to finish it (see 2.4 of the course manual ) Use the materials in this module to help make your project. Write a blog post describing what you've been up to THIS WEEK on your project (your successes, your failures, the help you may have found/received), and link to your 'faillog' (ie. the notes on your process that you upload to your GitHub). Readings No formal readings are assigned this week. Below you can watch a video of me sonifying a topic model of John Adam's mind.","title":"Communicating your Findings"},{"location":"module-5/Humanities Visualization/#humanities-visualization-june-11-19-2018","text":"In this week, I want you to focus on the Capstone Exericse. You do not need to read or annotate any pieces this week. YOU DO have to write a blog post explaining what you've been up to this week.","title":"Humanities Visualization &mdash; June 11-19, 2018"},{"location":"module-5/Humanities Visualization/#concepts","text":"In this module, we will be exploring the nuts and bolts of visualization. However, we will also be thinking about what it means to visualize 'data' from a humanities perspective. Following Drucker, we're going to imagine what it means to think about our data not as things received (ie. empirically observed) but rather as capta , as things taken/transformed. It means visualizing and dealing with the intepretive process that got us to this point. What's more, we need to be aware of 'screen essentialism' and how it might be blinkering us to the possibilities of what humanities visualization could be. Finally, we need to be aware of the ways our digital 'templates' that we use reproduce ways of thinking and being that are antithetical to humanities' perspectives. The following are worth reading on these issues: Drucker, J. \"Humanities approaches to graphical display\". DHQ 2011.5 http://www.digitalhumanities.org/dhq/vol/5/1/000091/000091.html Williams, G. \"Disability, Universal Design, and the Digital Humanities\" Debates in the Digital Humanities 2012. http://dhdebates.gc.cuny.edu/debates/text/44 Owens, T. \"Discovery and Justification Are Different: Notes on Science-ing the Humanities\" http://www.trevorowens.org/2012/11/discovery-and-justification-are-different-notes-on-sciencing-the-humanities/ Owens, T. \"Defining Data for Humanists: Text, Artifact, Information, or Evidence?\" Journal of Digital Humanities 2011 1.1. http://journalofdigitalhumanities.org/1-1/defining-data-for-humanists-by-trevor-owens/ Watters, Audrey. \"Men (Still) Explain Technology to Me: Gender and Education Technology\" Hackeducation I also have a number of pieces of my own archaeological work that I think provide examples of how humanistic visualization can be a driver of interpretation and understanding. For instance, one thing I am currently working on is the possibility for sound to be a better representation of humanistic data . Oh, and by the way: maps are great, but sometimes, maps of ideas are even better; check out this landscape of Last.fm Folksonomy (PDF downloads in new tab) . (If you have any facility with Python, you might like this library that allows you to generate similar self-organizing maps ). Since Python 3 is installed in your DH Box, you're all set! (By the way, I find dabapps piece on python very helpful anytime I set out to do any Python work on my own computer.)","title":"Concepts"},{"location":"module-5/Humanities Visualization/#what-you-need-to-do-this-week","text":"Work on your project. You have until midnight August 16th to submit it. See the Final Project requirements . Remember that all supporting files need to be in their own GitHub repository (it is not necessary to share the Canadian war diary files, unless you have created some sort of dataset from them), while the final project itself has to be mounted on your own domain. Talk to me and talk to each other in Slack. Feel free to collaborate, but keep a record of who does what and how much. If you missed completing a module, now might also be a good time to finish it (see 2.4 of the course manual ) Use the materials in this module to help make your project. Write a blog post describing what you've been up to THIS WEEK on your project (your successes, your failures, the help you may have found/received), and link to your 'faillog' (ie. the notes on your process that you upload to your GitHub).","title":"What you need to do this week"},{"location":"module-5/Humanities Visualization/#readings","text":"No formal readings are assigned this week. Below you can watch a video of me sonifying a topic model of John Adam's mind.","title":"Readings"},{"location":"supporting materials/","text":"A page tei","title":"Home"},{"location":"supporting materials/#a-page","text":"tei","title":"A page"},{"location":"supporting materials/cyoa.txt/","text":"Choose your own adventure! In this exercise, I want you to pick a text or network analysis tool from DIRT Directory's list or the DH Resources page and figure out how to use it. You can use our version of the Colonial Newspaper Database as your source text. Write up your own basic tutorial explaining what the tool does, how you got it to work on your data, and what you think it might be telling us or be useful for. Fork and add the link to your tutorial (which you'll have as a .md file in your repository) to this document.","title":"Choose Your Own Adventure"},{"location":"supporting materials/cyoa.txt/#choose-your-own-adventure","text":"In this exercise, I want you to pick a text or network analysis tool from DIRT Directory's list or the DH Resources page and figure out how to use it. You can use our version of the Colonial Newspaper Database as your source text. Write up your own basic tutorial explaining what the tool does, how you got it to work on your data, and what you think it might be telling us or be useful for. Fork and add the link to your tutorial (which you'll have as a .md file in your repository) to this document.","title":"Choose your own adventure!"},{"location":"supporting materials/geoparsing-w-python.txt/","text":"Geoparsing with Python This exercise draws from the work of Fred Gibbs . Extract, transform, and save as CSV Extract geocoded placenames from a text file Create a KML file with Python In this exercise, you will need to have Python installed on your machine. You can download it from Python's website . I have version 2.7.9 on this machine, and know that what follows works with that version. NB Mac comes presinstalled with Python. There is no need to install it again. You should also read and understand Fred Gibbs' tutorial on installing Python modules because you will need to install some helper modules. In Module 3, you used the NER to extract place names from a text. After some further munging with regex, you might have ended up with a CSV that looks like this one on my GitHub . Use Open Refine to open that CSV file. In the same way you tidied up in the Open Refine tutorial in Module 3 , clean up this CSV so that you merge together place names appropriately (ie. so that '4ustin' gets merged with 'Austin'). Do this for all the columns. Export the table as a new CSV call it cleaned-places.csv . Open that CSV in your spreadsheet program. Copy and paste all of the columns so that they become a single list (ie. one column of place names). Using your spreadsheet's filtering options, see if you can remove any more duplicates. (It might be useful to keep track of how many duplicates you delete, in a new file, eg. Texas,200 that kind of information might be handy, as in the mapping texts project (PDF downloads in new tab) ). Save the file you were removing the duplicates from (which has just a single column of unique place names) as placelist.txt . Now, at this point, we're going to open up our text editor and create a new Python program, following Gibbs' tutorial . His complete script is at the bottom of his post, but make sure you understand everything that is going on. Do you see the places where he has to import new Python modules to make his script work? Make sure you've installed those modules. Let's call your completed script geoparse.py . Done that? Good. Open your terminal, navigate to the folder you're working in, and run your script by typing the following: $ python geoparse.py Did it work? Did you get an error message? It's entirely possible that you got this message: Traceback (most recent call last): File geolocate.py , line 14, in module lat = json['results'][0]['geometry']['location']['lat'] IndexError: list index out of range ...but check your folder. Do you have a geocoded-places.txt file? If so, it worked! Or at least, it got most of your places from the Google maps API. (For the rest, you can try uploading your places list to Scargill's geoparser and then copying and pasting the output to an Excel file. This parser will give you several columns of results, where the first column represents its best guess and the other columns other possibilities). You can now import your geocoded places into many other software packages. Gibbs also shows us how to convert our list into KML, the format that Google Earth and Google Maps can read. Try out Gibbs' tutorial on creating KML files . You can double-click on the resulting KML file, and if you have Google Earth installed, it will open up there. Within Google Earth, you can start adding more information, other annotations... pretty soon, you'll have a complete map! Remember to upload your scripts, data, and obersvations to your open notebook. (Incidentally, if you wanted to load this material into Palladio you'd need a file that looked like this the following. Place Coordinates MEXICO 23.634501,-102.552784 California 36.778261,-119.4179324 Brazos 32.661389,-98.121667 etc. That is, a tab between 'Place' and 'Coordinates' in the first line, a tab between 'Mexico' and the latitude, and a comma between latitude and logitude. Best way to effect this transformation? Probably using regex. (It's unfortunate that Palladio doesn't accept straightforward place, latitude, longitude comma separated data.)","title":"Geoparsing with Python"},{"location":"supporting materials/geoparsing-w-python.txt/#geoparsing-with-python","text":"This exercise draws from the work of Fred Gibbs . Extract, transform, and save as CSV Extract geocoded placenames from a text file Create a KML file with Python In this exercise, you will need to have Python installed on your machine. You can download it from Python's website . I have version 2.7.9 on this machine, and know that what follows works with that version. NB Mac comes presinstalled with Python. There is no need to install it again. You should also read and understand Fred Gibbs' tutorial on installing Python modules because you will need to install some helper modules. In Module 3, you used the NER to extract place names from a text. After some further munging with regex, you might have ended up with a CSV that looks like this one on my GitHub . Use Open Refine to open that CSV file. In the same way you tidied up in the Open Refine tutorial in Module 3 , clean up this CSV so that you merge together place names appropriately (ie. so that '4ustin' gets merged with 'Austin'). Do this for all the columns. Export the table as a new CSV call it cleaned-places.csv . Open that CSV in your spreadsheet program. Copy and paste all of the columns so that they become a single list (ie. one column of place names). Using your spreadsheet's filtering options, see if you can remove any more duplicates. (It might be useful to keep track of how many duplicates you delete, in a new file, eg. Texas,200 that kind of information might be handy, as in the mapping texts project (PDF downloads in new tab) ). Save the file you were removing the duplicates from (which has just a single column of unique place names) as placelist.txt . Now, at this point, we're going to open up our text editor and create a new Python program, following Gibbs' tutorial . His complete script is at the bottom of his post, but make sure you understand everything that is going on. Do you see the places where he has to import new Python modules to make his script work? Make sure you've installed those modules. Let's call your completed script geoparse.py . Done that? Good. Open your terminal, navigate to the folder you're working in, and run your script by typing the following: $ python geoparse.py Did it work? Did you get an error message? It's entirely possible that you got this message: Traceback (most recent call last): File geolocate.py , line 14, in module lat = json['results'][0]['geometry']['location']['lat'] IndexError: list index out of range ...but check your folder. Do you have a geocoded-places.txt file? If so, it worked! Or at least, it got most of your places from the Google maps API. (For the rest, you can try uploading your places list to Scargill's geoparser and then copying and pasting the output to an Excel file. This parser will give you several columns of results, where the first column represents its best guess and the other columns other possibilities). You can now import your geocoded places into many other software packages. Gibbs also shows us how to convert our list into KML, the format that Google Earth and Google Maps can read. Try out Gibbs' tutorial on creating KML files . You can double-click on the resulting KML file, and if you have Google Earth installed, it will open up there. Within Google Earth, you can start adding more information, other annotations... pretty soon, you'll have a complete map! Remember to upload your scripts, data, and obersvations to your open notebook. (Incidentally, if you wanted to load this material into Palladio you'd need a file that looked like this the following. Place Coordinates MEXICO 23.634501,-102.552784 California 36.778261,-119.4179324 Brazos 32.661389,-98.121667 etc. That is, a tab between 'Place' and 'Coordinates' in the first line, a tab between 'Mexico' and the latitude, and a comma between latitude and logitude. Best way to effect this transformation? Probably using regex. (It's unfortunate that Palladio doesn't accept straightforward place, latitude, longitude comma separated data.)","title":"Geoparsing with Python"},{"location":"supporting materials/gephi.txt/","text":"Working with Gephi Before we go much further, I would recommend that you look at the work of Clement Levallois, who has a suite of excellent tutorials on working with Gephi . The tutorial below is adapted from our open draft of The Macroscope . Introduction Gephi is quickly becoming the tool of choice for network analysts who do not need the full suite of algorithms offered by Pajek or UCINET . It is relatively easy to use (eclipsed in this only by NodeXL ), it is usable on all platforms, it can analyze fairly large networks, and it creates beautiful visualizations. The development community is also extremely active, with improvements being added constantly. We recommend Gephi for the majority of historians undertaking serious network analysis research. Download and install Gephi onto your machine. Installing Gephi on OS 10 Mavericks NB Since writing this, Gephi announced the release of Gephi 0.9 . This newer Gephi should solve these problems. Mac users might have some trouble installing Gephi 0.8. We have found that, on Mac OS X Mavericks, Gephi does not load properly after installation. This is a Java-related issue, so you\u2019ll need to install an earlier version of Java than the one provided. To fix this: Control click (or right-click) on the Gephi package. Select \u201cshow package contents.\u201d Click on \u201cContents Resources Gephi etc.\u201d Control-click (or right-click) on gephi.conf and open with your text editor. Find the line reading: #jdkhome=\"/path/to/jdk\" and paste the following code: jdkhome=\"/System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Home Save that file. Then, go to Apple support and install the older version of Java (Java 6). Once that is installed, Gephi should run normally. Run Gephi once it is installed. You will be presented with a welcome prompting you to Open a recent file, Create a new project, or Load a sample file. Click \u201cNew Project\u201d and then click the \u201cData Laboratory\u201d tab on the horizontal bar at the top of the Gephi window. When Not To Use Networks Networks analysis can be a powerful method for engaging with a historical dataset but it is often not the most appropriate. Any historical database may be represented as a network with enough contriving but few should be. One could take the Atlantic trade network, including cities, ships, relevant governments and corporations, and connect them all together in a giant multipartite network. It would be extremely difficult to derive any meaning from this network, especially using traditional network analysis methodologies. Clustering coefficients (a useful network metric), for example, would be meaningless in this situation, as it would be impossible for the neighbors of any one node to be neighbors with one another. Network scientists have developed algorithms for multimodal networks , but they are often created for fairly specific purposes, and one should be very careful before applying them to a different dataset and using that analysis to explore a historiographical question. Networks, as they are commonly encoded, also suffer from a profound lack of nuance. It is all well to say that, because Henry Oldenburg corresponded with both Gottfried Leibniz and John Milton, he was the short connection between the two men. However, the encoded network holds no information of whether Oldenburg actually transferred information between the two or whether that connection was at all meaningful. In a flight network, Las Vegas and Atlanta might both have very high betweenness centrality because people from many other cities fly to or from those airports, and yet one is much more of a hub than the other. This is because, largely, people tend to fly directly back and forth from Las Vegas, rather than through it, whereas people tend to fly through Atlanta from one city to another. This is the type of information that network analysis, as it is currently used, is not well equipped to handle. Whether this shortcoming affects a historical inquiry depends primarily on the inquiry itself. When deciding whether to use networks to explore a historiographical question, the first question should be: to what extent is connectivity specifically important to this research project? The next should be: can any of the network analyses my collaborators or I know how to employ be specifically useful in the research project? If either answer is negative, another approach is probably warranted. Texan Correspondence You will need the information you created in Module 3, after cleaning the correspondence data using Open Refine . Umm, I never did manage that Open Refine stuff... In Module 3, you used Notepad++ or Textwrangler, regular expressions, and Open Refine to create a comma-separated value file like ****.csv of the diplomatic correspondence of the Republic of Texas. The final version of the file you created has a row for each letter listed in the volume, and in each row the name of the sender and the recipient. The file should look like this: source,target Sam Houston,J. Pinckney Henderson James Webb,Alc6e La Branche David G. Burnet,Richard G. Dunlap ... This file is called 'an edge list' it's a list of connections, or edges, between the individuals. If you no longer have the file, you can find it on The Macroscope website . Quick instructions for getting the data into Gephi Open Gephi by double-clicking its icon. Click \"New project\". The middle pane of the interface window is the \u201cData Laboratory,\u201d where you can interact with your network data in spreadsheet format. This is where we can import the data cleaned up in Open Refine. In the Data Laboratory, select \u201cImport Spreadsheet.\u201d Press the ellipsis \u201c...\u201d and locate the CSV you created. Make sure that the Separator is listed as \u201cComma\u201d and the \u201cAs table\u201d is listed as \u201cEdges table.\u201d Press \u201cNext,\u201d then \u201cFinish.\u201d Your data should load up. Click on the \"Overview\u201d tab and you will be presented with a tangled network graph. Navigating Gephi Gephi is broken up into three panes: Overview , Data Laboratory , and Preview . The Overview pane is used to manipulate the visual properties of the network: change colors of nodes or edges, lay them out in different ways, and so forth. The Overview pane is also where you can apply algorithms to the network, like those you learned about in the previous chapter. The Data Laboratory is for adding, manipulating, and removing data. Use the Preview pane to do some final tweaks on the look and feel of the network and to export an image for publication. There is one tweak that needs to done in the Data Table before the dataset is fully ready to be explored in Gephi. Click on the Data Laboratory tab. Click on the \u201cNodes\u201d tab in the Data Table (this should be open already) and notice that, of the three columns, \u201cLabel\u201d (the furthermost field on the right) is blank in every row. This will be a problem when viewing the network visualization, as those labels are essential for the network to be meaningful. In the \u201cNodes\u201d tab, click \u201cCopy data to other column\u201d at the bottom, select \u201cID\u201d, and press \u201cOk\u201d. Upon doing so, the \u201cLabel\u201d column will be filled with the appropriate labels for each correspondent. While you\u2019re still in the Data Laboratory, look in the \u201cEdges\u201d tab and notice there is a \u201cWeight\u201d column. Gephi automatically counted every time a letter was sent from correspondent A to correspondent B and summed up all the occurrences, resulting in the \u201cWeight.\u201d This means that J. Pinckney Henderson sent three letters to James Webb, because Henderson is in the \u201cSource\u201d column, Webb in the \u201cTarget\u201d, and the \u201cWeight\u201d is three. Clicking on the Overview pane will take you to a visual representation of the network you just imported. In the middle of the screen, you will see your network in the \u201cGraph\u201d tab. The \u201cContext\u201d tab, at the top right, will show that you imported 234 nodes and 394 edges. At first, all the nodes will be randomly strewn across the screen and make little visual sense. Fix the nodes by selecting a layout in the \u201cLayout\u201d tab \u2013 the best one for beginners is \u201cForce Atlas 2.\u201d Press the \u201cRun\u201d button and watch the nodes and edges reorganize on the screen into something slightly more manageable. After the layout runs for a few minutes, re-press the button (now labeled \u201cStop\u201d) to settle the nodes in their place. You just ran a force-directed layout. Each dot is a correspondent in the network, and lines between dots represent letters sent between individuals. Thicker lines represent more letters, and arrows represent the direction the letters were sent, such that there may be up to two lines connecting any two correspondents (one for each direction). About two-dozen smaller components of the network will appear to shoot off into the distance, unconnected from the large, connected component in the middle. For the purpose of this exercise, we are not interested in those disconnected components, so the next step will be to filter them out of the network. The first step is to calculate which components of the network are connected to which others; do this by clicking \u201cRun\u201d next to the text that says \u201cConnected Components\u201d in the \u201cStatistics\u201d tab on the right side. Once there, select \u201cUnDirected\u201d and press \u201cOK.\u201d Press \u201cClose\u201d when the report pops up indicating that the algorithm has finished running. Now that this is done, Gephi knows which is the giant connected component and has labeled that component \u201c0\u201d. To filter out everything but the giant component, click on the \u201cFilters\u201d tab on the right side and browse to \"Component ID Integer (Node)\" in the folder directory (you\u2019ll find it under \"Attributes,\" then \"Equal\"). Double-click \"Component ID Integer (Node)\" and click the \"Filter\" button at the bottom. Doing this removes the disconnected bundles of nodes. There are many possible algorithms you could use for the analysis step, but in this case you will use the PageRank of each node in the network. This measurement calculates the prestige of a correspondent according to how often others write to him or her. The process is circular, such that correspondents with high prestige will confer their prestige on those they write to, who in turn pass their prestige along to their own correspondents. For the moment let us take its results to equate with a correspondent\u2019s importance in the Republic of Texas letter network. Calculate the PageRank by clicking on the \"Run\" button next to \"PageRank\" in the \"Statistics\" tab. You will be presented with a prompt asking for a few parameters; make sure \"Directed\" network is selected and that the algorithm is taking edge weight into account (by selecting \"Use edge weight\"). Leave all other parameters at their default. Press \"OK\". Once PageRank is calculated, if you click back into the \"Data Laboratory\" and select the \"Nodes\" list in the Data Table, you can see that a new \"PageRank\" column has been added, with values for every node. The higher the PageRank, the more central a correspondent is in the network. Going back to the Overview pane, you can visualize this centrality by changing the size of each correspondent\u2019s node based on its PageRank. Do this in the \"Ranking\" tab on the left side of the Overview pane. Make sure \"Nodes\" is selected, press the icon of a little red diamond, and select PageRank from the drop-down menu. In the parameter options just below, enter the \"Min size\" as 1 and the \"Max size\" as 10. Press \"Apply,\" and watch the nodes resize based on their PageRank. To be on the safe side and decrease clutter, re-run the \"Force Atlas 2\" layout as described above, making sure to keep the \"Prevent Overlap\" box checked. At this point, the network is processed enough to visualize in the Preview pane, to finally begin making sense of the data. In Preview, on the left side, select \"Show Labels,\" \"Proportional Size,\" \"Rescale Weight,\" and deselect \"Curved\" edges. Press \"Refresh.\" So what have we got? The visualization immediately reveals apparent structure: central figures on the top (Ashbel Smith and Anson Jones) and bottom (Mirabeau B. Lamar, James Webb, and James Treat), and two central figures who connect the two split communities (James Hamilton and James S. Mayfield). A quick search online shows the top network to be associated with the last president of the Republic of Texas, Anson Jones; whereas the bottom network largely revolves around the second president, Mirabeau Lamar. Experts on this period in history could use this analysis to understand the structure of communities building to the annexation of Texas or they could ask meta-questions about the nature of the data themselves. For example, why is Sam Houston, the first and third president of the Republic of Texas, barely visible in this network? Write up your own observations on this process in your open notebook, and export your Gephi file as a .graphml file (because Gephi's .gephi format is a bit unstable, always save as or export your work in a variety of formats). Upload that to your repository.","title":"Social Network Analysis with Gephi"},{"location":"supporting materials/gephi.txt/#working-with-gephi","text":"Before we go much further, I would recommend that you look at the work of Clement Levallois, who has a suite of excellent tutorials on working with Gephi . The tutorial below is adapted from our open draft of The Macroscope .","title":"Working with Gephi"},{"location":"supporting materials/gephi.txt/#introduction","text":"Gephi is quickly becoming the tool of choice for network analysts who do not need the full suite of algorithms offered by Pajek or UCINET . It is relatively easy to use (eclipsed in this only by NodeXL ), it is usable on all platforms, it can analyze fairly large networks, and it creates beautiful visualizations. The development community is also extremely active, with improvements being added constantly. We recommend Gephi for the majority of historians undertaking serious network analysis research. Download and install Gephi onto your machine.","title":"Introduction"},{"location":"supporting materials/gephi.txt/#installing-gephi-on-os-10-mavericks","text":"NB Since writing this, Gephi announced the release of Gephi 0.9 . This newer Gephi should solve these problems. Mac users might have some trouble installing Gephi 0.8. We have found that, on Mac OS X Mavericks, Gephi does not load properly after installation. This is a Java-related issue, so you\u2019ll need to install an earlier version of Java than the one provided. To fix this: Control click (or right-click) on the Gephi package. Select \u201cshow package contents.\u201d Click on \u201cContents Resources Gephi etc.\u201d Control-click (or right-click) on gephi.conf and open with your text editor. Find the line reading: #jdkhome=\"/path/to/jdk\" and paste the following code: jdkhome=\"/System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Home Save that file. Then, go to Apple support and install the older version of Java (Java 6). Once that is installed, Gephi should run normally. Run Gephi once it is installed. You will be presented with a welcome prompting you to Open a recent file, Create a new project, or Load a sample file. Click \u201cNew Project\u201d and then click the \u201cData Laboratory\u201d tab on the horizontal bar at the top of the Gephi window.","title":"Installing Gephi on OS 10 Mavericks"},{"location":"supporting materials/gephi.txt/#when-not-to-use-networks","text":"Networks analysis can be a powerful method for engaging with a historical dataset but it is often not the most appropriate. Any historical database may be represented as a network with enough contriving but few should be. One could take the Atlantic trade network, including cities, ships, relevant governments and corporations, and connect them all together in a giant multipartite network. It would be extremely difficult to derive any meaning from this network, especially using traditional network analysis methodologies. Clustering coefficients (a useful network metric), for example, would be meaningless in this situation, as it would be impossible for the neighbors of any one node to be neighbors with one another. Network scientists have developed algorithms for multimodal networks , but they are often created for fairly specific purposes, and one should be very careful before applying them to a different dataset and using that analysis to explore a historiographical question. Networks, as they are commonly encoded, also suffer from a profound lack of nuance. It is all well to say that, because Henry Oldenburg corresponded with both Gottfried Leibniz and John Milton, he was the short connection between the two men. However, the encoded network holds no information of whether Oldenburg actually transferred information between the two or whether that connection was at all meaningful. In a flight network, Las Vegas and Atlanta might both have very high betweenness centrality because people from many other cities fly to or from those airports, and yet one is much more of a hub than the other. This is because, largely, people tend to fly directly back and forth from Las Vegas, rather than through it, whereas people tend to fly through Atlanta from one city to another. This is the type of information that network analysis, as it is currently used, is not well equipped to handle. Whether this shortcoming affects a historical inquiry depends primarily on the inquiry itself. When deciding whether to use networks to explore a historiographical question, the first question should be: to what extent is connectivity specifically important to this research project? The next should be: can any of the network analyses my collaborators or I know how to employ be specifically useful in the research project? If either answer is negative, another approach is probably warranted.","title":"When Not To Use Networks"},{"location":"supporting materials/gephi.txt/#texan-correspondence","text":"You will need the information you created in Module 3, after cleaning the correspondence data using Open Refine .","title":"Texan Correspondence"},{"location":"supporting materials/gephi.txt/#umm-i-never-did-manage-that-open-refine-stuff","text":"In Module 3, you used Notepad++ or Textwrangler, regular expressions, and Open Refine to create a comma-separated value file like ****.csv of the diplomatic correspondence of the Republic of Texas. The final version of the file you created has a row for each letter listed in the volume, and in each row the name of the sender and the recipient. The file should look like this: source,target Sam Houston,J. Pinckney Henderson James Webb,Alc6e La Branche David G. Burnet,Richard G. Dunlap ... This file is called 'an edge list' it's a list of connections, or edges, between the individuals. If you no longer have the file, you can find it on The Macroscope website .","title":"Umm, I never did manage that Open Refine stuff..."},{"location":"supporting materials/gephi.txt/#quick-instructions-for-getting-the-data-into-gephi","text":"Open Gephi by double-clicking its icon. Click \"New project\". The middle pane of the interface window is the \u201cData Laboratory,\u201d where you can interact with your network data in spreadsheet format. This is where we can import the data cleaned up in Open Refine. In the Data Laboratory, select \u201cImport Spreadsheet.\u201d Press the ellipsis \u201c...\u201d and locate the CSV you created. Make sure that the Separator is listed as \u201cComma\u201d and the \u201cAs table\u201d is listed as \u201cEdges table.\u201d Press \u201cNext,\u201d then \u201cFinish.\u201d Your data should load up. Click on the \"Overview\u201d tab and you will be presented with a tangled network graph.","title":"Quick instructions for getting the data into Gephi"},{"location":"supporting materials/gephi.txt/#navigating-gephi","text":"Gephi is broken up into three panes: Overview , Data Laboratory , and Preview . The Overview pane is used to manipulate the visual properties of the network: change colors of nodes or edges, lay them out in different ways, and so forth. The Overview pane is also where you can apply algorithms to the network, like those you learned about in the previous chapter. The Data Laboratory is for adding, manipulating, and removing data. Use the Preview pane to do some final tweaks on the look and feel of the network and to export an image for publication. There is one tweak that needs to done in the Data Table before the dataset is fully ready to be explored in Gephi. Click on the Data Laboratory tab. Click on the \u201cNodes\u201d tab in the Data Table (this should be open already) and notice that, of the three columns, \u201cLabel\u201d (the furthermost field on the right) is blank in every row. This will be a problem when viewing the network visualization, as those labels are essential for the network to be meaningful. In the \u201cNodes\u201d tab, click \u201cCopy data to other column\u201d at the bottom, select \u201cID\u201d, and press \u201cOk\u201d. Upon doing so, the \u201cLabel\u201d column will be filled with the appropriate labels for each correspondent. While you\u2019re still in the Data Laboratory, look in the \u201cEdges\u201d tab and notice there is a \u201cWeight\u201d column. Gephi automatically counted every time a letter was sent from correspondent A to correspondent B and summed up all the occurrences, resulting in the \u201cWeight.\u201d This means that J. Pinckney Henderson sent three letters to James Webb, because Henderson is in the \u201cSource\u201d column, Webb in the \u201cTarget\u201d, and the \u201cWeight\u201d is three. Clicking on the Overview pane will take you to a visual representation of the network you just imported. In the middle of the screen, you will see your network in the \u201cGraph\u201d tab. The \u201cContext\u201d tab, at the top right, will show that you imported 234 nodes and 394 edges. At first, all the nodes will be randomly strewn across the screen and make little visual sense. Fix the nodes by selecting a layout in the \u201cLayout\u201d tab \u2013 the best one for beginners is \u201cForce Atlas 2.\u201d Press the \u201cRun\u201d button and watch the nodes and edges reorganize on the screen into something slightly more manageable. After the layout runs for a few minutes, re-press the button (now labeled \u201cStop\u201d) to settle the nodes in their place. You just ran a force-directed layout. Each dot is a correspondent in the network, and lines between dots represent letters sent between individuals. Thicker lines represent more letters, and arrows represent the direction the letters were sent, such that there may be up to two lines connecting any two correspondents (one for each direction). About two-dozen smaller components of the network will appear to shoot off into the distance, unconnected from the large, connected component in the middle. For the purpose of this exercise, we are not interested in those disconnected components, so the next step will be to filter them out of the network. The first step is to calculate which components of the network are connected to which others; do this by clicking \u201cRun\u201d next to the text that says \u201cConnected Components\u201d in the \u201cStatistics\u201d tab on the right side. Once there, select \u201cUnDirected\u201d and press \u201cOK.\u201d Press \u201cClose\u201d when the report pops up indicating that the algorithm has finished running. Now that this is done, Gephi knows which is the giant connected component and has labeled that component \u201c0\u201d. To filter out everything but the giant component, click on the \u201cFilters\u201d tab on the right side and browse to \"Component ID Integer (Node)\" in the folder directory (you\u2019ll find it under \"Attributes,\" then \"Equal\"). Double-click \"Component ID Integer (Node)\" and click the \"Filter\" button at the bottom. Doing this removes the disconnected bundles of nodes. There are many possible algorithms you could use for the analysis step, but in this case you will use the PageRank of each node in the network. This measurement calculates the prestige of a correspondent according to how often others write to him or her. The process is circular, such that correspondents with high prestige will confer their prestige on those they write to, who in turn pass their prestige along to their own correspondents. For the moment let us take its results to equate with a correspondent\u2019s importance in the Republic of Texas letter network. Calculate the PageRank by clicking on the \"Run\" button next to \"PageRank\" in the \"Statistics\" tab. You will be presented with a prompt asking for a few parameters; make sure \"Directed\" network is selected and that the algorithm is taking edge weight into account (by selecting \"Use edge weight\"). Leave all other parameters at their default. Press \"OK\". Once PageRank is calculated, if you click back into the \"Data Laboratory\" and select the \"Nodes\" list in the Data Table, you can see that a new \"PageRank\" column has been added, with values for every node. The higher the PageRank, the more central a correspondent is in the network. Going back to the Overview pane, you can visualize this centrality by changing the size of each correspondent\u2019s node based on its PageRank. Do this in the \"Ranking\" tab on the left side of the Overview pane. Make sure \"Nodes\" is selected, press the icon of a little red diamond, and select PageRank from the drop-down menu. In the parameter options just below, enter the \"Min size\" as 1 and the \"Max size\" as 10. Press \"Apply,\" and watch the nodes resize based on their PageRank. To be on the safe side and decrease clutter, re-run the \"Force Atlas 2\" layout as described above, making sure to keep the \"Prevent Overlap\" box checked. At this point, the network is processed enough to visualize in the Preview pane, to finally begin making sense of the data. In Preview, on the left side, select \"Show Labels,\" \"Proportional Size,\" \"Rescale Weight,\" and deselect \"Curved\" edges. Press \"Refresh.\"","title":"Navigating Gephi"},{"location":"supporting materials/gephi.txt/#so-what-have-we-got","text":"The visualization immediately reveals apparent structure: central figures on the top (Ashbel Smith and Anson Jones) and bottom (Mirabeau B. Lamar, James Webb, and James Treat), and two central figures who connect the two split communities (James Hamilton and James S. Mayfield). A quick search online shows the top network to be associated with the last president of the Republic of Texas, Anson Jones; whereas the bottom network largely revolves around the second president, Mirabeau Lamar. Experts on this period in history could use this analysis to understand the structure of communities building to the annexation of Texas or they could ask meta-questions about the nature of the data themselves. For example, why is Sam Houston, the first and third president of the Republic of Texas, barely visible in this network? Write up your own observations on this process in your open notebook, and export your Gephi file as a .graphml file (because Gephi's .gephi format is a bit unstable, always save as or export your work in a variety of formats). Upload that to your repository.","title":"So what have we got?"},{"location":"supporting materials/gh-pages/","text":"Creating a GitHub pages site with MkDocs In Module 5 , you learned how to set up and serve a basic webpage on GitHub's gh-pages. GitHub allows you to host a website on a personal URL with the syntax https://myname.github.io . Many developers will use a static site generator to create a simple website run on gh-pages. Some popular static site generators are Jekyll , Hugo , and MkDocs . In fact, this website is built with MkDocs. Static site generators are command line tools that use templates to build and create websites. Users define basic information in a yaml or toml filetype, write in markdown, and use a simple command to build it into a functioning static site. Due to the popularity of static site generators, most contain simple methods to link it to gh-pages. In this exercise, we will use MkDocs in the DH Box to build a site that updates to GitHub and pushes changes to a https://myname.github.io URL. We will use DH Box to download MkDocs and build the site. Although your changes will be saved in a GitHub repository, when your DH Box account expires, you will have to clone the website again and redownload everything. Therefore, if you want to use your MkDocs site past the purposes of this course, you may want to follow these steps on your desktop. These steps will be similar to using Mac's terminal, but may be different on a Windows machine. Preparing MkDocs in DH Box Navigate to the DH Box command line. Type $ sudo pip install mkdocs into the command line. Type $ mkdocs --version to ensure MkDocs was installed properly. Type $ mkdocs new my-site or whatever you want your site to be called. Mine is called static-site . Typically, you could now view your site in your browser by typing $ mkdocs serve . This would not work within DH Box, though, since DH Box runs as a virtual environment within your browser. It would work from your desktop terminal. Type $ ls to view the files MkDocs created. You will define your site within the mkdocs.yml file. Type $ nano mkdocs.yml to open the config file. Change the site_name to your site's name. Paste the following underneath site_name : pages: - Home: index.md theme: readthedocs This sets up our Home and About pages and adds the theme called Read the Docs. Hit ctrl-x, Y, enter to save and exit nano. Creating a git repository Navigate to GitHub and create a new repository with the same name as your MkDocs folder. For example, my MkDocs folder is called static-site . Therefore, I call my GitHub repository static-site . Navigate back to the command line. Type $ git init to initialize the MkDocs folder as a git repository. Type $ git add . to add the files and folders. Type $ git commit -m \"First commit\" to commit your changes. Type $ git remote add origin https://github.com/ yourusername / yourrepository .git , making sure to add your URL to the GitHub repository. Type $ git push -u origin master to push your changes. Type $ mkdocs gh-deploy to push your folder to a gh-pages site. MkDocs will show a message which says the following: INFO - Your documentation should shortly be available at: https://myname.github.io/my-site/`. The above means it pushed your changes to a project folder called my-site and not the main https://myname.github.io/ . Navigate to your URL https://myname.github.io/my-site/ . You should now see a webpage that looks a lot like this workbook. Adding content to your site Let's say we now want to add an About page and a Blog page to our site. We need to define and create these files. Navigate to the command line. Type $ ls to view your MkDocs files and folders. You will notice now a site folder has been generated. This is simply the site folder created by MkDocs which is served on your github.io site. Type $ nano mkdocs.yml to open the config file. Add an about page and a posts page. Your file should contain the following: site_name: Static Site Example pages: - Home: index.md - About: about.md - Blog: blog.md theme: readthedocs Hit ctrl-x, Y, enter to save and exit nano. Type $ cd docs to enter the docs folder. Type $ touch about.md to create the About file and touch blog.md to create the Blog file. Type $ ls to make sure the files were created. Type $ cd .. to go back to the main MkDocs folder. Type $ git add . to add the files. Type $ git commit -m \"Added about and blog files\" to commit your changes. Type $ git push -u origin master to push your changes. This ensures all your changes are pushed to your GitHub repository. Type $ mkdocs gh-deploy to push your folder to a gh-pages site. Reload your gh-pages site https://myname.github.io/ . It may take some time for your changes to update to your gh-pages site. Back in your docs folder, you can use nano to edit any of the markdown files. If you add any files, make sure to declare them in mkdocs.yml . Customizing your theme MkDocs also allows you to customize your theme. We don't want to directly edit the theme files, so we will create a new one. Navigate to the command line. In your main MkDocs folder, type $ nano mkdocs.yml to open your configuration file. Add the following text below the other configuration details: extra_css: - 'css/extra.css' This declaration tells the site to first look for styles in a folder called css in a file called extra.css before going to the default theme. Hit ctrl-x, Y, enter to save and exit nano. Type $ cd docs to navigate to the docs folder. Type $ mkdir css to create a folder called css. Type $ cd css to enter the css folder. Type $ nano extra.css to create an enter a css file called extra. Let's change the font of our site to Open Sans. Add the following to your css file: h1, h2, h3, h4, h5, h6, legend { font-family: 'Open Sans', sans-serif; } Hit ctrl-x, Y, enter to save and exit nano. Type $ cd .. twice to go back up to your main MkDocs folder. Use git to add your files, commit, push, and then use mkdocs to deploy to gh-pages to view your changes. You can now use you knowledge to customize your theme. Remember to use your browser inspector tool to check which rules you want to change.","title":"GitHub Pages"},{"location":"supporting materials/gh-pages/#creating-a-github-pages-site-with-mkdocs","text":"In Module 5 , you learned how to set up and serve a basic webpage on GitHub's gh-pages. GitHub allows you to host a website on a personal URL with the syntax https://myname.github.io . Many developers will use a static site generator to create a simple website run on gh-pages. Some popular static site generators are Jekyll , Hugo , and MkDocs . In fact, this website is built with MkDocs. Static site generators are command line tools that use templates to build and create websites. Users define basic information in a yaml or toml filetype, write in markdown, and use a simple command to build it into a functioning static site. Due to the popularity of static site generators, most contain simple methods to link it to gh-pages. In this exercise, we will use MkDocs in the DH Box to build a site that updates to GitHub and pushes changes to a https://myname.github.io URL. We will use DH Box to download MkDocs and build the site. Although your changes will be saved in a GitHub repository, when your DH Box account expires, you will have to clone the website again and redownload everything. Therefore, if you want to use your MkDocs site past the purposes of this course, you may want to follow these steps on your desktop. These steps will be similar to using Mac's terminal, but may be different on a Windows machine.","title":"Creating a GitHub pages site with MkDocs"},{"location":"supporting materials/gh-pages/#preparing-mkdocs-in-dh-box","text":"Navigate to the DH Box command line. Type $ sudo pip install mkdocs into the command line. Type $ mkdocs --version to ensure MkDocs was installed properly. Type $ mkdocs new my-site or whatever you want your site to be called. Mine is called static-site . Typically, you could now view your site in your browser by typing $ mkdocs serve . This would not work within DH Box, though, since DH Box runs as a virtual environment within your browser. It would work from your desktop terminal. Type $ ls to view the files MkDocs created. You will define your site within the mkdocs.yml file. Type $ nano mkdocs.yml to open the config file. Change the site_name to your site's name. Paste the following underneath site_name : pages: - Home: index.md theme: readthedocs This sets up our Home and About pages and adds the theme called Read the Docs. Hit ctrl-x, Y, enter to save and exit nano.","title":"Preparing MkDocs in DH Box"},{"location":"supporting materials/gh-pages/#creating-a-git-repository","text":"Navigate to GitHub and create a new repository with the same name as your MkDocs folder. For example, my MkDocs folder is called static-site . Therefore, I call my GitHub repository static-site . Navigate back to the command line. Type $ git init to initialize the MkDocs folder as a git repository. Type $ git add . to add the files and folders. Type $ git commit -m \"First commit\" to commit your changes. Type $ git remote add origin https://github.com/ yourusername / yourrepository .git , making sure to add your URL to the GitHub repository. Type $ git push -u origin master to push your changes. Type $ mkdocs gh-deploy to push your folder to a gh-pages site. MkDocs will show a message which says the following: INFO - Your documentation should shortly be available at: https://myname.github.io/my-site/`. The above means it pushed your changes to a project folder called my-site and not the main https://myname.github.io/ . Navigate to your URL https://myname.github.io/my-site/ . You should now see a webpage that looks a lot like this workbook.","title":"Creating a git repository"},{"location":"supporting materials/gh-pages/#adding-content-to-your-site","text":"Let's say we now want to add an About page and a Blog page to our site. We need to define and create these files. Navigate to the command line. Type $ ls to view your MkDocs files and folders. You will notice now a site folder has been generated. This is simply the site folder created by MkDocs which is served on your github.io site. Type $ nano mkdocs.yml to open the config file. Add an about page and a posts page. Your file should contain the following: site_name: Static Site Example pages: - Home: index.md - About: about.md - Blog: blog.md theme: readthedocs Hit ctrl-x, Y, enter to save and exit nano. Type $ cd docs to enter the docs folder. Type $ touch about.md to create the About file and touch blog.md to create the Blog file. Type $ ls to make sure the files were created. Type $ cd .. to go back to the main MkDocs folder. Type $ git add . to add the files. Type $ git commit -m \"Added about and blog files\" to commit your changes. Type $ git push -u origin master to push your changes. This ensures all your changes are pushed to your GitHub repository. Type $ mkdocs gh-deploy to push your folder to a gh-pages site. Reload your gh-pages site https://myname.github.io/ . It may take some time for your changes to update to your gh-pages site. Back in your docs folder, you can use nano to edit any of the markdown files. If you add any files, make sure to declare them in mkdocs.yml .","title":"Adding content to your site"},{"location":"supporting materials/gh-pages/#customizing-your-theme","text":"MkDocs also allows you to customize your theme. We don't want to directly edit the theme files, so we will create a new one. Navigate to the command line. In your main MkDocs folder, type $ nano mkdocs.yml to open your configuration file. Add the following text below the other configuration details: extra_css: - 'css/extra.css' This declaration tells the site to first look for styles in a folder called css in a file called extra.css before going to the default theme. Hit ctrl-x, Y, enter to save and exit nano. Type $ cd docs to navigate to the docs folder. Type $ mkdir css to create a folder called css. Type $ cd css to enter the css folder. Type $ nano extra.css to create an enter a css file called extra. Let's change the font of our site to Open Sans. Add the following to your css file: h1, h2, h3, h4, h5, h6, legend { font-family: 'Open Sans', sans-serif; } Hit ctrl-x, Y, enter to save and exit nano. Type $ cd .. twice to go back up to your main MkDocs folder. Use git to add your files, commit, push, and then use mkdocs to deploy to gh-pages to view your changes. You can now use you knowledge to customize your theme. Remember to use your browser inspector tool to check which rules you want to change.","title":"Customizing your theme"},{"location":"supporting materials/git-rstudio/","text":"Using Git with rstudio-pubs-static Before you can use git to keep track of your changes to your R project, you need to tell the git program (which keeps snapshots of your changes) who you are. To do this, execute the following commands in the command line: $ git config --global user.email \"you@example.com\" $ git config --global user.name \"Your Name\" Go back to RStudio. Under 'Tools' select 'Version Control' then 'project setup'. Under 'Version control system' select the 'Git' tab. You now have a new option in the pane at top right, beside 'Environment' and 'History': 'git'. Click on 'Git'. The panel now displays all of the files created in this project folder. Tick off the files you want to commit. Click on the 'Commit' button. A new window opens called 'RStudio: Review Changes'. This window shows you a preview of the text of each file, in green where material has been added, red where material has been deleted (these are the 'difs'). Add a commit message into the top right 'commit message' box. You've now made a local commit to your git repository! If things go horribly wrong, you can roll back the changes. Now, let's setup your GitHub repo for this. Go to your GitHub account. Make a new repository; initialize it with a readme.md Back in RStudio, click on the 'more' gearwheel on the Git tab. Select shell (you could do this from the command line too, when you're in your project folder). This will open up a box into which you can type commands; we're going to tell git the location of our remote repository, add that info into the config, and do two pulls. Open shell and execute the following commands: $ git remote add origin https://github.com/YOUR-ACCOUNT/YOUR-REPO.git $ git config remote.origin.url https://github.com/YOUR-ACCOUNT/YOUR-REPO.git $ git pull -u origin master And you now can push your changes to your remote repository whenever you make a new commit. There is a variation of markdown called RMarkdown that enables you to embed working R code into a document, and then 'knit' it into HTML or slide shows or PDFs. When you push those to a GitHub repo, you are now making data publications! The official R Markdown information can be found on the RStudio website .","title":"Git in RStudio"},{"location":"supporting materials/git-rstudio/#using-git-with-rstudio-pubs-static","text":"Before you can use git to keep track of your changes to your R project, you need to tell the git program (which keeps snapshots of your changes) who you are. To do this, execute the following commands in the command line: $ git config --global user.email \"you@example.com\" $ git config --global user.name \"Your Name\" Go back to RStudio. Under 'Tools' select 'Version Control' then 'project setup'. Under 'Version control system' select the 'Git' tab. You now have a new option in the pane at top right, beside 'Environment' and 'History': 'git'. Click on 'Git'. The panel now displays all of the files created in this project folder. Tick off the files you want to commit. Click on the 'Commit' button. A new window opens called 'RStudio: Review Changes'. This window shows you a preview of the text of each file, in green where material has been added, red where material has been deleted (these are the 'difs'). Add a commit message into the top right 'commit message' box. You've now made a local commit to your git repository! If things go horribly wrong, you can roll back the changes. Now, let's setup your GitHub repo for this. Go to your GitHub account. Make a new repository; initialize it with a readme.md Back in RStudio, click on the 'more' gearwheel on the Git tab. Select shell (you could do this from the command line too, when you're in your project folder). This will open up a box into which you can type commands; we're going to tell git the location of our remote repository, add that info into the config, and do two pulls. Open shell and execute the following commands: $ git remote add origin https://github.com/YOUR-ACCOUNT/YOUR-REPO.git $ git config remote.origin.url https://github.com/YOUR-ACCOUNT/YOUR-REPO.git $ git pull -u origin master And you now can push your changes to your remote repository whenever you make a new commit. There is a variation of markdown called RMarkdown that enables you to embed working R code into a document, and then 'knit' it into HTML or slide shows or PDFs. When you push those to a GitHub repo, you are now making data publications! The official R Markdown information can be found on the RStudio website .","title":"Using Git with rstudio-pubs-static"},{"location":"supporting materials/glitch/","text":"Glitching Files Glitching files messes with our expectations of what digital data should be. There is actually quite a large body of literature on the why and how of glitching (try Glitch's GitHub for a place to start ). For us as digital historians, glitching digital images or other documents reminds of us the ephemerality of digital data. It might also raise other questions about the composition of historical photography, and the ways that no image is an objective record of the past. In the exercise below, you will do the following: Build on what you learned about APIs and regex to download images from the Open Context repository of archaeological data. Use a script that will perform the same manipulations on every image in your folder. Use another script to create a static website with all of your images, a gallery of Glitch. Spend some time on the articles and resources curated on Glitch's GitHub to begin to explore the philosophy and aesthetic of Glitch. Workflow we're going for: Get images from Open Context ( ocget.sh ) Glitch them ( do-glitch.sh ) Make a website out of them ( expose ) Push them to GitHub Pages ( git ) Open Context Open Context publishes archaeological data on the web. Archaeology generates vast amounts of information, both through excavation and through analysis. Open Context exists to publish curated versions of this data with unique digital object identifiers so that the source data of archaeology can be re-examined and re-studied in the future. Archaeology, uniquely amongst the historical disciplines, tends to destroy its subject matter, so reproducibility and open access data are extremely important issues. At Open Context's API page , they explain how to programmatically obtain information from their site. Check out this example search on Open Context . Open that up, give it a play right now, and see what kinds of information exist. Try to craft a search that retrieves some interesting information. Once you've crafted a search that retrieves something of interest, it's time to build a script that will retrieve that information for you. Below I have modified a script from Ian Milligan's API example to grab 10 records from the 'animal bone' category on Open Context. You could use that as a basis for your own search. In DH Box, create a new folder with mkdir for this exercise, then cd into it. Open the nano editor and paste the script below in a new file. Make sure to read the comments, because there are a few lines you have to customize. (You'll get file not found errors if you don't pay attention!) The line beginning with 'sed' searches for the thumbnail key, and marks it off with a tilde ~ . The | character is called a 'pipe' and it pipes the output of the command before it into the input of the command after it. The first pipe passes the output of that first sed command to grep, which finds the lines marked with the tilde to the tr (trim) command, which then deletes all of the other information; the next pipe passes to a series of sed commands which then get rid of whitespace and the word thumbnail, the tilde, and any commas, leaving us with a list of direct URLs to the thumbnails. The wget command grabs each thumbnail in the list in turn, waiting 2 seconds between requests, and using a limited amount of bandwidth (we're good digital citizens, remember). #! /bin/bash # we already know how to ask the api for the infromation we're interested in, from studying opencontex.org' api documentation. Below we ask for a 10 records related to animal bones curl 'https://opencontext.org/media-search/.json?prop=rel--oc-gen-cat-object||rel--oc-gen-cat-animal-bone response=uri-meta rows=10' results.txt # the next two lines take the results.txt file that is quite messy and clean it up. I'll try to explain what they all mean. Basically, tr deletes a given character - so we delete quotation marks \"\\\"\" (the slash tells the computer not to treat the quotation mark as a computer code, but as a quotation mark itself), to erase spaces, and to find the phrase \"key:\" and delete it too. sed -r 's/(.+\\bthumbnail\\b.+)/~\\1/g' results.txt | grep '~'| tr -d \"\\\"\" | tr -d \"{\" | tr -d \"}\" | tr -s \" \" | sed '/^\\s*$/d' | tr -d ' ' | sed 's/\\bthumbnail:\\b//g' | sed 's/,//g' | sed 's/~//g' thumbnailstograb.txt # gotta grab 'em all. # in the line below, put the full path to your images file, eg mine is home/shawngraham/oc/images. Yours might be home/yourname/oc/images wget -i thumbnailstograb.txt -P path/to/images --limit-rate=20k -w 2 Save it as ocget.sh , then use $chmod 755 ocget.sh to make it executable. Then $ mkdir images so you have a place to put the images into. \"Hey, images keep appearing even though I ctrl+c to stop the download,\" I hear some of you say. Well, that doesn't necessarily stop anything; when you ran the command, you started a process and it might still be ticking away in the background. If this seems to be happening to you, type ps ux to get a list of running processes. If wget is still running in the backround, you'll see it listed in the right-most column. The first number in that row is its PIDnumber; to kill it, type kill -9 PIDnumber where you swap in the actual PIDnumber. Also , before you re-run this script, you need to delete the results.txt file and the thumbnailstograb.txt file, like so: rm results.txt etc. Otherwise, it'll just add your results to the end of the previous results, making your list longer and longer... An image glitch script in Python The next step is to glitch the image. Go to the bndr GitHub repository . This is a Python package for mucking about the bits inside a jpg image. Install it at the command line with $ pip install bndr . Make a new folder for the output of this process: mkdir out . Now, this code is meant to be run on one file at a time, like this: $ bndrimg photo-name.jpg . Typing in each file name by hand would take a very long time to glitch everything. So instead, we write a little script that we'll call do-glitch.sh . In Nano, paste the following: #! /bin/bash # do-glitch.sh # run the glitch script on each image in the images folder for file in images/*.jpg; do bndrimg \"$file\"; done # move the glitched files to the output folder find ./ -name '*out.png' -exec cp -prv '{}' 'out/' ';' This isn't the most elegant script, but it works, and frankly, that's all that matters sometimes. Change the permissions so we can run it by typing $ chmod 755 do-glitch.sh . This script reads each file name in the images folder, and passes them one at a time to the bndr command (you can't run it when you're inside the images folder, remember). Then, we move just the glitched images to the 'out' folder. Download some of the images to your own machine using the File Manager to see your glitched art! A static website generator for photography Finally, let's turn that folder of pictures into a website. We're going to use the Expos\u00e9 site generator, which you can get from the Expos\u00e9 GitHub repository . Take a moment to read through the details of that package. Use the cd command to get back up to your main directory (ie. don't do this when you're in your images folder). Grab the Expos\u00e9 code with the following command: $ git clone https://github.com/Jack000/Expose.git The Expos\u00e9 script relies on some helpers and so on in its folder. But we would like to be able to run that command no matter which folder we're in. We tell the computer that when we type expose we actually mean, the version that lives in that location where we just downloaded it. That is to say, we use the alias command. On my machine, it looks like this: alias expose=/home/shawngraham/oc/Expose/expose.sh On your machine, it might be in a different location. Note also that the Expos\u00e9 repo is uppercase 'E', while the command is lowercase e'. Also ,every time you start a session, you'll need to do this alias command or else the computer won't know what you're talking about. To generate the static site, cd into your out folder that holds your glitched images. At the prompt, type $ expose . Ta da! You now have a fully functional website showing off your Glitch art. You can customize it to use an alternative theme, a few bells and whistles; see the Expos\u00e9 documentation. One thing that you should do is to add captions for your images. You write these as either .txt or . md files, but make sure the file name for the caption is the same as for the image and put it in the same folder as the source images! You do this before you run the expose script. So beaker-pic.png would have as a caption file beaker-pic.txt or beaker-pic.md . You specify where the caption goes on the image in the YAML; that is, the metadata for your caption. So, if we opened beaker-pic.md , it might contain the following: --- top: 30 left: 5 width: 30 height: 20 textcolor: #ffffff --- # A Picture of a Beaker Beakers were used to carry rare aromatic herbs and spices... More possibilities for sorting out the position of the text are discussed in the Expos\u00e9 documentation. Host the site on GitHub You can push the code for your site to GitHub, and then use GitHub's gh-pages feature to serve it up as a live website! Expos\u00e9 creates all of the code for the website inside a new folder called _site . Type $ cd _site Turn this folder into a git repository by typing git init . Go to your account on GitHub.com (in another browser window). Click on the + at the top-right to make a new repository. Call it whatever you want, but do not initialize it with a readme. Back at the command line in your _site folder, type $ git add . This stages all of the files and subfolders for a new commit. Type $ git commit -m \"first commit\" to make a commit message. Tell git where your remote repository is by typing $ git remote add origin https://github.com/YOUR-ACCOUNT/YOUR-NEW-REPO-YOU-JUST-MADE.git Push your materials to GitHub by typing $ git push -u origin master . (Git might ask for your account username and password.) Once that finishes, go back to GitHub and reload the page at https://github.com/YOUR-ACCOUNT/YOUR-NEW-REPO-YOU-JUST-MADE . You should see your changes. On the button where it says 'Branch: Master', click on the down arrow. In the box where it says faintly 'Find or create a branch', type gh-pages . This is a special branch which tells GitHub, 'serve this up as actual code when the user goes to the special github.io version of GitHub'. Click on the gearwheel icon, to go to the settings page (You can also find it at https://github.com/YOUR-ACCOUNT/YOUR-NEW-REPO-YOU-JUST-MADE/settings .) Scroll down to the box that says 'GitHub Pages'. In the green box, it says 'you're site is published at https://YOUR-ACCOUNT.github.io/YOUR-NEW-REPO-YOU-JUST-MADE '. Click on that link and you'll see your site! (Visit my site on gh pages .) Conclusion I see no reason why digital history cannot bleed into art. Art is meant to provoke, to prompt reflection, discussion, and controversy. Glitching images confounds our expectation of what digital data are supposed to be, supposed to do. If you choose your pictures carefully, glitching can tell a story as profound as any essay.","title":"Glitch"},{"location":"supporting materials/glitch/#glitching-files","text":"Glitching files messes with our expectations of what digital data should be. There is actually quite a large body of literature on the why and how of glitching (try Glitch's GitHub for a place to start ). For us as digital historians, glitching digital images or other documents reminds of us the ephemerality of digital data. It might also raise other questions about the composition of historical photography, and the ways that no image is an objective record of the past. In the exercise below, you will do the following: Build on what you learned about APIs and regex to download images from the Open Context repository of archaeological data. Use a script that will perform the same manipulations on every image in your folder. Use another script to create a static website with all of your images, a gallery of Glitch. Spend some time on the articles and resources curated on Glitch's GitHub to begin to explore the philosophy and aesthetic of Glitch.","title":"Glitching Files"},{"location":"supporting materials/glitch/#workflow-were-going-for","text":"Get images from Open Context ( ocget.sh ) Glitch them ( do-glitch.sh ) Make a website out of them ( expose ) Push them to GitHub Pages ( git )","title":"Workflow we're going for:"},{"location":"supporting materials/glitch/#open-context","text":"Open Context publishes archaeological data on the web. Archaeology generates vast amounts of information, both through excavation and through analysis. Open Context exists to publish curated versions of this data with unique digital object identifiers so that the source data of archaeology can be re-examined and re-studied in the future. Archaeology, uniquely amongst the historical disciplines, tends to destroy its subject matter, so reproducibility and open access data are extremely important issues. At Open Context's API page , they explain how to programmatically obtain information from their site. Check out this example search on Open Context . Open that up, give it a play right now, and see what kinds of information exist. Try to craft a search that retrieves some interesting information. Once you've crafted a search that retrieves something of interest, it's time to build a script that will retrieve that information for you. Below I have modified a script from Ian Milligan's API example to grab 10 records from the 'animal bone' category on Open Context. You could use that as a basis for your own search. In DH Box, create a new folder with mkdir for this exercise, then cd into it. Open the nano editor and paste the script below in a new file. Make sure to read the comments, because there are a few lines you have to customize. (You'll get file not found errors if you don't pay attention!) The line beginning with 'sed' searches for the thumbnail key, and marks it off with a tilde ~ . The | character is called a 'pipe' and it pipes the output of the command before it into the input of the command after it. The first pipe passes the output of that first sed command to grep, which finds the lines marked with the tilde to the tr (trim) command, which then deletes all of the other information; the next pipe passes to a series of sed commands which then get rid of whitespace and the word thumbnail, the tilde, and any commas, leaving us with a list of direct URLs to the thumbnails. The wget command grabs each thumbnail in the list in turn, waiting 2 seconds between requests, and using a limited amount of bandwidth (we're good digital citizens, remember). #! /bin/bash # we already know how to ask the api for the infromation we're interested in, from studying opencontex.org' api documentation. Below we ask for a 10 records related to animal bones curl 'https://opencontext.org/media-search/.json?prop=rel--oc-gen-cat-object||rel--oc-gen-cat-animal-bone response=uri-meta rows=10' results.txt # the next two lines take the results.txt file that is quite messy and clean it up. I'll try to explain what they all mean. Basically, tr deletes a given character - so we delete quotation marks \"\\\"\" (the slash tells the computer not to treat the quotation mark as a computer code, but as a quotation mark itself), to erase spaces, and to find the phrase \"key:\" and delete it too. sed -r 's/(.+\\bthumbnail\\b.+)/~\\1/g' results.txt | grep '~'| tr -d \"\\\"\" | tr -d \"{\" | tr -d \"}\" | tr -s \" \" | sed '/^\\s*$/d' | tr -d ' ' | sed 's/\\bthumbnail:\\b//g' | sed 's/,//g' | sed 's/~//g' thumbnailstograb.txt # gotta grab 'em all. # in the line below, put the full path to your images file, eg mine is home/shawngraham/oc/images. Yours might be home/yourname/oc/images wget -i thumbnailstograb.txt -P path/to/images --limit-rate=20k -w 2 Save it as ocget.sh , then use $chmod 755 ocget.sh to make it executable. Then $ mkdir images so you have a place to put the images into. \"Hey, images keep appearing even though I ctrl+c to stop the download,\" I hear some of you say. Well, that doesn't necessarily stop anything; when you ran the command, you started a process and it might still be ticking away in the background. If this seems to be happening to you, type ps ux to get a list of running processes. If wget is still running in the backround, you'll see it listed in the right-most column. The first number in that row is its PIDnumber; to kill it, type kill -9 PIDnumber where you swap in the actual PIDnumber. Also , before you re-run this script, you need to delete the results.txt file and the thumbnailstograb.txt file, like so: rm results.txt etc. Otherwise, it'll just add your results to the end of the previous results, making your list longer and longer...","title":"Open Context"},{"location":"supporting materials/glitch/#an-image-glitch-script-in-python","text":"The next step is to glitch the image. Go to the bndr GitHub repository . This is a Python package for mucking about the bits inside a jpg image. Install it at the command line with $ pip install bndr . Make a new folder for the output of this process: mkdir out . Now, this code is meant to be run on one file at a time, like this: $ bndrimg photo-name.jpg . Typing in each file name by hand would take a very long time to glitch everything. So instead, we write a little script that we'll call do-glitch.sh . In Nano, paste the following: #! /bin/bash # do-glitch.sh # run the glitch script on each image in the images folder for file in images/*.jpg; do bndrimg \"$file\"; done # move the glitched files to the output folder find ./ -name '*out.png' -exec cp -prv '{}' 'out/' ';' This isn't the most elegant script, but it works, and frankly, that's all that matters sometimes. Change the permissions so we can run it by typing $ chmod 755 do-glitch.sh . This script reads each file name in the images folder, and passes them one at a time to the bndr command (you can't run it when you're inside the images folder, remember). Then, we move just the glitched images to the 'out' folder. Download some of the images to your own machine using the File Manager to see your glitched art!","title":"An image glitch script in Python"},{"location":"supporting materials/glitch/#a-static-website-generator-for-photography","text":"Finally, let's turn that folder of pictures into a website. We're going to use the Expos\u00e9 site generator, which you can get from the Expos\u00e9 GitHub repository . Take a moment to read through the details of that package. Use the cd command to get back up to your main directory (ie. don't do this when you're in your images folder). Grab the Expos\u00e9 code with the following command: $ git clone https://github.com/Jack000/Expose.git The Expos\u00e9 script relies on some helpers and so on in its folder. But we would like to be able to run that command no matter which folder we're in. We tell the computer that when we type expose we actually mean, the version that lives in that location where we just downloaded it. That is to say, we use the alias command. On my machine, it looks like this: alias expose=/home/shawngraham/oc/Expose/expose.sh On your machine, it might be in a different location. Note also that the Expos\u00e9 repo is uppercase 'E', while the command is lowercase e'. Also ,every time you start a session, you'll need to do this alias command or else the computer won't know what you're talking about. To generate the static site, cd into your out folder that holds your glitched images. At the prompt, type $ expose . Ta da! You now have a fully functional website showing off your Glitch art. You can customize it to use an alternative theme, a few bells and whistles; see the Expos\u00e9 documentation. One thing that you should do is to add captions for your images. You write these as either .txt or . md files, but make sure the file name for the caption is the same as for the image and put it in the same folder as the source images! You do this before you run the expose script. So beaker-pic.png would have as a caption file beaker-pic.txt or beaker-pic.md . You specify where the caption goes on the image in the YAML; that is, the metadata for your caption. So, if we opened beaker-pic.md , it might contain the following: --- top: 30 left: 5 width: 30 height: 20 textcolor: #ffffff --- # A Picture of a Beaker Beakers were used to carry rare aromatic herbs and spices... More possibilities for sorting out the position of the text are discussed in the Expos\u00e9 documentation.","title":"A static website generator for photography"},{"location":"supporting materials/glitch/#host-the-site-on-github","text":"You can push the code for your site to GitHub, and then use GitHub's gh-pages feature to serve it up as a live website! Expos\u00e9 creates all of the code for the website inside a new folder called _site . Type $ cd _site Turn this folder into a git repository by typing git init . Go to your account on GitHub.com (in another browser window). Click on the + at the top-right to make a new repository. Call it whatever you want, but do not initialize it with a readme. Back at the command line in your _site folder, type $ git add . This stages all of the files and subfolders for a new commit. Type $ git commit -m \"first commit\" to make a commit message. Tell git where your remote repository is by typing $ git remote add origin https://github.com/YOUR-ACCOUNT/YOUR-NEW-REPO-YOU-JUST-MADE.git Push your materials to GitHub by typing $ git push -u origin master . (Git might ask for your account username and password.) Once that finishes, go back to GitHub and reload the page at https://github.com/YOUR-ACCOUNT/YOUR-NEW-REPO-YOU-JUST-MADE . You should see your changes. On the button where it says 'Branch: Master', click on the down arrow. In the box where it says faintly 'Find or create a branch', type gh-pages . This is a special branch which tells GitHub, 'serve this up as actual code when the user goes to the special github.io version of GitHub'. Click on the gearwheel icon, to go to the settings page (You can also find it at https://github.com/YOUR-ACCOUNT/YOUR-NEW-REPO-YOU-JUST-MADE/settings .) Scroll down to the box that says 'GitHub Pages'. In the green box, it says 'you're site is published at https://YOUR-ACCOUNT.github.io/YOUR-NEW-REPO-YOU-JUST-MADE '. Click on that link and you'll see your site! (Visit my site on gh pages .)","title":"Host the site on GitHub"},{"location":"supporting materials/glitch/#conclusion","text":"I see no reason why digital history cannot bleed into art. Art is meant to provoke, to prompt reflection, discussion, and controversy. Glitching images confounds our expectation of what digital data are supposed to be, supposed to do. If you choose your pictures carefully, glitching can tell a story as profound as any essay.","title":"Conclusion"},{"location":"supporting materials/graphing-the-net.txt/","text":"Graphing the Net It may be that you are interested in the structure of links on the internet. Perhaps you'd like to see how a particular topic plays out across Wikiepedia. What you'd have to do is a web-crawl . Below are quick instructions for setting up a webcrawl, which will follow every link on a page to a particular depth, and echo the results through your browser into Gephi for visualization and analysis. You will need: The Chrome browser with Site Spider Mk II installed Gephi HTTP Graph Generator Plugin for Gephi installed NB You can install the graph generator plugin from within Gephi: Select 'Tools' on the main menu ribbon at the top of the screen (and not the 'plugins' item). Within 'Tools', select 'plugins' and then 'available plugins'. Search for 'HTTPGraph'. Tick off the box and install it. When finished, close and restart Gephi with a new project. Getting set up to scrape In Chrome, go to the settings page (the 'hamburger' icon at the extreme right of the address bar, or by typing chrome://settings/ in the address bar. Click on 'Show advanced settings' at the bottom of the page. Scroll down to 'Network' and click on 'Change proxy settings'. In the popup that opens, click the 'Connections' tab, and then the 'LAN Settings' button. Another popup will open. Select the 'Use a proxy server for your LAN'. Enter 127.0.0.1 for the address, and 8088 for the port. Now, whenever you go to a website, the info will be echoed through that port. We need to set Gephi up to hear what's being passed. Open Gephi and start a new project. Go to 'File', then 'Generate', and then 'http graph'. A pop up will open, asking you to specify a port. Input 8088 . Accept the defaults, and press OK. On the Overview panel, nodes will begin to appear when you go to a URL in Chrome. Begin the scrape Go back to Chrome. Put in the URL that you want to start your scrape on, eg http://en.wikipedia.org/wiki/Archaeology . Click the SiteSpider II button in your toolbar. Site spider will open a popup asking you how far and how deep you want to scrape. Set the SiteSpider II parameters accordingly. Hit 'go' and then flip over to your Gephi window. You'll see the network begin to populate! Let it run as long as you want. When you're finished, save your network by 'exporting' it (on the file menu) as any other format than .gephi . I say this because I find sometimes the .gephi format is unstable. You can then filter your network for resources or html pages or what have you. I suggest deleting the node labelled 127.0.0.1 because that's your computer and it will throw off any metrics you choose to calculate. What does it all mean? Well, that depends. For an example of why you might want to do all this, and what you might find, see 'Shouting Into the Void?' a piece where I tried to understand the shape of the archaeological web.","title":"Graphing the Net"},{"location":"supporting materials/graphing-the-net.txt/#graphing-the-net","text":"It may be that you are interested in the structure of links on the internet. Perhaps you'd like to see how a particular topic plays out across Wikiepedia. What you'd have to do is a web-crawl . Below are quick instructions for setting up a webcrawl, which will follow every link on a page to a particular depth, and echo the results through your browser into Gephi for visualization and analysis.","title":"Graphing the Net"},{"location":"supporting materials/graphing-the-net.txt/#you-will-need","text":"The Chrome browser with Site Spider Mk II installed Gephi HTTP Graph Generator Plugin for Gephi installed NB You can install the graph generator plugin from within Gephi: Select 'Tools' on the main menu ribbon at the top of the screen (and not the 'plugins' item). Within 'Tools', select 'plugins' and then 'available plugins'. Search for 'HTTPGraph'. Tick off the box and install it. When finished, close and restart Gephi with a new project.","title":"You will need:"},{"location":"supporting materials/graphing-the-net.txt/#getting-set-up-to-scrape","text":"In Chrome, go to the settings page (the 'hamburger' icon at the extreme right of the address bar, or by typing chrome://settings/ in the address bar. Click on 'Show advanced settings' at the bottom of the page. Scroll down to 'Network' and click on 'Change proxy settings'. In the popup that opens, click the 'Connections' tab, and then the 'LAN Settings' button. Another popup will open. Select the 'Use a proxy server for your LAN'. Enter 127.0.0.1 for the address, and 8088 for the port. Now, whenever you go to a website, the info will be echoed through that port. We need to set Gephi up to hear what's being passed. Open Gephi and start a new project. Go to 'File', then 'Generate', and then 'http graph'. A pop up will open, asking you to specify a port. Input 8088 . Accept the defaults, and press OK. On the Overview panel, nodes will begin to appear when you go to a URL in Chrome.","title":"Getting set up to scrape"},{"location":"supporting materials/graphing-the-net.txt/#begin-the-scrape","text":"Go back to Chrome. Put in the URL that you want to start your scrape on, eg http://en.wikipedia.org/wiki/Archaeology . Click the SiteSpider II button in your toolbar. Site spider will open a popup asking you how far and how deep you want to scrape. Set the SiteSpider II parameters accordingly. Hit 'go' and then flip over to your Gephi window. You'll see the network begin to populate! Let it run as long as you want. When you're finished, save your network by 'exporting' it (on the file menu) as any other format than .gephi . I say this because I find sometimes the .gephi format is unstable. You can then filter your network for resources or html pages or what have you. I suggest deleting the node labelled 127.0.0.1 because that's your computer and it will throw off any metrics you choose to calculate.","title":"Begin the scrape"},{"location":"supporting materials/graphing-the-net.txt/#what-does-it-all-mean","text":"Well, that depends. For an example of why you might want to do all this, and what you might find, see 'Shouting Into the Void?' a piece where I tried to understand the shape of the archaeological web.","title":"What does it all mean?"},{"location":"supporting materials/inkscape/","text":"Sprucing up a PDF in Inkscape Some of the tools that we used in Module 4 give visual output as raster images, others as vectors. Sometimes, we would like to tweak these outputs to make them more visually appealling, or clearer, or more useful. A program like MS Paint is only useful for dealing with raster images (and then, only in certain kinds of cases). We need a program that can deal with both, and also, lets us edit the image by treating each edit we do as a mostly-transparent layer on top of the image. That way, we can add, rearrange, hide or reveal, our edits to create a composite image. A free program that is immensely useful in this regard is Inkscape . Inkscape is also quite useful in that we can open a PDF file in it, break the visual elements of the PDF into individual layers, and then rearrange/touch up/fix them up to make them more esthetically appealing. Raster versus Vector The first thing to know is that graphic images come in two distinct flavours raster and vector. Raster images are composed of pixels, such that if you zoom into them, they become a pointilist blur (if you remember Ferris Beuller's Day Off , there's a scene where Cameron stares and stares at a painting, falling into its individual points of colour). Vector images on the other hand are described by mathematical functions, of lines and arcs and areas. No matter how deep you delve into these kinds of images, the image is always sharp because the zoom is just another function of the mathematics describing the image. Rasters: blocks of colours Vectors: descriptions of points and arcs In this first exercise, we will take the plot we generated in Module 4's exercise on topic modeling in R where we made a bar chart showing the number of articles by year. In R we exported that plot as a PDF. In Inkscape, we can import that PDF and 'explode' it so that we can manipulate its parts individually. We are going to take the simple bar chart and make it more legible, more visually appealing, for incorporation on a webpage. Download and install Inkscape . NB Mac the installation instructions are a bit more complicated for Mac. Pay attention and follow closely! Download the PDF we generated in R called publication-year.pdf (this downloads the PDF) . Open that PDF. It's a pretty plain graphic. Right away there are at least two things we could do to make it more visually appealling. We could change the orientation of the characters in the y-axis to make them more legible. We can highlight bars of interest. And we could apply a colour scheme more generally that would make our graphic legible to folks with colour-blindness (see the Going Further section below). Start Inkscape. Click File Import and then navigate to where you saved publication-year.pdf . Click Ok when you've got it selected. In the next pop-up, just accept the default settings and click 'ok'. Your Inkscape window should now resemble the following: The PDF is now a layer in the new image you are creating in Inkscape. You can save this drawing, with its information about the layers and what is in each one by clicking File Save As. ( Visit GitHub for my version ). 'SVG' stands for 'scalable vector graphic'. (SVG is a kind of text file that describes the complete geometry of your illustration). Do you see the bounding box around the plot? If you grab any of those handles (the double-arrow things), you can make it bigger or smaller on your sheet. To retain the image proportions, hold ctrl + shift as you drag. We can't edit any of the other elements yet we can't change the colour of the bars, or the fonts of the text. We have to tell Inkscape to 'explode' these elements into their own 'objects'. In the menu bar at top, got to Object Ungroup. There are now a series of overlapping bounding boxes around each object. Zoom in (by pressing the + plus sign on your keyboard) so that you're looking at the numbers of the y-axis. We're going to rotate these by 90 degrees to make them more legible. Select the arrow icon from the toolbar on the left side of the screen. Click on the '50'. You'll get a bounding box around it. Click Object Rotate 90 CW. The 50 is now rotated! Do the same for the other numbers. Save. (If you double-click on the number, you might trigger the 'text edit' function. If you do that, no problem you can change the font, change the number... although if you did that, it'd be a bit dishonest, right? Click on the arrow pointer icon in the toolbar again to get out of the text-editing function). Let's imagine, for whatever reason, that you wanted to change one of the bars to a different colour, to highlight its importance to your argument. With the arrow icon, click on one of the bars so that you get the bounding box around it. Then, click on one of the colours from the palette at the bottom. Boom! You've got a newly colourized bar. Save. Add a legend. Write it so that the important message you want your viewer to get is immediately clear. Choose a font from Inkscape's included fonts that supports your message. To export your image so that you can use it in a website or paper, click Edit Select All in All Layers. Every element of your image will now have a bounding box around it. Go to File Export Bitmap. Never mind the options in the popup; just hit 'Export'. Inkscape will automatically assign your drawing a name with .png ; visit my version on GitHub . Remember if you want to edit this image again later, hit the 'Save' button to save it as an SVG. The SVG will preserve all your layer information, while the PNG file is the visual representation (the PNG is in fact a raster graphic). Most browsers can handle SVG files, so you could use that in your website; programs like Word seem to be able to handle raster graphics better than they do SVG. You might want to experiment. In any event, every journal has different requirements for image formats. You would export your image to whatever those specifications are. Going further In infoheap's tutorial on inkscape , you will learn how to load a custom colour palette. Why might you want to do that? You should be designing your work so that it is as universally accessible as possible. Many folks are colour-blind. Use Color Brewer to generate a colour-blind safe palette. Then look for the 'GIMP and Inkscape GIMP color palette for this scheme.' Click on that link, and you'll get a text file with the scheme you generated. Use that scheme to alter the colours on your plot.","title":"Inkscape"},{"location":"supporting materials/inkscape/#sprucing-up-a-pdf-in-inkscape","text":"Some of the tools that we used in Module 4 give visual output as raster images, others as vectors. Sometimes, we would like to tweak these outputs to make them more visually appealling, or clearer, or more useful. A program like MS Paint is only useful for dealing with raster images (and then, only in certain kinds of cases). We need a program that can deal with both, and also, lets us edit the image by treating each edit we do as a mostly-transparent layer on top of the image. That way, we can add, rearrange, hide or reveal, our edits to create a composite image. A free program that is immensely useful in this regard is Inkscape . Inkscape is also quite useful in that we can open a PDF file in it, break the visual elements of the PDF into individual layers, and then rearrange/touch up/fix them up to make them more esthetically appealing.","title":"Sprucing up a PDF in Inkscape"},{"location":"supporting materials/inkscape/#raster-versus-vector","text":"The first thing to know is that graphic images come in two distinct flavours raster and vector. Raster images are composed of pixels, such that if you zoom into them, they become a pointilist blur (if you remember Ferris Beuller's Day Off , there's a scene where Cameron stares and stares at a painting, falling into its individual points of colour). Vector images on the other hand are described by mathematical functions, of lines and arcs and areas. No matter how deep you delve into these kinds of images, the image is always sharp because the zoom is just another function of the mathematics describing the image. Rasters: blocks of colours Vectors: descriptions of points and arcs In this first exercise, we will take the plot we generated in Module 4's exercise on topic modeling in R where we made a bar chart showing the number of articles by year. In R we exported that plot as a PDF. In Inkscape, we can import that PDF and 'explode' it so that we can manipulate its parts individually. We are going to take the simple bar chart and make it more legible, more visually appealing, for incorporation on a webpage. Download and install Inkscape . NB Mac the installation instructions are a bit more complicated for Mac. Pay attention and follow closely! Download the PDF we generated in R called publication-year.pdf (this downloads the PDF) . Open that PDF. It's a pretty plain graphic. Right away there are at least two things we could do to make it more visually appealling. We could change the orientation of the characters in the y-axis to make them more legible. We can highlight bars of interest. And we could apply a colour scheme more generally that would make our graphic legible to folks with colour-blindness (see the Going Further section below). Start Inkscape. Click File Import and then navigate to where you saved publication-year.pdf . Click Ok when you've got it selected. In the next pop-up, just accept the default settings and click 'ok'. Your Inkscape window should now resemble the following: The PDF is now a layer in the new image you are creating in Inkscape. You can save this drawing, with its information about the layers and what is in each one by clicking File Save As. ( Visit GitHub for my version ). 'SVG' stands for 'scalable vector graphic'. (SVG is a kind of text file that describes the complete geometry of your illustration). Do you see the bounding box around the plot? If you grab any of those handles (the double-arrow things), you can make it bigger or smaller on your sheet. To retain the image proportions, hold ctrl + shift as you drag. We can't edit any of the other elements yet we can't change the colour of the bars, or the fonts of the text. We have to tell Inkscape to 'explode' these elements into their own 'objects'. In the menu bar at top, got to Object Ungroup. There are now a series of overlapping bounding boxes around each object. Zoom in (by pressing the + plus sign on your keyboard) so that you're looking at the numbers of the y-axis. We're going to rotate these by 90 degrees to make them more legible. Select the arrow icon from the toolbar on the left side of the screen. Click on the '50'. You'll get a bounding box around it. Click Object Rotate 90 CW. The 50 is now rotated! Do the same for the other numbers. Save. (If you double-click on the number, you might trigger the 'text edit' function. If you do that, no problem you can change the font, change the number... although if you did that, it'd be a bit dishonest, right? Click on the arrow pointer icon in the toolbar again to get out of the text-editing function). Let's imagine, for whatever reason, that you wanted to change one of the bars to a different colour, to highlight its importance to your argument. With the arrow icon, click on one of the bars so that you get the bounding box around it. Then, click on one of the colours from the palette at the bottom. Boom! You've got a newly colourized bar. Save. Add a legend. Write it so that the important message you want your viewer to get is immediately clear. Choose a font from Inkscape's included fonts that supports your message. To export your image so that you can use it in a website or paper, click Edit Select All in All Layers. Every element of your image will now have a bounding box around it. Go to File Export Bitmap. Never mind the options in the popup; just hit 'Export'. Inkscape will automatically assign your drawing a name with .png ; visit my version on GitHub . Remember if you want to edit this image again later, hit the 'Save' button to save it as an SVG. The SVG will preserve all your layer information, while the PNG file is the visual representation (the PNG is in fact a raster graphic). Most browsers can handle SVG files, so you could use that in your website; programs like Word seem to be able to handle raster graphics better than they do SVG. You might want to experiment. In any event, every journal has different requirements for image formats. You would export your image to whatever those specifications are.","title":"Raster versus Vector"},{"location":"supporting materials/inkscape/#going-further","text":"In infoheap's tutorial on inkscape , you will learn how to load a custom colour palette. Why might you want to do that? You should be designing your work so that it is as universally accessible as possible. Many folks are colour-blind. Use Color Brewer to generate a colour-blind safe palette. Then look for the 'GIMP and Inkscape GIMP color palette for this scheme.' Click on that link, and you'll get a text file with the scheme you generated. Use that scheme to alter the colours on your plot.","title":"Going further"},{"location":"supporting materials/leaflet.txt/","text":"Making a map website with Leaflet.js The leaflet.js library allows you to create quite nice interactive maps in a web-browser, that are also mobile friendly. Here, we'll build a map that uses our georectified map as the base layer (rather than as an image overlay). I won't go into the details, but rather will provide you enough guidance to get going. The documentation for Leaflet is quite extensive, and many other tutorials abound. Setup Create a new GitHub repository for this exercise. Create a new branch called gh-pages . We will be putting our HTML on the gh-pages branch, so that your username /github.io/ repo map.html can serve us up the webpage when we're done. Leaflet comes with a number of excellent tutorials . We're going to look at the first one . Go to the Leaflet quick-start tutorial and read through it carefully. In essence, you create a webpage that draws its instructions on how to handle geographic data and how to style that data from the leaflet.js source. That way, the browser knows how to render all the geographic information you're about to give it. Do you see where Leaflet is calling on geographic information? The following code is calling on a background layer from the Mapbox service: L.tileLayer('http://{s}.tiles.mapbox.com/v3/MapID/{z}/{x}/{y}.png', { attribution: 'Map data copy; a href=\"http://openstreetmap.org\" OpenStreetMap /a contributors, a href=\"http://creativecommons.org/licenses/by-sa/2.0/\" CC-BY-SA /a , Imagery \u00a9 a href=\"http://mapbox.com\" Mapbox /a ', maxZoom: 18 }).addTo(map); Instead of using Mapbox, We can slot the URL to the georectified map we made in Module 4 in there! That is, swap out the URL from the line beginning L.tileLayer . You'd change the copyright notice, etc, too, obviously. The other bits of code that create callouts, polygons, and so on, are all using decimal degrees to locate the drawn elements on top of that map. The following code can be copied and repeated in the document, with new coordinates in decimal degrees for each new point: L.marker([51.5, -0.09]).addTo(map) .bindPopup(\" b Hello world! /b br / I am a popup.\").openPopup(); In the second line, between the quotation marks, you can use regular HTML to style your text, include pictures, and so on. In a new browser window, open the example map from the quickstart guide . Right-click the page and select View source to inspect the code. So let's get started Create a new HTML document in your gh-pages branch of your repo. Copy the HTML from the quickstart map (right-click and select 'View source' on Leaflet's quick start example page . Paste the code in your new HTML document in your gh-pages branch of your repo. Call it map.html and commit your changes. Change the source map to point to a georectified map you made in Module 4. Using the Ottawa Fire Insurance map I used as an example in Module 4, I created this map . Right click and view my page source to see what I changed up. NB You could keep the basic Mapbox service base map, and render the Ottawa Fire Insurance map as an overlay reference documentation . Or you could do a series of overlays, showing the change in the city over time. (My favourite example of a Leaflet-powered historical map visualization is the Slave Revolt in Jamaica project by Vincent Brown.) But you don't necessarily have to do this. Add a series of markers with historical information by duplicating and then changing up the L.marker settings to your own data. Commit your changes! This all just makes the map. The rest of the webpage would have to be styled as you would normally for a webpage. That is, you'd probably want to add an explanation about what the map shows, how it was created, how the user ought to interact with it, links to your source data, and so on. The easiest place to add all that kind of information would be between the following two tags in the page source: /script /body Going further Let's say you have a whole bunch of information that you want to represent on the map. Perhaps it's in a well organized CSV file, with a latitude and a longitude column in decimal degrees. Adding points one at a time to the map as described above would take ages. Instead, let's convert that CSV to geojson, and then use BootLeaf to make a map. BootLeaf is a template that uses a common HTML template package, Bootstrap , as a wrapper for a Leaflet powered map that draws its points of interest from a geojson file. To get this up and running, do the following steps: Go to the GitHub repo for BootLeaf . Fork a copy to a new repo (you have to be logged into GitHub) by hitting the 'Fork' button. In your copy of BootLeaf, you now have a gh-pages version of the site. If you go to yourusername .github.io/bootleaf you should see an active version of the map. Now, the map is grabbing its data from a series of geojson files. You can use the 'to geo json' service to convert your CSV to geojson. There are other services. Clone your repository in your desktop (by pressing the clone your repo in desktop). Open your desktop client, and make sure you're in the gh-pages branch. Using your Windows explorer or Mac Finder, put your newly created geojson file in the data folder. Commit and sync your changes. To add your data to the dropdown menu, you need to change the following code in the index.html file: li class=\"dropdown\" a class=\"dropdown-toggle\" id=\"downloadDrop\" href=\"#\" role=\"button\" data-toggle=\"dropdown\" i class=\"fa fa-cloud-download white\" /i nbsp; nbsp;Download b class=\"caret\" /b /a ul class=\"dropdown-menu\" li a href=\"data/boroughs.geojson\" download=\"boroughs.geojson\" target=\"_blank\" data-toggle=\"collapse\" data-target=\".navbar-collapse.in\" i class=\"fa fa-download\" /i nbsp; nbsp;Boroughs /a /li li a href=\"data/subways.geojson\" download=\"subways.geojson\" target=\"_blank\" data-toggle=\"collapse\" data-target=\".navbar-collapse.in\" i class=\"fa fa-download\" /i nbsp; nbsp;Subway Lines /a /li li a href=\"data/DOITT_THEATER_01_13SEPT2010.geojson\" download=\"theaters.geojson\" target=\"_blank\" data-toggle=\"collapse\" data-target=\".navbar-collapse.in\" i class=\"fa fa-download\" /i nbsp; nbsp;Theaters /a /li li a href=\"data/DOITT_MUSEUM_01_13SEPT2010.geojson\" download=\"museums.geojson\" target=\"_blank\" data-toggle=\"collapse\" data-target=\".navbar-collapse.in\" i class=\"fa fa-download\" /i nbsp; nbsp;Museums /a /li /ul And since you probably don't want that other stuff, you could delete it. You could change the 'about' pop-up like the following: div class=\"tab-pane fade active in\" id=\"about\" p A simple, responsive template for building web mapping applications with a href=\"http://getbootstrap.com/\" Bootstrap 3 /a , a href=\"http://leafletjs.com/\" target=\"_blank\" Leaflet /a , and a href=\"http://twitter.github.io/typeahead.js/\" target=\"_blank\" typeahead.js /a . Open source, MIT licensed, and available on a href=\"https://github.com/bmcbride/bootleaf\" target=\"_blank\" GitHub /a . /p div class=\"panel panel-primary\" div class=\"panel-heading\" Features /div ul class=\"list-group\" li class=\"list-group-item\" Fullscreen mobile-friendly map template with responsive navbar and modal placeholders /li li class=\"list-group-item\" jQuery loading of external GeoJSON files /li li class=\"list-group-item\" Logical multiple layer marker clustering via the a href=\"https://github.com/Leaflet/Leaflet.markercluster\" target=\"_blank\" leaflet marker cluster plugin /a /li li class=\"list-group-item\" Elegant client-side multi-layer feature search with autocomplete using a href=\"http://twitter.github.io/typeahead.js/\" target=\"_blank\" typeahead.js /a /li li class=\"list-group-item\" Responsive sidebar feature list synced with map bounds, which includes sorting and filtering via a href=\"http://listjs.com/\" target=\"_blank\" list.js /a /li li class=\"list-group-item\" Marker icons included in grouped layer control via the a href=\"https://github.com/ismyrnow/Leaflet.groupedlayercontrol\" target=\"_blank\" grouped layer control plugin /a /li /ul /div And you have to remove the following: !-- Remove this maptiks analytics code from your BootLeaf implementation -- script src=\"//cdn.maptiks.com/maptiks-leaflet.min.js\" /script script maptiks.trackcode='c7ca251e-9c17-47ef-ac33-e0fb24e05976'; /script !-- End maptiks analytics code -- Now the really hard part: putting your own base maps in. To insert your own base map, you have to find, and modify, a file called app.js . You should be able to find it by following the path: bootleaf/assets/js/app.js . You need to change these lines to point to your maps like the following: /* Basemap Layers */ var mapquestOSM = L.tileLayer(\"http://{s}.mqcdn.com/tiles/1.0.0/osm/{z}/{x}/{y}.png\", { maxZoom: 19, subdomains: [\"otile1\", \"otile2\", \"otile3\", \"otile4\"], attribution: 'Tiles courtesy of a href=\"http://www.mapquest.com/\" target=\"_blank\" MapQuest /a img src=\"http://developer.mapquest.com/content/osm/mq_logo.png\" . Map data (c) a href=\"http://www.openstreetmap.org/\" target=\"_blank\" OpenStreetMap /a contributors, CC-BY-SA.' }); var mapquestOAM = L.tileLayer(\"http://{s}.mqcdn.com/tiles/1.0.0/sat/{z}/{x}/{y}.jpg\", { maxZoom: 18, subdomains: [\"oatile1\", \"oatile2\", \"oatile3\", \"oatile4\"], attribution: 'Tiles courtesy of a href=\"http://www.mapquest.com/\" target=\"_blank\" MapQuest /a . Portions Courtesy NASA/JPL-Caltech and U.S. Depart. of Agriculture, Farm Service Agency' }); var mapquestHYB = L.layerGroup([L.tileLayer(\"http://{s}.mqcdn.com/tiles/1.0.0/sat/{z}/{x}/{y}.jpg\", { maxZoom: 18, subdomains: [\"oatile1\", \"oatile2\", \"oatile3\", \"oatile4\"] }), L.tileLayer(\"http://{s}.mqcdn.com/tiles/1.0.0/hyb/{z}/{x}/{y}.png\", { maxZoom: 19, subdomains: [\"oatile1\", \"oatile2\", \"oatile3\", \"oatile4\"], attribution: 'Labels courtesy of a href=\"http://www.mapquest.com/\" target=\"_blank\" MapQuest /a img src=\"http://developer.mapquest.com/content/osm/mq_logo.png\" . Map data (c) a href=\"http://www.openstreetmap.org/\" target=\"_blank\" OpenStreetMap /a contributors, CC-BY-SA. Portions Courtesy NASA/JPL-Caltech and U.S. Depart. of Agriculture, Farm Service Agency' })]); ...and then you'd have to go through the rest of that file and change up the .geojson pointers to point to your own data. Visit GitHub for a template for mapping with Leaflet , drawing all of your point data and ancillary information from a CSV file. Study the index.html file carefully to identify which lines you'd modify to change the base map, and to identify how elements in the CSV are being rendered on the screen. Visit this example that a former student made . Further reading: Arian Katsimbras on making beautiful maps with Tilemill .","title":"Leaflet"},{"location":"supporting materials/leaflet.txt/#making-a-map-website-with-leafletjs","text":"The leaflet.js library allows you to create quite nice interactive maps in a web-browser, that are also mobile friendly. Here, we'll build a map that uses our georectified map as the base layer (rather than as an image overlay). I won't go into the details, but rather will provide you enough guidance to get going. The documentation for Leaflet is quite extensive, and many other tutorials abound.","title":"Making a map website with Leaflet.js"},{"location":"supporting materials/leaflet.txt/#setup","text":"Create a new GitHub repository for this exercise. Create a new branch called gh-pages . We will be putting our HTML on the gh-pages branch, so that your username /github.io/ repo map.html can serve us up the webpage when we're done. Leaflet comes with a number of excellent tutorials . We're going to look at the first one . Go to the Leaflet quick-start tutorial and read through it carefully. In essence, you create a webpage that draws its instructions on how to handle geographic data and how to style that data from the leaflet.js source. That way, the browser knows how to render all the geographic information you're about to give it. Do you see where Leaflet is calling on geographic information? The following code is calling on a background layer from the Mapbox service: L.tileLayer('http://{s}.tiles.mapbox.com/v3/MapID/{z}/{x}/{y}.png', { attribution: 'Map data copy; a href=\"http://openstreetmap.org\" OpenStreetMap /a contributors, a href=\"http://creativecommons.org/licenses/by-sa/2.0/\" CC-BY-SA /a , Imagery \u00a9 a href=\"http://mapbox.com\" Mapbox /a ', maxZoom: 18 }).addTo(map); Instead of using Mapbox, We can slot the URL to the georectified map we made in Module 4 in there! That is, swap out the URL from the line beginning L.tileLayer . You'd change the copyright notice, etc, too, obviously. The other bits of code that create callouts, polygons, and so on, are all using decimal degrees to locate the drawn elements on top of that map. The following code can be copied and repeated in the document, with new coordinates in decimal degrees for each new point: L.marker([51.5, -0.09]).addTo(map) .bindPopup(\" b Hello world! /b br / I am a popup.\").openPopup(); In the second line, between the quotation marks, you can use regular HTML to style your text, include pictures, and so on. In a new browser window, open the example map from the quickstart guide . Right-click the page and select View source to inspect the code.","title":"Setup"},{"location":"supporting materials/leaflet.txt/#so-lets-get-started","text":"Create a new HTML document in your gh-pages branch of your repo. Copy the HTML from the quickstart map (right-click and select 'View source' on Leaflet's quick start example page . Paste the code in your new HTML document in your gh-pages branch of your repo. Call it map.html and commit your changes. Change the source map to point to a georectified map you made in Module 4. Using the Ottawa Fire Insurance map I used as an example in Module 4, I created this map . Right click and view my page source to see what I changed up. NB You could keep the basic Mapbox service base map, and render the Ottawa Fire Insurance map as an overlay reference documentation . Or you could do a series of overlays, showing the change in the city over time. (My favourite example of a Leaflet-powered historical map visualization is the Slave Revolt in Jamaica project by Vincent Brown.) But you don't necessarily have to do this. Add a series of markers with historical information by duplicating and then changing up the L.marker settings to your own data. Commit your changes! This all just makes the map. The rest of the webpage would have to be styled as you would normally for a webpage. That is, you'd probably want to add an explanation about what the map shows, how it was created, how the user ought to interact with it, links to your source data, and so on. The easiest place to add all that kind of information would be between the following two tags in the page source: /script /body","title":"So let's get started"},{"location":"supporting materials/leaflet.txt/#going-further","text":"Let's say you have a whole bunch of information that you want to represent on the map. Perhaps it's in a well organized CSV file, with a latitude and a longitude column in decimal degrees. Adding points one at a time to the map as described above would take ages. Instead, let's convert that CSV to geojson, and then use BootLeaf to make a map. BootLeaf is a template that uses a common HTML template package, Bootstrap , as a wrapper for a Leaflet powered map that draws its points of interest from a geojson file. To get this up and running, do the following steps: Go to the GitHub repo for BootLeaf . Fork a copy to a new repo (you have to be logged into GitHub) by hitting the 'Fork' button. In your copy of BootLeaf, you now have a gh-pages version of the site. If you go to yourusername .github.io/bootleaf you should see an active version of the map. Now, the map is grabbing its data from a series of geojson files. You can use the 'to geo json' service to convert your CSV to geojson. There are other services. Clone your repository in your desktop (by pressing the clone your repo in desktop). Open your desktop client, and make sure you're in the gh-pages branch. Using your Windows explorer or Mac Finder, put your newly created geojson file in the data folder. Commit and sync your changes. To add your data to the dropdown menu, you need to change the following code in the index.html file: li class=\"dropdown\" a class=\"dropdown-toggle\" id=\"downloadDrop\" href=\"#\" role=\"button\" data-toggle=\"dropdown\" i class=\"fa fa-cloud-download white\" /i nbsp; nbsp;Download b class=\"caret\" /b /a ul class=\"dropdown-menu\" li a href=\"data/boroughs.geojson\" download=\"boroughs.geojson\" target=\"_blank\" data-toggle=\"collapse\" data-target=\".navbar-collapse.in\" i class=\"fa fa-download\" /i nbsp; nbsp;Boroughs /a /li li a href=\"data/subways.geojson\" download=\"subways.geojson\" target=\"_blank\" data-toggle=\"collapse\" data-target=\".navbar-collapse.in\" i class=\"fa fa-download\" /i nbsp; nbsp;Subway Lines /a /li li a href=\"data/DOITT_THEATER_01_13SEPT2010.geojson\" download=\"theaters.geojson\" target=\"_blank\" data-toggle=\"collapse\" data-target=\".navbar-collapse.in\" i class=\"fa fa-download\" /i nbsp; nbsp;Theaters /a /li li a href=\"data/DOITT_MUSEUM_01_13SEPT2010.geojson\" download=\"museums.geojson\" target=\"_blank\" data-toggle=\"collapse\" data-target=\".navbar-collapse.in\" i class=\"fa fa-download\" /i nbsp; nbsp;Museums /a /li /ul And since you probably don't want that other stuff, you could delete it. You could change the 'about' pop-up like the following: div class=\"tab-pane fade active in\" id=\"about\" p A simple, responsive template for building web mapping applications with a href=\"http://getbootstrap.com/\" Bootstrap 3 /a , a href=\"http://leafletjs.com/\" target=\"_blank\" Leaflet /a , and a href=\"http://twitter.github.io/typeahead.js/\" target=\"_blank\" typeahead.js /a . Open source, MIT licensed, and available on a href=\"https://github.com/bmcbride/bootleaf\" target=\"_blank\" GitHub /a . /p div class=\"panel panel-primary\" div class=\"panel-heading\" Features /div ul class=\"list-group\" li class=\"list-group-item\" Fullscreen mobile-friendly map template with responsive navbar and modal placeholders /li li class=\"list-group-item\" jQuery loading of external GeoJSON files /li li class=\"list-group-item\" Logical multiple layer marker clustering via the a href=\"https://github.com/Leaflet/Leaflet.markercluster\" target=\"_blank\" leaflet marker cluster plugin /a /li li class=\"list-group-item\" Elegant client-side multi-layer feature search with autocomplete using a href=\"http://twitter.github.io/typeahead.js/\" target=\"_blank\" typeahead.js /a /li li class=\"list-group-item\" Responsive sidebar feature list synced with map bounds, which includes sorting and filtering via a href=\"http://listjs.com/\" target=\"_blank\" list.js /a /li li class=\"list-group-item\" Marker icons included in grouped layer control via the a href=\"https://github.com/ismyrnow/Leaflet.groupedlayercontrol\" target=\"_blank\" grouped layer control plugin /a /li /ul /div And you have to remove the following: !-- Remove this maptiks analytics code from your BootLeaf implementation -- script src=\"//cdn.maptiks.com/maptiks-leaflet.min.js\" /script script maptiks.trackcode='c7ca251e-9c17-47ef-ac33-e0fb24e05976'; /script !-- End maptiks analytics code -- Now the really hard part: putting your own base maps in. To insert your own base map, you have to find, and modify, a file called app.js . You should be able to find it by following the path: bootleaf/assets/js/app.js . You need to change these lines to point to your maps like the following: /* Basemap Layers */ var mapquestOSM = L.tileLayer(\"http://{s}.mqcdn.com/tiles/1.0.0/osm/{z}/{x}/{y}.png\", { maxZoom: 19, subdomains: [\"otile1\", \"otile2\", \"otile3\", \"otile4\"], attribution: 'Tiles courtesy of a href=\"http://www.mapquest.com/\" target=\"_blank\" MapQuest /a img src=\"http://developer.mapquest.com/content/osm/mq_logo.png\" . Map data (c) a href=\"http://www.openstreetmap.org/\" target=\"_blank\" OpenStreetMap /a contributors, CC-BY-SA.' }); var mapquestOAM = L.tileLayer(\"http://{s}.mqcdn.com/tiles/1.0.0/sat/{z}/{x}/{y}.jpg\", { maxZoom: 18, subdomains: [\"oatile1\", \"oatile2\", \"oatile3\", \"oatile4\"], attribution: 'Tiles courtesy of a href=\"http://www.mapquest.com/\" target=\"_blank\" MapQuest /a . Portions Courtesy NASA/JPL-Caltech and U.S. Depart. of Agriculture, Farm Service Agency' }); var mapquestHYB = L.layerGroup([L.tileLayer(\"http://{s}.mqcdn.com/tiles/1.0.0/sat/{z}/{x}/{y}.jpg\", { maxZoom: 18, subdomains: [\"oatile1\", \"oatile2\", \"oatile3\", \"oatile4\"] }), L.tileLayer(\"http://{s}.mqcdn.com/tiles/1.0.0/hyb/{z}/{x}/{y}.png\", { maxZoom: 19, subdomains: [\"oatile1\", \"oatile2\", \"oatile3\", \"oatile4\"], attribution: 'Labels courtesy of a href=\"http://www.mapquest.com/\" target=\"_blank\" MapQuest /a img src=\"http://developer.mapquest.com/content/osm/mq_logo.png\" . Map data (c) a href=\"http://www.openstreetmap.org/\" target=\"_blank\" OpenStreetMap /a contributors, CC-BY-SA. Portions Courtesy NASA/JPL-Caltech and U.S. Depart. of Agriculture, Farm Service Agency' })]); ...and then you'd have to go through the rest of that file and change up the .geojson pointers to point to your own data. Visit GitHub for a template for mapping with Leaflet , drawing all of your point data and ancillary information from a CSV file. Study the index.html file carefully to identify which lines you'd modify to change the base map, and to identify how elements in the CSV are being rendered on the screen. Visit this example that a former student made . Further reading: Arian Katsimbras on making beautiful maps with Tilemill .","title":"Going further"},{"location":"supporting materials/multimode-networks.txt/","text":"Transforming 2-mode network data to 1-mode Networks can be composed of all sorts of things. Trains, busses, Uber, metro all of these can be combined into a 'transportation' network. Books, authors, editors, funders, censors, sponsors a publishing network. Students, profs, classes, universities an education network. Anytime you have more than one kind of thing (however defined) in your network, you formally have a bipartite (or tripartite, and so on, depending on the number of things) network. Visualizing such a network as nodes and edges can be a useful heuristic exercise. But If you are doing this in Gephi, and you run some of Gephi's metrics (statistics) on a bipartite network, your results may not mean what you think they mean. The metrics, the algorithms assume 1-mode networks. Thus, running them on 2-mode (or more!) networks will result in ....issues. Now in truth, it's not quite so clear-cut to simply say, 'convert every bimodal network to unimodal before running any stats' (read Scott's discussion here ), but as a rule of thumb for when you're getting started, you'll be on much firmer methodological grounds if you transform your mutlimodal networks into a series of 1-mode networks. So convert every bimodal network to unimodal before running any statistics on your network(s). Thus, this network: ProfA - student1 (where - is a directed relationship, 'teaches') ProfA - student2 ProfB - student3 ProbB - student1 ...can be transformed into two networks, one where profA is connected to profB by virtue of a shared student (student1). That is, profs connected by students; and the inverse: students connected by profs. You could then run your metrics, and get two different perspectives on the educational dynamics of this particular university. In this exercise, you'll transform a network of women and social organizations into two 1-mode networks. The data The data for this exercise comes from a former Carleton public history MA student, Peter Holdsworth. Peter lodged a copy of his MA research on Figshare . Peter was interested in the social networks surrounding ideas of commemoration of the centenerary of the War of 1812, in 1912. He studied the membership rolls for women\u2019s service organization in Ontario both before and after that centenerary. By making his data public, Peter enables others to build upon his own research in a way not commonly done in history. (You can follow Peter on Twitter .) Right-click and 'save link' to get the data files you'll need for this exercise Configuring Gephi There is a plugin for Gephi that we will use to transform our network. Open Gephi. Across the top of Gephi in the menu ribbon you\u2019ll see File Workspace View Tools Window Plugins Help . To get and install the plugin, select Tools Plugins (The top level menu item 'Plugins' is empty and not used a useful reminder that Gephi is still in beta ). In the popup, under \u2018available plugins\u2019 look for \u2018MultimodeNetworksTransformation\u2019. Tick this box, then click on Install. Follow the instructions, ignore any warnings, click on \u2018finish\u2019. You may or may not need to restart Gephi to get the plugin running. If you suddenly see on the far right of the Gephi window a new tab beside \u2018statistics\u2019, \u2018filters\u2019, called \u2018Multimode Network\u2019, then you\u2019re ok. Importing the data Under \u2018File\u2019, select New project. On the Data Laboratory tab, select Import spreadsheet, and in the pop-up, make sure to select under \u2018As table: EDGES table. Select women-orgs.csv . Click \u2018Next\u2019. Then click Finish. On the data table, have \u2018edges\u2019 selected. This is showing you the source and the target for each link (AKA \u2018edge\u2019). This implies a directionality to the relationship that we just don\u2019t know \u2013 so down below, when we get to statistics, we will always have to make sure to tell Gephi that we want the network treated as \u2018undirected\u2019. More on that below. Loading your CSV file, step 1. Loading your CSV file, step 2 Click on \u2018Copy data to other column\u2019. Select \u2018Id\u2019. In the pop-up, select \u2018Label\u2019. You now have your edges labelled. Just as you did above, now import NODES women-names.csv . Making sure you're on the Nodes page in the data laboratory, copy ID to Label to that your nodes are labelled. NB You can always add more attribute data to your network this way, as long as you always use a column called Id so that Gephi knows where to slot the new information. Make sure to never tick off the box labeled \u2018force nodes to be created as new ones\u2019 Prepping your data We're now going to manipulate the data a bit in order to get it ready for the transformation. In this data, we have women, and we have organizations. We need to tell Gephi which rows are the organizations. In Peter's original data input phase, he decided to use a unique number for each woman so that he would minimize data entry errors (misspellings and so on), as well as control for the use of maiden and married names. Miss Eliza Smith might become, at a later date, Mrs. George Doe. In Peter's scheme, this is the same person (remember in Module 2 in the TEI exercise how we dealt with such issues?). On your NODES page in the data laboratory, add new column, make it boolean. Call it \u2018organization\u2019 In the Filter box, type [a-z], and select Id \u2013 this filters out all the women. (What would you have to do to filter out the organizations? Remember, this is a regex search!) Tick off the check boxes in the \u2018organization\u2019 columns. I note a TYPO in the image above, 'a-b'. that should be, 'a-z' Save this as women-organizations-2-mode.gephi . Pro tip : always export your data from Gephi (File Export) in .net or .graphml or .gefx format as the .gephi format (which is your only option under File Save as) is unstable. That is, sometimes Gephi won't read .gephi files! I did say this was beta software). Transforming the network At this point, you have a two mode network in Gephi. You could click on the 'Overview' panel and play with some of the layouts and begin to form impressions about the nature of your data. Remember though any metrics calculated at this point would be largely spurious. Let's transform this two-mode network so that we can explore how women are connected to other women via shared membership. On the multimode networks projection tab: Click Load attributes. In \u2018attribute type\u2019, select organization In left matrix, select \u2018false \u2013 true\u2019 (or \u2018null \u2013 true\u2019) In right matrix, select \u2018true \u2013 false\u2019. (or \u2018true \u2013 null\u2019) (do you see why this is the case? what would selecting the inverse accomplish?) Select \u2018remove edges\u2019 and \u2018remove nodes\u2019. Once you hit \u2018run\u2019, organizations will be removed from your bipartite network, leaving you with a single-mode network. hit \u2018run\u2019. Save as women-to-women-network.gephi and export as women-to-women.net NB If your nodes data table is blank, your filter might still be active. Make sure the filter box is clear. You should be left with a list of women (ie. a list of nodes where the identifiers are numbers, per Peter's schema). At this point, you could re-start Gephi and reload your women-organizations-2-mode.gephi file and re-run the multimode networks projection so that you are left with an organization to organization network. Do this, and save and export with appropriate file names. Exploring this network Peter's data has a number of attributes describing it, including the membership year. So let's see what this network of women looks like in 1902. Under the filters tab at the right side of the Gephi interface, select \u2018attributes \u2013 equal\u2019 and then drag \u20181902\u2019 to the queries box. In \u2018pattern\u2019 enter [0-9] and tick the \u2018use regex\u2019 box. Click ok and then click \u2018filter\u2019. You should now have a network with 188 nodes and 8728 edges, showing the women who were active in 1902. Let\u2019s learn something about this network. Under the Statistics tab at the right side of the Gephi interface, do the following: Run \u2018avg. path length\u2019 by clicking on \u2018run\u2019. In the pop up that opens, select \u2018undirected\u2019 (as we know nothing about directionality in this network; we simply know that two women were members of the same organization at the same time. Note also that if the same pair were members of the more than one organziation, the weight of their connection will be corresponding stronger). Click ok. Run \u2018modularity\u2019 to look for subgroups. Make sure \u2018randomize\u2019 and \u2018use weights\u2019 are selected. Leave \u2018resolution\u2019 at 1.0 We selected 'average path length' because one of the byproducts of this routine is 'betweeness centrality'. We're making an assumption here that a woman who has a high betweeness centrality score was in a position to affect information flow in 1902 society. Modularity looks at similar patterns of connections to cluster women who have more-or-less similar connections into groups. Let\u2019s visualize what we\u2019ve just learned. On the \u2018partition\u2019 tab, over on the left side of the \u2018overview\u2019 screen, click on nodes, then click the green arrows beside \u2018choose a partition parameter\u2019. Click on \u2018choose a partition parameter\u2019. Scroll down to modularity class. The different groups will be listed, with their colours and their % composition of the network. Hit \u2018apply\u2019 to recolour your network graph. Let\u2019s resize the nodes to show off betweeness-centrality (to figure out which woman was in the greatest position to influence flows of information in this network.) Click \u2018ranking\u2019. (It's on the left side of the interface, beside 'partition' and just below 'overview'. Click \u2018nodes\u2019. Click the down arrow on \u2018choose a rank parameter\u2019. Select \u2018betweeness centrality\u2019. Click the red diamond. This will resize the nodes according to their \u2018betweeness centrality\u2019. Click \u2018apply\u2019. Down at the bottom of the middle panel, click the large black \u2018T\u2019 to display labels. Click the black letter \u2018A\u2019 and select \u2018node size\u2019. Mrs. Mary Elliot-Murray-Kynynmound and Mrs. John Henry Wilson should now dominate your network (if you go back to the original data zip, you'll be able to find Peter's key to figure out who's who). Who were these ladies? What organizations were they members of? Who were they connected to? To the archives! Congratulations! You\u2019ve imported historical network data into Gephi, transformed it, manipulated it, and run some analyses. Play with the settings on \u2018preview\u2019 in order to share your visualization as SVG, PDF, or PNG. Now go back to your original Gephi file, and recast it as organizations to organizations via shared members, to figure out which organizations were key in early 20th century Ontario\u2026 make appropriate notes in your open notebook.","title":"Transforming 2-mode network data to 1-mode"},{"location":"supporting materials/multimode-networks.txt/#transforming-2-mode-network-data-to-1-mode","text":"Networks can be composed of all sorts of things. Trains, busses, Uber, metro all of these can be combined into a 'transportation' network. Books, authors, editors, funders, censors, sponsors a publishing network. Students, profs, classes, universities an education network. Anytime you have more than one kind of thing (however defined) in your network, you formally have a bipartite (or tripartite, and so on, depending on the number of things) network. Visualizing such a network as nodes and edges can be a useful heuristic exercise. But If you are doing this in Gephi, and you run some of Gephi's metrics (statistics) on a bipartite network, your results may not mean what you think they mean. The metrics, the algorithms assume 1-mode networks. Thus, running them on 2-mode (or more!) networks will result in ....issues. Now in truth, it's not quite so clear-cut to simply say, 'convert every bimodal network to unimodal before running any stats' (read Scott's discussion here ), but as a rule of thumb for when you're getting started, you'll be on much firmer methodological grounds if you transform your mutlimodal networks into a series of 1-mode networks. So convert every bimodal network to unimodal before running any statistics on your network(s). Thus, this network: ProfA - student1 (where - is a directed relationship, 'teaches') ProfA - student2 ProfB - student3 ProbB - student1 ...can be transformed into two networks, one where profA is connected to profB by virtue of a shared student (student1). That is, profs connected by students; and the inverse: students connected by profs. You could then run your metrics, and get two different perspectives on the educational dynamics of this particular university. In this exercise, you'll transform a network of women and social organizations into two 1-mode networks.","title":"Transforming 2-mode network data to 1-mode"},{"location":"supporting materials/multimode-networks.txt/#the-data","text":"The data for this exercise comes from a former Carleton public history MA student, Peter Holdsworth. Peter lodged a copy of his MA research on Figshare . Peter was interested in the social networks surrounding ideas of commemoration of the centenerary of the War of 1812, in 1912. He studied the membership rolls for women\u2019s service organization in Ontario both before and after that centenerary. By making his data public, Peter enables others to build upon his own research in a way not commonly done in history. (You can follow Peter on Twitter .) Right-click and 'save link' to get the data files you'll need for this exercise","title":"The data"},{"location":"supporting materials/multimode-networks.txt/#configuring-gephi","text":"There is a plugin for Gephi that we will use to transform our network. Open Gephi. Across the top of Gephi in the menu ribbon you\u2019ll see File Workspace View Tools Window Plugins Help . To get and install the plugin, select Tools Plugins (The top level menu item 'Plugins' is empty and not used a useful reminder that Gephi is still in beta ). In the popup, under \u2018available plugins\u2019 look for \u2018MultimodeNetworksTransformation\u2019. Tick this box, then click on Install. Follow the instructions, ignore any warnings, click on \u2018finish\u2019. You may or may not need to restart Gephi to get the plugin running. If you suddenly see on the far right of the Gephi window a new tab beside \u2018statistics\u2019, \u2018filters\u2019, called \u2018Multimode Network\u2019, then you\u2019re ok.","title":"Configuring Gephi"},{"location":"supporting materials/multimode-networks.txt/#importing-the-data","text":"Under \u2018File\u2019, select New project. On the Data Laboratory tab, select Import spreadsheet, and in the pop-up, make sure to select under \u2018As table: EDGES table. Select women-orgs.csv . Click \u2018Next\u2019. Then click Finish. On the data table, have \u2018edges\u2019 selected. This is showing you the source and the target for each link (AKA \u2018edge\u2019). This implies a directionality to the relationship that we just don\u2019t know \u2013 so down below, when we get to statistics, we will always have to make sure to tell Gephi that we want the network treated as \u2018undirected\u2019. More on that below. Loading your CSV file, step 1. Loading your CSV file, step 2 Click on \u2018Copy data to other column\u2019. Select \u2018Id\u2019. In the pop-up, select \u2018Label\u2019. You now have your edges labelled. Just as you did above, now import NODES women-names.csv . Making sure you're on the Nodes page in the data laboratory, copy ID to Label to that your nodes are labelled. NB You can always add more attribute data to your network this way, as long as you always use a column called Id so that Gephi knows where to slot the new information. Make sure to never tick off the box labeled \u2018force nodes to be created as new ones\u2019","title":"Importing the data"},{"location":"supporting materials/multimode-networks.txt/#prepping-your-data","text":"We're now going to manipulate the data a bit in order to get it ready for the transformation. In this data, we have women, and we have organizations. We need to tell Gephi which rows are the organizations. In Peter's original data input phase, he decided to use a unique number for each woman so that he would minimize data entry errors (misspellings and so on), as well as control for the use of maiden and married names. Miss Eliza Smith might become, at a later date, Mrs. George Doe. In Peter's scheme, this is the same person (remember in Module 2 in the TEI exercise how we dealt with such issues?). On your NODES page in the data laboratory, add new column, make it boolean. Call it \u2018organization\u2019 In the Filter box, type [a-z], and select Id \u2013 this filters out all the women. (What would you have to do to filter out the organizations? Remember, this is a regex search!) Tick off the check boxes in the \u2018organization\u2019 columns. I note a TYPO in the image above, 'a-b'. that should be, 'a-z' Save this as women-organizations-2-mode.gephi . Pro tip : always export your data from Gephi (File Export) in .net or .graphml or .gefx format as the .gephi format (which is your only option under File Save as) is unstable. That is, sometimes Gephi won't read .gephi files! I did say this was beta software).","title":"Prepping your data"},{"location":"supporting materials/multimode-networks.txt/#transforming-the-network","text":"At this point, you have a two mode network in Gephi. You could click on the 'Overview' panel and play with some of the layouts and begin to form impressions about the nature of your data. Remember though any metrics calculated at this point would be largely spurious. Let's transform this two-mode network so that we can explore how women are connected to other women via shared membership. On the multimode networks projection tab: Click Load attributes. In \u2018attribute type\u2019, select organization In left matrix, select \u2018false \u2013 true\u2019 (or \u2018null \u2013 true\u2019) In right matrix, select \u2018true \u2013 false\u2019. (or \u2018true \u2013 null\u2019) (do you see why this is the case? what would selecting the inverse accomplish?) Select \u2018remove edges\u2019 and \u2018remove nodes\u2019. Once you hit \u2018run\u2019, organizations will be removed from your bipartite network, leaving you with a single-mode network. hit \u2018run\u2019. Save as women-to-women-network.gephi and export as women-to-women.net NB If your nodes data table is blank, your filter might still be active. Make sure the filter box is clear. You should be left with a list of women (ie. a list of nodes where the identifiers are numbers, per Peter's schema). At this point, you could re-start Gephi and reload your women-organizations-2-mode.gephi file and re-run the multimode networks projection so that you are left with an organization to organization network. Do this, and save and export with appropriate file names.","title":"Transforming the network"},{"location":"supporting materials/multimode-networks.txt/#exploring-this-network","text":"Peter's data has a number of attributes describing it, including the membership year. So let's see what this network of women looks like in 1902. Under the filters tab at the right side of the Gephi interface, select \u2018attributes \u2013 equal\u2019 and then drag \u20181902\u2019 to the queries box. In \u2018pattern\u2019 enter [0-9] and tick the \u2018use regex\u2019 box. Click ok and then click \u2018filter\u2019. You should now have a network with 188 nodes and 8728 edges, showing the women who were active in 1902. Let\u2019s learn something about this network. Under the Statistics tab at the right side of the Gephi interface, do the following: Run \u2018avg. path length\u2019 by clicking on \u2018run\u2019. In the pop up that opens, select \u2018undirected\u2019 (as we know nothing about directionality in this network; we simply know that two women were members of the same organization at the same time. Note also that if the same pair were members of the more than one organziation, the weight of their connection will be corresponding stronger). Click ok. Run \u2018modularity\u2019 to look for subgroups. Make sure \u2018randomize\u2019 and \u2018use weights\u2019 are selected. Leave \u2018resolution\u2019 at 1.0 We selected 'average path length' because one of the byproducts of this routine is 'betweeness centrality'. We're making an assumption here that a woman who has a high betweeness centrality score was in a position to affect information flow in 1902 society. Modularity looks at similar patterns of connections to cluster women who have more-or-less similar connections into groups. Let\u2019s visualize what we\u2019ve just learned. On the \u2018partition\u2019 tab, over on the left side of the \u2018overview\u2019 screen, click on nodes, then click the green arrows beside \u2018choose a partition parameter\u2019. Click on \u2018choose a partition parameter\u2019. Scroll down to modularity class. The different groups will be listed, with their colours and their % composition of the network. Hit \u2018apply\u2019 to recolour your network graph. Let\u2019s resize the nodes to show off betweeness-centrality (to figure out which woman was in the greatest position to influence flows of information in this network.) Click \u2018ranking\u2019. (It's on the left side of the interface, beside 'partition' and just below 'overview'. Click \u2018nodes\u2019. Click the down arrow on \u2018choose a rank parameter\u2019. Select \u2018betweeness centrality\u2019. Click the red diamond. This will resize the nodes according to their \u2018betweeness centrality\u2019. Click \u2018apply\u2019. Down at the bottom of the middle panel, click the large black \u2018T\u2019 to display labels. Click the black letter \u2018A\u2019 and select \u2018node size\u2019. Mrs. Mary Elliot-Murray-Kynynmound and Mrs. John Henry Wilson should now dominate your network (if you go back to the original data zip, you'll be able to find Peter's key to figure out who's who). Who were these ladies? What organizations were they members of? Who were they connected to? To the archives! Congratulations! You\u2019ve imported historical network data into Gephi, transformed it, manipulated it, and run some analyses. Play with the settings on \u2018preview\u2019 in order to share your visualization as SVG, PDF, or PNG. Now go back to your original Gephi file, and recast it as organizations to organizations via shared members, to figure out which organizations were key in early 20th century Ontario\u2026 make appropriate notes in your open notebook.","title":"Exploring this network"},{"location":"supporting materials/ner/","text":"Using the Stanford NER to tag a corpus In our regular expressions example, we were able to extract some of the metadata from the document because it was more or less already formatted in such a way that we could write a pattern to find it. Sometimes however clear-cut patterns are not quite as easy to apply. For instance, what if we were interested in the place names that appear in the documents? What if we suspected that the focus of diplomatic activity shifted over time? This is where \u2018named entity recognition\u2019 can be useful. Named entity recognition covers a broad range of techniques, based on machine learning and statistical models of language to laboriously trained classifiers using dictionaries. One of the easiest to use out-of-the-box is the Stanford Named Entity Recognizer. In essence, we tell it \u2018here is a block of text \u2013 classify!\u2019 It will then process through the text, looking at the structure of your text and matching it against its statistical models of word use to identify person, organization, and locations. One can also expand that classification to extract time, money, percent, and date. Grab the Stanford NER Let us use the NER to extract person, organization, and locations. First, download the Stanford NER and extract it to your machine. Open the location where you extracted the files. On a Mac, double-click on the one called ner-gui.command . (Mac Users: there is also an excellent tutorial from Michelle Moravec you may wish to consult . On PC, double-click on ner-gui.bat . This opens up a new window (using Java) with \u2018Stanford Named Entity Recognizer\u2019 and also a terminal window. Don\u2019t touch the terminal window for now. ( PC users, hang on a moment \u2013 there is a bit more that you need to know before you can use this tool successfully. You will have to use the command line in order to get the output out). Running the NER via its GUI In the \u2018Stanford Named Entity Recognizer\u2019 window there is some default text. Click inside this window and delete the text. Click on \u2018File\u2019 and then \u2018Open,\u2019 and select your text for the diplomatic correspondence of the Republic of Texas (that you should still have from Module 3, Exercise 1 ). Since this text file contains a lot of extraneous information in it \u2013 information which we are not currently interested in, including the publishing information and the index table of letters \u2013 you should open the file in a text editor first and delete that information. We want just the letters for this exercise. Save with a new name and then open it using \u2018File open\u2019 in the Stanford NER. The file will open within the window. In the Stanford NER window, click on \u2018classifier\u2019 then \u2018load CRF from file\u2019. Navigate to where you unzipped the Stanford NER folder. Click on the \u2018classifier\u2019 folder. There are a number of files here; the ones that you are interested in end with .gz : english.all.3class.distsim.crf.ser.gz english.all.4class.distsim.crf.ser.gz eglish.muc.7class.distsim.crf.ser.gz These files correspond to these entities to extract: 3class: Location, Person, Organization 4class: Location, Person, Organization, Misc 7class: Time, Location, Organization, Person, Money, Percent, Date Select the location, person, and organization classifier (ie. 3class) Press \u2018Run NER.\u2019 At this point, the program will appear to \u2018hang\u2019 \u2013 nothing much will seem to be happening. However, in the background, the program has started to process your text. Depending on the size of your text, this could take anywhere from a few minutes to a few hours. Be patient! Watch the terminal window \u2013 once the program has results for you, these will start to scroll by in the terminal window. In the main program window, once the entire text has processed, the text will appear with colour-coded highlighting showing which words are location words, which ones are persons, which ones are organizations. You have now classified a text. NB Sometimes your computer may run out of memory. In that case, you\u2019ll see an error referring to \u201cOut of Heap Space\u201d in your terminal window. That\u2019s OK \u2013 just copy and paste a smaller bit of the document, say the first 10,000 lines or so. Then try again. Manipulating that data Mac users can grab the data and paste it elsewhere; PC users will have to run the NER from the command line to get usable output. Mac Users On a Mac, you can copy and paste the output from the terminal window into your text editor of choice. It will look something like the following: LOCATION: Texas PERSON: Moore ORGANIZATION: Suprema And so forth. Once you've pasted it into Textwrangler (or whatever text editor you use), you can now use regular expressions to manipulate the text further. More in a moment. PC Users On a PC, things are not so simple because the command window only shows a small fraction of the complete output \u2013 you cannot copy and paste it all! What we have to do instead is type in a command at the command prompt, rather than using the graphical interface, and then redirect the output into a new text file. Open a command prompt in the Stanford NER folder on your Windows machine (you can right-click on the folder in your windows explorer, and select \u2018open command prompt here\u2019). Type the following as a single line: java -mx500m -cp stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier classifiers/english.all.3class.distsim.crf.ser.gz -textFile texas-letters.txt -outputFormat inlineXML \u201cmy-ner-output.txt\u201d The first bit, java \u2013mx500m says how much memory to use. If you have 1gb of memory available, you can type java \u2013mx 1g (or 2g, or 3g, etc). The next part of the command calls the NER programme itself. You can set which classifier to use after the \u2013loadClassifier classifiers/ by typing in the exact file name for the classifier you wish to use (you are telling loadClassifier the exact path to the classifier). At \u2013textFile you give it the name of your input file (on our machine, called texas-letters.txt , and then specify the outputFormat . The character sends the output to a new text file, here called my-ner-output.txt . Hit enter, and a few moments later the programme will tell you something along the lines of the following: CRFCLassifier tagged 375281 words in 13745 documents at 10833.43 words per second Open the text file in Notepad++, and you\u2019ll see output like the following: In the name of the LOCATION Republic of Texas /LOCATION , Free, Sovereign and Independent. To all whom these Presents shall come or may in any wise concern. I PERSON Sam Houston /PERSON President thereof send Greeting Congratulations \u2013 you\u2019ve successfully tagged a document using a named entity recognizer! Now you need to do a bit more data-munging before you can do anything useful. Imagine you wanted to eventually visualize this as a network . You will need your regex skills again.","title":"Stanford Named Entity Recognizer"},{"location":"supporting materials/ner/#using-the-stanford-ner-to-tag-a-corpus","text":"In our regular expressions example, we were able to extract some of the metadata from the document because it was more or less already formatted in such a way that we could write a pattern to find it. Sometimes however clear-cut patterns are not quite as easy to apply. For instance, what if we were interested in the place names that appear in the documents? What if we suspected that the focus of diplomatic activity shifted over time? This is where \u2018named entity recognition\u2019 can be useful. Named entity recognition covers a broad range of techniques, based on machine learning and statistical models of language to laboriously trained classifiers using dictionaries. One of the easiest to use out-of-the-box is the Stanford Named Entity Recognizer. In essence, we tell it \u2018here is a block of text \u2013 classify!\u2019 It will then process through the text, looking at the structure of your text and matching it against its statistical models of word use to identify person, organization, and locations. One can also expand that classification to extract time, money, percent, and date.","title":"Using the Stanford NER to tag a corpus"},{"location":"supporting materials/ner/#grab-the-stanford-ner","text":"Let us use the NER to extract person, organization, and locations. First, download the Stanford NER and extract it to your machine. Open the location where you extracted the files. On a Mac, double-click on the one called ner-gui.command . (Mac Users: there is also an excellent tutorial from Michelle Moravec you may wish to consult . On PC, double-click on ner-gui.bat . This opens up a new window (using Java) with \u2018Stanford Named Entity Recognizer\u2019 and also a terminal window. Don\u2019t touch the terminal window for now. ( PC users, hang on a moment \u2013 there is a bit more that you need to know before you can use this tool successfully. You will have to use the command line in order to get the output out).","title":"Grab the Stanford NER"},{"location":"supporting materials/ner/#running-the-ner-via-its-gui","text":"In the \u2018Stanford Named Entity Recognizer\u2019 window there is some default text. Click inside this window and delete the text. Click on \u2018File\u2019 and then \u2018Open,\u2019 and select your text for the diplomatic correspondence of the Republic of Texas (that you should still have from Module 3, Exercise 1 ). Since this text file contains a lot of extraneous information in it \u2013 information which we are not currently interested in, including the publishing information and the index table of letters \u2013 you should open the file in a text editor first and delete that information. We want just the letters for this exercise. Save with a new name and then open it using \u2018File open\u2019 in the Stanford NER. The file will open within the window. In the Stanford NER window, click on \u2018classifier\u2019 then \u2018load CRF from file\u2019. Navigate to where you unzipped the Stanford NER folder. Click on the \u2018classifier\u2019 folder. There are a number of files here; the ones that you are interested in end with .gz : english.all.3class.distsim.crf.ser.gz english.all.4class.distsim.crf.ser.gz eglish.muc.7class.distsim.crf.ser.gz These files correspond to these entities to extract: 3class: Location, Person, Organization 4class: Location, Person, Organization, Misc 7class: Time, Location, Organization, Person, Money, Percent, Date Select the location, person, and organization classifier (ie. 3class) Press \u2018Run NER.\u2019 At this point, the program will appear to \u2018hang\u2019 \u2013 nothing much will seem to be happening. However, in the background, the program has started to process your text. Depending on the size of your text, this could take anywhere from a few minutes to a few hours. Be patient! Watch the terminal window \u2013 once the program has results for you, these will start to scroll by in the terminal window. In the main program window, once the entire text has processed, the text will appear with colour-coded highlighting showing which words are location words, which ones are persons, which ones are organizations. You have now classified a text. NB Sometimes your computer may run out of memory. In that case, you\u2019ll see an error referring to \u201cOut of Heap Space\u201d in your terminal window. That\u2019s OK \u2013 just copy and paste a smaller bit of the document, say the first 10,000 lines or so. Then try again.","title":"Running the NER via its GUI"},{"location":"supporting materials/ner/#manipulating-that-data","text":"Mac users can grab the data and paste it elsewhere; PC users will have to run the NER from the command line to get usable output.","title":"Manipulating that data"},{"location":"supporting materials/ner/#mac-users","text":"On a Mac, you can copy and paste the output from the terminal window into your text editor of choice. It will look something like the following: LOCATION: Texas PERSON: Moore ORGANIZATION: Suprema And so forth. Once you've pasted it into Textwrangler (or whatever text editor you use), you can now use regular expressions to manipulate the text further. More in a moment.","title":"Mac Users"},{"location":"supporting materials/ner/#pc-users","text":"On a PC, things are not so simple because the command window only shows a small fraction of the complete output \u2013 you cannot copy and paste it all! What we have to do instead is type in a command at the command prompt, rather than using the graphical interface, and then redirect the output into a new text file. Open a command prompt in the Stanford NER folder on your Windows machine (you can right-click on the folder in your windows explorer, and select \u2018open command prompt here\u2019). Type the following as a single line: java -mx500m -cp stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier classifiers/english.all.3class.distsim.crf.ser.gz -textFile texas-letters.txt -outputFormat inlineXML \u201cmy-ner-output.txt\u201d The first bit, java \u2013mx500m says how much memory to use. If you have 1gb of memory available, you can type java \u2013mx 1g (or 2g, or 3g, etc). The next part of the command calls the NER programme itself. You can set which classifier to use after the \u2013loadClassifier classifiers/ by typing in the exact file name for the classifier you wish to use (you are telling loadClassifier the exact path to the classifier). At \u2013textFile you give it the name of your input file (on our machine, called texas-letters.txt , and then specify the outputFormat . The character sends the output to a new text file, here called my-ner-output.txt . Hit enter, and a few moments later the programme will tell you something along the lines of the following: CRFCLassifier tagged 375281 words in 13745 documents at 10833.43 words per second Open the text file in Notepad++, and you\u2019ll see output like the following: In the name of the LOCATION Republic of Texas /LOCATION , Free, Sovereign and Independent. To all whom these Presents shall come or may in any wise concern. I PERSON Sam Houston /PERSON President thereof send Greeting Congratulations \u2013 you\u2019ve successfully tagged a document using a named entity recognizer! Now you need to do a bit more data-munging before you can do anything useful. Imagine you wanted to eventually visualize this as a network . You will need your regex skills again.","title":"PC Users"},{"location":"supporting materials/netviz/","text":"Using igraph to visualize network data In this exercise, we are doing a quick first pass on the network data generated from the Republic of Texas correspondence. I am providing you with the edge list , the links of to from that we generated earlier. I am also providing you the node list , the list of individuals extracted from that edge list. If we keep our nodes and edges separated in CSV tables, we can add other attributes later (things like date, or number of times a pair of individuals corresponded; ie. weight, or gender, or location, or what have you) to create different views or analyses. I used Open Refine to clean this data up for you (recall our lesson on Open Refine ). Remember, 80 percent of all digital history work involves cleaning data! Below is the data that you need; download both files and use the File Manager to load them into your DH Box into your new R project for this tutorial. List of links (this downloads a CSV file) List of nodes(this downloads a CSV file) This tutorial will illustrate to you some of the ways the igraph package can be used to create quick visualizations or to generate network statistics. These of course on their own mean very little: this is where your historical sensibility comes into play, triangulating this generated data with other things you know about the historical context of the time, place and players! Remember to make a new project in RStudio by clicking the down arrow on the R button at the right side of the RStudio interface; start the project in a new folder. Then using the DH Box File Manager, upload the nodes and links data from your computer to that folder you just created. Installing igraph Once that's accomplished, go back into RStudio and start a new script: # install igraph; this might take a long time # you only run this line the first time you install igraph: install.packages('igraph') # a lot of stuff gets downloaded and installed. # now tell RStudio you want to use the igraph pacakge and its functions: library('igraph') Getting the data into your project For future reference, we're adapting our script from a more in-depth tutorial of R igraph . Bringing data into R is straightforward. We create a variable for nodes and a variable for links and load the data from our CSV files into them like the following: # now let's load up the data by putting the csv files into nodes and links. nodes - read.csv(\"texasnodes.csv\", header=T, as.is=T) links - read.csv(\"texaslinks.csv\", header=T, as.is=T) We can examine the start or end of a variable with the head or tail command; these look at the first few lines at the start ( head ) or end ( tail ) of the variable, like the following: #examine data head(nodes) head(links) nrow(nodes); length(unique(nodes$id)) # which gives the number of nodes in our data nrow(links); nrow(unique(links[,c(\"source\", \"target\")])) # which gives the number of sources, and number of targets # which means some people sent more than one letter, and some people received more than one letter Let's rearrange things so that instead of the following: Alice - Bob, 1 letter Alice - Bob, 1 letter Alice - Bob, 1 letter we have the following: Alice - Bob, 3 letters That is, we're going to count up the number of times a particular relationship exists, and assign that count to the weight column. Much tidier all around! We do that like the following: links - aggregate(links[,3], links[,-3], sum) links - links[order(links$target, links$source),] colnames(links)[3] - \"weight\" rownames(links) - NULL head(links) You should see the following: source target weight 1 James Webb Abb6 Anduz6 1 2 A. de Saligny Abner S. Lipscomb 1 3 E. W. Moore Abner S. Lipscomb 1 4 James Hamilton Abner S. Lipscomb 14 5 James Treat Abner S. Lipscomb 11 6 Nathaniel Amory Abner S. Lipscomb 1 Now, let's tell R to stitch our links together into a network object (a data frame) that it can visualize and analyze, like the following: # let's make a net # notice that we are telling igraph that the network is directed, that the relationship Alice to Bob is different than Bob's to Alice (Alice is the _sender_, and Bob is the _receiver_) # In older DH Box version of igraph in RStudio: net - graph.data.frame(d=links, vertices=nodes, directed=T) # OR Newer version of igraph in desktop RStudio: net - graph_from_data_frame(d=links, vertices=nodes, directed=T) # type 'net' again and run the line to see how the network is represented. net # let's visualizae it plot(net, edge.arrow.size=.4,vertex.label=NA) # two quite distinct groupings, it would appear. Before we jump down the rabbit hole of visualization, let's recognize right now that visualizing a network is only rarely of analytical value. The value of network analysis comes from the various questions we can now start posing of our data when it is represented as a network. In this correspondence network, who is in the centre of the web? To whom would information flow? To whom would information leak? Are there cliques or ingroups? When we identify such individuals, how does that confirm or confound our expectations of the period and place? Many different kinds of metrics can be calculated (and the Kateto R igraph tutorial will show you how ) but it's always worth remembering that a metric is only meaningful for a given network when we're dealing with similar things a network of people who write letters to one another; a network of banks that swap mortgages with one another. These are called 'one mode networks'. A network of people connected to the banks they use a two mode network, because it connects two different kinds of things might be useful to visualize but the metrics calculated might not be valid if the metric was designed to work on a one-mode network (for more on this and allied issues, visit Scott Weingart's Demystifying Networks ). Given our correspondence network, let's imagine that 'closeness' (a measure of how central a person is) and 'betweenness' (a measure of how many different strands of the network pass through this person) are the most historically interesting. Further, we're going to try to determine if there are subgroups in our network, cliques. ## the 'degree' of a node is the count of its connections. In this code chunk, we calculate degree, then make both a histogram of the counts and a plot of the network where we size the nodes proportionately to their degree. What do we learn from these two visualizations? deg - degree(net, mode=\"all\") hist(deg, breaks=1:vcount(net)-1, main=\"Histogram of node degree\") plot(net, vertex.size=deg*2, vertex.label = NA) ## write this info to file for safekeeping write.csv(deg, 'degree.csv') Now we'll look at closeness. If you know the width or diameter of your network (the maximum number of steps to get across it), then the node that is on average the shortest number of steps from all the others is the one that is closest. We calculate it like the following: closepeople - closeness(net, mode=\"all\", weights=NA) sort(closepeople, decreasing = T) # so that we see who is most close first write.csv(closepeople, 'closeness.csv') # so we have it on file. We can ask which individuals are hubs, and which are authorities. In the lingo, 'hubs' are individuals with many outgoing links (they sent lots of letters) while 'authorities' are individuals who received lots of letters. In the code below, can you work out what command to give to write the hub score or the authority scores to a file? # In older DH Box version of igraph in RStudio: hs - hub.score(net, weights=NA)$vector as - authority.score(net, weights=NA)$vector # OR Newer version of igraph in desktop RStudio: hs - hub_score(net, weights=NA)$vector as - authority_score(net, weights=NA)$vector par(mfrow=c(1,2)) # vertex.label.cex sets the size of the label; play with the sizes until you see something appealing. plot(net, vertex.size=hs*40, vertex.label.cex =.2, edge.arrow.size=.1, main=\"Hubs\") plot(net, vertex.size=as*20, vertex.label = NA, edge.arrow.size=.1, main=\"Authorities\") Let's look for 'modules' within this network. Broadly speaking, these are clumps of nodes that have more or less the same pattern of ties between them, within the group, than without. # In older DH Box version of igraph in RStudio: cfg - fastgreedy.community(as.undirected(net)) # OR Newer version of igraph in desktop RStudio: cfg - cluster_fast_greedy(as.undirected(net)) lapply(cfg, function(x) write.table( data.frame(x), 'cfg.csv' , append= T, sep=',' )) We create a new variable called cfg and get the cluster_fast_greedy algorithm to perform its calculations. The next line writes the groups to a file, separating each group with an x . (If you tried write.csv as before, you'll get an error message because the output of the algorithm gives a different kind of data type. R is fussy like this.) Examine that file what groups do you spot? What might these mean, if you went back to the content of the original letters? Visualization The line below will plot out our network, colouring it by the communities discerned above, like the following: plot(cfg, net, vertex.size = 1, vertex.label.cex =.2, edge.arrow.size=.1, main=\"Communities\") You can export the plot by clicking on the 'Export' button in the plot panel, to PDF or to PNG. But this is a pretty ugly network. We need to apply a layout to try to make it more visually understandable. There are many different layout options in igraph. We'll assign the layout we want to a variable, and then we'll give that variable to the plot command like the following: # In older DH Box version of igraph in RStudio: l1 - layout.fruchterman.reingold(net) # OR Newer version of igraph in desktop RStudio: l1 - layout_with_fr(net) plot(cfg, net, layout=l1, vertex.size = 1, vertex.label.cex =.2, edge.arrow.size=.1, main=\"Communities\") The layout_with_fr is calling the 'Fruchterman-Reingold' algorithm, a kind of layout that imagines each node as a repulsive power, pushing against all the other nodes (it's part of a family of layouts called 'Forced Atlas'). That also means that if you ran the plot command a couple of times, the nodes might end up in different places each time they are jostling for relative position, and this positioning itself carries no meaning in and of itself (so if a node is at the top of the screen, or the centre of the screen, don't read that to mean anything about importance). Good luck! Remember to save your script, and upload the entire project folder to your GitHub repository.","title":"Network visualization with igraph"},{"location":"supporting materials/netviz/#using-igraph-to-visualize-network-data","text":"In this exercise, we are doing a quick first pass on the network data generated from the Republic of Texas correspondence. I am providing you with the edge list , the links of to from that we generated earlier. I am also providing you the node list , the list of individuals extracted from that edge list. If we keep our nodes and edges separated in CSV tables, we can add other attributes later (things like date, or number of times a pair of individuals corresponded; ie. weight, or gender, or location, or what have you) to create different views or analyses. I used Open Refine to clean this data up for you (recall our lesson on Open Refine ). Remember, 80 percent of all digital history work involves cleaning data! Below is the data that you need; download both files and use the File Manager to load them into your DH Box into your new R project for this tutorial. List of links (this downloads a CSV file) List of nodes(this downloads a CSV file) This tutorial will illustrate to you some of the ways the igraph package can be used to create quick visualizations or to generate network statistics. These of course on their own mean very little: this is where your historical sensibility comes into play, triangulating this generated data with other things you know about the historical context of the time, place and players! Remember to make a new project in RStudio by clicking the down arrow on the R button at the right side of the RStudio interface; start the project in a new folder. Then using the DH Box File Manager, upload the nodes and links data from your computer to that folder you just created.","title":"Using igraph to visualize network data"},{"location":"supporting materials/netviz/#installing-igraph","text":"Once that's accomplished, go back into RStudio and start a new script: # install igraph; this might take a long time # you only run this line the first time you install igraph: install.packages('igraph') # a lot of stuff gets downloaded and installed. # now tell RStudio you want to use the igraph pacakge and its functions: library('igraph')","title":"Installing igraph"},{"location":"supporting materials/netviz/#getting-the-data-into-your-project","text":"For future reference, we're adapting our script from a more in-depth tutorial of R igraph . Bringing data into R is straightforward. We create a variable for nodes and a variable for links and load the data from our CSV files into them like the following: # now let's load up the data by putting the csv files into nodes and links. nodes - read.csv(\"texasnodes.csv\", header=T, as.is=T) links - read.csv(\"texaslinks.csv\", header=T, as.is=T) We can examine the start or end of a variable with the head or tail command; these look at the first few lines at the start ( head ) or end ( tail ) of the variable, like the following: #examine data head(nodes) head(links) nrow(nodes); length(unique(nodes$id)) # which gives the number of nodes in our data nrow(links); nrow(unique(links[,c(\"source\", \"target\")])) # which gives the number of sources, and number of targets # which means some people sent more than one letter, and some people received more than one letter Let's rearrange things so that instead of the following: Alice - Bob, 1 letter Alice - Bob, 1 letter Alice - Bob, 1 letter we have the following: Alice - Bob, 3 letters That is, we're going to count up the number of times a particular relationship exists, and assign that count to the weight column. Much tidier all around! We do that like the following: links - aggregate(links[,3], links[,-3], sum) links - links[order(links$target, links$source),] colnames(links)[3] - \"weight\" rownames(links) - NULL head(links) You should see the following: source target weight 1 James Webb Abb6 Anduz6 1 2 A. de Saligny Abner S. Lipscomb 1 3 E. W. Moore Abner S. Lipscomb 1 4 James Hamilton Abner S. Lipscomb 14 5 James Treat Abner S. Lipscomb 11 6 Nathaniel Amory Abner S. Lipscomb 1 Now, let's tell R to stitch our links together into a network object (a data frame) that it can visualize and analyze, like the following: # let's make a net # notice that we are telling igraph that the network is directed, that the relationship Alice to Bob is different than Bob's to Alice (Alice is the _sender_, and Bob is the _receiver_) # In older DH Box version of igraph in RStudio: net - graph.data.frame(d=links, vertices=nodes, directed=T) # OR Newer version of igraph in desktop RStudio: net - graph_from_data_frame(d=links, vertices=nodes, directed=T) # type 'net' again and run the line to see how the network is represented. net # let's visualizae it plot(net, edge.arrow.size=.4,vertex.label=NA) # two quite distinct groupings, it would appear. Before we jump down the rabbit hole of visualization, let's recognize right now that visualizing a network is only rarely of analytical value. The value of network analysis comes from the various questions we can now start posing of our data when it is represented as a network. In this correspondence network, who is in the centre of the web? To whom would information flow? To whom would information leak? Are there cliques or ingroups? When we identify such individuals, how does that confirm or confound our expectations of the period and place? Many different kinds of metrics can be calculated (and the Kateto R igraph tutorial will show you how ) but it's always worth remembering that a metric is only meaningful for a given network when we're dealing with similar things a network of people who write letters to one another; a network of banks that swap mortgages with one another. These are called 'one mode networks'. A network of people connected to the banks they use a two mode network, because it connects two different kinds of things might be useful to visualize but the metrics calculated might not be valid if the metric was designed to work on a one-mode network (for more on this and allied issues, visit Scott Weingart's Demystifying Networks ). Given our correspondence network, let's imagine that 'closeness' (a measure of how central a person is) and 'betweenness' (a measure of how many different strands of the network pass through this person) are the most historically interesting. Further, we're going to try to determine if there are subgroups in our network, cliques. ## the 'degree' of a node is the count of its connections. In this code chunk, we calculate degree, then make both a histogram of the counts and a plot of the network where we size the nodes proportionately to their degree. What do we learn from these two visualizations? deg - degree(net, mode=\"all\") hist(deg, breaks=1:vcount(net)-1, main=\"Histogram of node degree\") plot(net, vertex.size=deg*2, vertex.label = NA) ## write this info to file for safekeeping write.csv(deg, 'degree.csv') Now we'll look at closeness. If you know the width or diameter of your network (the maximum number of steps to get across it), then the node that is on average the shortest number of steps from all the others is the one that is closest. We calculate it like the following: closepeople - closeness(net, mode=\"all\", weights=NA) sort(closepeople, decreasing = T) # so that we see who is most close first write.csv(closepeople, 'closeness.csv') # so we have it on file. We can ask which individuals are hubs, and which are authorities. In the lingo, 'hubs' are individuals with many outgoing links (they sent lots of letters) while 'authorities' are individuals who received lots of letters. In the code below, can you work out what command to give to write the hub score or the authority scores to a file? # In older DH Box version of igraph in RStudio: hs - hub.score(net, weights=NA)$vector as - authority.score(net, weights=NA)$vector # OR Newer version of igraph in desktop RStudio: hs - hub_score(net, weights=NA)$vector as - authority_score(net, weights=NA)$vector par(mfrow=c(1,2)) # vertex.label.cex sets the size of the label; play with the sizes until you see something appealing. plot(net, vertex.size=hs*40, vertex.label.cex =.2, edge.arrow.size=.1, main=\"Hubs\") plot(net, vertex.size=as*20, vertex.label = NA, edge.arrow.size=.1, main=\"Authorities\") Let's look for 'modules' within this network. Broadly speaking, these are clumps of nodes that have more or less the same pattern of ties between them, within the group, than without. # In older DH Box version of igraph in RStudio: cfg - fastgreedy.community(as.undirected(net)) # OR Newer version of igraph in desktop RStudio: cfg - cluster_fast_greedy(as.undirected(net)) lapply(cfg, function(x) write.table( data.frame(x), 'cfg.csv' , append= T, sep=',' )) We create a new variable called cfg and get the cluster_fast_greedy algorithm to perform its calculations. The next line writes the groups to a file, separating each group with an x . (If you tried write.csv as before, you'll get an error message because the output of the algorithm gives a different kind of data type. R is fussy like this.) Examine that file what groups do you spot? What might these mean, if you went back to the content of the original letters?","title":"Getting the data into your project"},{"location":"supporting materials/netviz/#visualization","text":"The line below will plot out our network, colouring it by the communities discerned above, like the following: plot(cfg, net, vertex.size = 1, vertex.label.cex =.2, edge.arrow.size=.1, main=\"Communities\") You can export the plot by clicking on the 'Export' button in the plot panel, to PDF or to PNG. But this is a pretty ugly network. We need to apply a layout to try to make it more visually understandable. There are many different layout options in igraph. We'll assign the layout we want to a variable, and then we'll give that variable to the plot command like the following: # In older DH Box version of igraph in RStudio: l1 - layout.fruchterman.reingold(net) # OR Newer version of igraph in desktop RStudio: l1 - layout_with_fr(net) plot(cfg, net, layout=l1, vertex.size = 1, vertex.label.cex =.2, edge.arrow.size=.1, main=\"Communities\") The layout_with_fr is calling the 'Fruchterman-Reingold' algorithm, a kind of layout that imagines each node as a repulsive power, pushing against all the other nodes (it's part of a family of layouts called 'Forced Atlas'). That also means that if you ran the plot command a couple of times, the nodes might end up in different places each time they are jostling for relative position, and this positioning itself carries no meaning in and of itself (so if a node is at the top of the screen, or the centre of the screen, don't read that to mean anything about importance). Good luck! Remember to save your script, and upload the entire project folder to your GitHub repository.","title":"Visualization"},{"location":"supporting materials/open-refine/","text":"An Introduction to Open Refine This text was adopted from the first drafts of The Macroscope An alternative Open Refine exercise is offered by Thomas Padilla and you may wish to give it a try instead. That would be acceptable. Install Open Refine Open Refine (formerly Google Refine) is a powerful tool for working with messy data: cleaning it; transforming it from one format into another; extending it with web services; and linking it to databases like Freebase (deprecated as of August 2016). This exercise does not use DH Box. In this exercise, we are going to use the Open Refne tool that originated with Google. Since 2012, it has been open source and freely available online. Using it takes a bit of getting used to, however. Start by doing the following: Visit the Open Refine home page and watch the three videos. Download Open Refine to your machine . Follow the installation instructions. Start Open Refine by double clicking on its icon. This will open a new browser window, pointing to http://127.0.0.1:3333 . This location is your own computer, so even though it looks like it\u2019s running on the internet, it isn\u2019t. The 3333 is a \u2018port\u2019, meaning that Open Refine is running much like a server, serving up a webpage via that port to the browser. (If the browser window doesn't open automatically, open one and put http://127.0.0.1:3333 in the address bar.) Start Cleaning our Texan Correspondence Make sure you have your data handy that you created in Module 3, Exercise 1 , the Texan correspondence. You can get it out of your DH Box by using the DH Box File Manager. Navigate to where you were working on it, then click on the file name. This will download it to your downloads folder. Move your working file to somewhere safe on your computer. Start a new project by clicking on the \u2018Create project\u2019 tab on the left side of the screen. Click on \u2018Choose files\u2019 and select the Texan correspondence CSV file. Open Refine will load this data and it will give you a preview of your data. Name your project in the box on the top right side (eg. 'm3-exercise2' or similar) and then click \u2018Create project\u2019. It may take a few minutes. Once your project has started, one of the columns that should be visible in your data is \"Sender\". Click on the arrow to the left of \"Sender\" in OpenRefine and select Facet - Text Facet. Do the same with the arrow next to \"Recipient\". A box will appear on the left side of the browser showing all 189 names listed as senders in the spreadsheet. The spreadsheet itself is nearly a thousand rows, so immediately we see that, in this correspondence collection, some names are used multiple times. You may also have noticed that many of the names suffered from errors in the text scan (OCR or Optical Character Recognition errors), rendering some identical names from the book as similar, but not the same, in the spreadsheet. For example the recipient \"Juan de Dios Cafiedo\" is occasionally listed as \"Juan de Dios CaAedo\". Any subsequent analysis will need these errors to be cleared up, and OpenRefine will help fix them. Within the \"Sender\" facet box on the left side, click on the button labeled \"Cluster\". This feature presents various automatic ways of merging values that appear to be the same. Play with the values in the drop-down boxes and notice how the number of clusters found change depending on which method is used. Because the methods are slightly different, each will present different matches that may or may not be useful. If you see two values which should be merged, e.g. \"Ashbel Smith\" and \". Ashbel Smith\", check the box to the right in the 'Merge' column and click the 'Merge Selected Re-Cluster' button below. Go through the various cluster methods one-at-a-time, including changing number values, and merge the values which appear to be the same. \"Juan de Dios CaAedo\" clearly should be merged with \"Juan de Dios Cafiedo\", however \"Correspondent in Tampico\" probably should not be merged with \"Correspondent at Vera Cruz.\" Since we are not experts, we will have to use our best judgement in these cases or get cracking on some more research to help us make the call. By the end, you should have reduced the number of unique Senders from 189 to around 150. Repeat these steps with Recipients, reducing unique Recipients from 192 to about 160. To finish the automatic cleaning of the data, click the arrow next to \"Sender\" and select 'Edit Cells - Common transforms - Trim leading and trailing whitespace'. Repeat step 12 for \"Recipient\". The resulting spreadsheet will not be perfect, but it will be much easier to clean by hand than it would have been before taking this step. Click on \u2018Export\u2019 at the top right of the window to get your data back out as a .csv file. Now what? The text you've just cleaned could now be loaded into something like Palladio or Gephi or Connect the Dots for network analysis! However, every network analysis program has its own idiosyncracies. Gephi and Connect the Dots, for instance, can import lists of relationships if the CSV file has columns labelled 'source' and 'target'. (Connect the Dots will only accept those two columns, so you'd have to delete the date column if you wanted to give that a try.) Let's assume that's where we want to visualize and analyze this data. In order to get this correspondence data into a network visualization tool, we will have to rename the \"Sender\" column to \"source\" and the \"Recipient\" column to \"target\". You could do this in a spreadsheet, of course. But since you have Open Refine running, do the following: In the arrow to the left of Sender in the main OpenRefine window, select Edit column - Rename this column, and rename the column \"source\". In the arrow to the left of Recipient in the main OpenRefine window, select Edit column - Rename this column, and rename the column \"target\". In the top right of the window, select 'Export - Custom tabular exporter'. Notice that \"source\", \"target\", and \"Date\" are checked in the content tab; uncheck \"Date\", as it will not be used in Gephi (networks where the nodes have different dates, ie. dynamic networks, are beyond us for the moment). Go to the download tab and change the download option from 'Tab-separated values (TSV)' to 'Comma-separated values (CSV)' and press download. The file will likely download to your automatic download directory. We will revisit this file later. Go ahead and drop this file into the Palladio interface. Do you see any interesting patterns? Make a note! Upload your cleaned file with a new name back into your DH Box; we will use this in the next module. Remember to copy your notes and any other information/observations/thoughts to your GitHub repo Optional: Going further with Open Refine Named Entity Extraction Say we wanted, instead of the correspondence network, a visualization of all the places named in this body of letters. It might be interesting to visualize on a map the changing focus of Texas' diplomatic attention over time. There is a plugin for Open Refine that does what is called Named Entity Extraction . The plugin, and how to install and use it, is available on the Free Your Metadata website . Use regex on your original document containing the letters to clean up the data so that you have one letter per line (rather than the index did you notice that the full text of all the letters was in the original file?). Import the file into Open Refine. Extract Named Entities. Visualize the results in a spreadsheet. Write up a 'how to' in your notebook explaining these steps in detail. An interesting use case is discussed online and on the Free Your Metadata website (PDF downloads in new tab) . Further Help: Visit Kalani Craig's page on maintaining digital data . Optional: Exploring other Named Entity Extraction tools Voyant Tools RezoViz Voyant-Tools is a text analysis suite that we will explore in more depth in the Module 4 . Feel free to load your material into it and begin exploring; there's nothing you can break. One interesting tool in Voyant is called RezoViz , which will extract entities and tie them together into a network based on appearing in the same document. Upload some of your Canadian war diary texts to Voyant-Tools. In the top right, there's a 'save' icon. Select 'url for a different tool/skin'. Select 'RezoViz' from the tools list that pops up. A new URL will appear in the box. Copy and paste the URL into a new browser window (works best on Chrome). What kinds of questions could this answer? Stanford NER Download Stanford NER . Mac instructions for Stanford NER (this link is to Michelle Moravec's instructions, for Mac). Windows: If you're on windows and want to do this, things are a bit more complicated. Download and unzip the NER package: Open a command prompt in the Stanford NER folder on your Windows machine (you can right-click on the folder in your windows explorer, and select \u2018Open command prompt here\u2019). Changing the file names as appropriate, type the following as a single line (highlight the text with your mouse it scrolls to the right beyond the page, and then copy it): java -mx500m -cp stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier classifiers/english.all.3class.distsim.crf.ser.gz -textFile texas-letters.txt -outputFormat inlineXML \u201cmy-ner-output.txt\u201d The first bit, java \u2013mx500m says how much memory to use. If you have 1gb of memory available, you can type java \u2013mx 1g (or 2g, or 3g, etc). The next part of the command calls the NER programme itself. You can set which classifier to use after the \u2013loadClassifier classifiers/ by typing in the exact file name for the classifier you wish to use (you are telling loadClassifier the exact path to the classifier). At \u2013textFile you give it the name of your input file (on our machine, called texas-letters.txt , and then specify the outputFormat . The character sends the output to a new text file, here called my-ner-output.txt . Hit enter, and a few moments later the programme will tell you something along the lines of the following: CRFCLassifier tagged 375281 words in 13745 documents at 10833.43 words per second Open the text file in your text editor, and you\u2019ll see an output like the following: In the name of the LOCATION Republic of Texas /LOCATION , Free, Sovereign and Independent. To all whom these Presents shall come or may in any wise concern. I PERSON Sam Houston /PERSON President thereof send Greeting Congratulations! You've tagged a body of letters. What next? You could organize this into XML, you could visualize, you could regex to find and extract all of your locations, or persons, or... there are many possibilities.","title":"Open Refine"},{"location":"supporting materials/open-refine/#an-introduction-to-open-refine","text":"This text was adopted from the first drafts of The Macroscope An alternative Open Refine exercise is offered by Thomas Padilla and you may wish to give it a try instead. That would be acceptable.","title":"An Introduction to Open Refine"},{"location":"supporting materials/open-refine/#install-open-refine","text":"Open Refine (formerly Google Refine) is a powerful tool for working with messy data: cleaning it; transforming it from one format into another; extending it with web services; and linking it to databases like Freebase (deprecated as of August 2016). This exercise does not use DH Box. In this exercise, we are going to use the Open Refne tool that originated with Google. Since 2012, it has been open source and freely available online. Using it takes a bit of getting used to, however. Start by doing the following: Visit the Open Refine home page and watch the three videos. Download Open Refine to your machine . Follow the installation instructions. Start Open Refine by double clicking on its icon. This will open a new browser window, pointing to http://127.0.0.1:3333 . This location is your own computer, so even though it looks like it\u2019s running on the internet, it isn\u2019t. The 3333 is a \u2018port\u2019, meaning that Open Refine is running much like a server, serving up a webpage via that port to the browser. (If the browser window doesn't open automatically, open one and put http://127.0.0.1:3333 in the address bar.)","title":"Install Open Refine"},{"location":"supporting materials/open-refine/#start-cleaning-our-texan-correspondence","text":"Make sure you have your data handy that you created in Module 3, Exercise 1 , the Texan correspondence. You can get it out of your DH Box by using the DH Box File Manager. Navigate to where you were working on it, then click on the file name. This will download it to your downloads folder. Move your working file to somewhere safe on your computer. Start a new project by clicking on the \u2018Create project\u2019 tab on the left side of the screen. Click on \u2018Choose files\u2019 and select the Texan correspondence CSV file. Open Refine will load this data and it will give you a preview of your data. Name your project in the box on the top right side (eg. 'm3-exercise2' or similar) and then click \u2018Create project\u2019. It may take a few minutes. Once your project has started, one of the columns that should be visible in your data is \"Sender\". Click on the arrow to the left of \"Sender\" in OpenRefine and select Facet - Text Facet. Do the same with the arrow next to \"Recipient\". A box will appear on the left side of the browser showing all 189 names listed as senders in the spreadsheet. The spreadsheet itself is nearly a thousand rows, so immediately we see that, in this correspondence collection, some names are used multiple times. You may also have noticed that many of the names suffered from errors in the text scan (OCR or Optical Character Recognition errors), rendering some identical names from the book as similar, but not the same, in the spreadsheet. For example the recipient \"Juan de Dios Cafiedo\" is occasionally listed as \"Juan de Dios CaAedo\". Any subsequent analysis will need these errors to be cleared up, and OpenRefine will help fix them. Within the \"Sender\" facet box on the left side, click on the button labeled \"Cluster\". This feature presents various automatic ways of merging values that appear to be the same. Play with the values in the drop-down boxes and notice how the number of clusters found change depending on which method is used. Because the methods are slightly different, each will present different matches that may or may not be useful. If you see two values which should be merged, e.g. \"Ashbel Smith\" and \". Ashbel Smith\", check the box to the right in the 'Merge' column and click the 'Merge Selected Re-Cluster' button below. Go through the various cluster methods one-at-a-time, including changing number values, and merge the values which appear to be the same. \"Juan de Dios CaAedo\" clearly should be merged with \"Juan de Dios Cafiedo\", however \"Correspondent in Tampico\" probably should not be merged with \"Correspondent at Vera Cruz.\" Since we are not experts, we will have to use our best judgement in these cases or get cracking on some more research to help us make the call. By the end, you should have reduced the number of unique Senders from 189 to around 150. Repeat these steps with Recipients, reducing unique Recipients from 192 to about 160. To finish the automatic cleaning of the data, click the arrow next to \"Sender\" and select 'Edit Cells - Common transforms - Trim leading and trailing whitespace'. Repeat step 12 for \"Recipient\". The resulting spreadsheet will not be perfect, but it will be much easier to clean by hand than it would have been before taking this step. Click on \u2018Export\u2019 at the top right of the window to get your data back out as a .csv file.","title":"Start Cleaning our Texan Correspondence"},{"location":"supporting materials/open-refine/#now-what","text":"The text you've just cleaned could now be loaded into something like Palladio or Gephi or Connect the Dots for network analysis! However, every network analysis program has its own idiosyncracies. Gephi and Connect the Dots, for instance, can import lists of relationships if the CSV file has columns labelled 'source' and 'target'. (Connect the Dots will only accept those two columns, so you'd have to delete the date column if you wanted to give that a try.) Let's assume that's where we want to visualize and analyze this data. In order to get this correspondence data into a network visualization tool, we will have to rename the \"Sender\" column to \"source\" and the \"Recipient\" column to \"target\". You could do this in a spreadsheet, of course. But since you have Open Refine running, do the following: In the arrow to the left of Sender in the main OpenRefine window, select Edit column - Rename this column, and rename the column \"source\". In the arrow to the left of Recipient in the main OpenRefine window, select Edit column - Rename this column, and rename the column \"target\". In the top right of the window, select 'Export - Custom tabular exporter'. Notice that \"source\", \"target\", and \"Date\" are checked in the content tab; uncheck \"Date\", as it will not be used in Gephi (networks where the nodes have different dates, ie. dynamic networks, are beyond us for the moment). Go to the download tab and change the download option from 'Tab-separated values (TSV)' to 'Comma-separated values (CSV)' and press download. The file will likely download to your automatic download directory. We will revisit this file later. Go ahead and drop this file into the Palladio interface. Do you see any interesting patterns? Make a note! Upload your cleaned file with a new name back into your DH Box; we will use this in the next module. Remember to copy your notes and any other information/observations/thoughts to your GitHub repo","title":"Now what?"},{"location":"supporting materials/open-refine/#optional-going-further-with-open-refine-named-entity-extraction","text":"Say we wanted, instead of the correspondence network, a visualization of all the places named in this body of letters. It might be interesting to visualize on a map the changing focus of Texas' diplomatic attention over time. There is a plugin for Open Refine that does what is called Named Entity Extraction . The plugin, and how to install and use it, is available on the Free Your Metadata website . Use regex on your original document containing the letters to clean up the data so that you have one letter per line (rather than the index did you notice that the full text of all the letters was in the original file?). Import the file into Open Refine. Extract Named Entities. Visualize the results in a spreadsheet. Write up a 'how to' in your notebook explaining these steps in detail. An interesting use case is discussed online and on the Free Your Metadata website (PDF downloads in new tab) . Further Help: Visit Kalani Craig's page on maintaining digital data .","title":"Optional: Going further with Open Refine &mdash; Named Entity Extraction"},{"location":"supporting materials/open-refine/#optional-exploring-other-named-entity-extraction-tools","text":"","title":"Optional: Exploring other Named Entity Extraction tools"},{"location":"supporting materials/open-refine/#voyant-tools-rezoviz","text":"Voyant-Tools is a text analysis suite that we will explore in more depth in the Module 4 . Feel free to load your material into it and begin exploring; there's nothing you can break. One interesting tool in Voyant is called RezoViz , which will extract entities and tie them together into a network based on appearing in the same document. Upload some of your Canadian war diary texts to Voyant-Tools. In the top right, there's a 'save' icon. Select 'url for a different tool/skin'. Select 'RezoViz' from the tools list that pops up. A new URL will appear in the box. Copy and paste the URL into a new browser window (works best on Chrome). What kinds of questions could this answer?","title":"Voyant Tools RezoViz"},{"location":"supporting materials/open-refine/#stanford-ner","text":"Download Stanford NER . Mac instructions for Stanford NER (this link is to Michelle Moravec's instructions, for Mac). Windows: If you're on windows and want to do this, things are a bit more complicated. Download and unzip the NER package: Open a command prompt in the Stanford NER folder on your Windows machine (you can right-click on the folder in your windows explorer, and select \u2018Open command prompt here\u2019). Changing the file names as appropriate, type the following as a single line (highlight the text with your mouse it scrolls to the right beyond the page, and then copy it): java -mx500m -cp stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier classifiers/english.all.3class.distsim.crf.ser.gz -textFile texas-letters.txt -outputFormat inlineXML \u201cmy-ner-output.txt\u201d The first bit, java \u2013mx500m says how much memory to use. If you have 1gb of memory available, you can type java \u2013mx 1g (or 2g, or 3g, etc). The next part of the command calls the NER programme itself. You can set which classifier to use after the \u2013loadClassifier classifiers/ by typing in the exact file name for the classifier you wish to use (you are telling loadClassifier the exact path to the classifier). At \u2013textFile you give it the name of your input file (on our machine, called texas-letters.txt , and then specify the outputFormat . The character sends the output to a new text file, here called my-ner-output.txt . Hit enter, and a few moments later the programme will tell you something along the lines of the following: CRFCLassifier tagged 375281 words in 13745 documents at 10833.43 words per second Open the text file in your text editor, and you\u2019ll see an output like the following: In the name of the LOCATION Republic of Texas /LOCATION , Free, Sovereign and Independent. To all whom these Presents shall come or may in any wise concern. I PERSON Sam Houston /PERSON President thereof send Greeting Congratulations! You've tagged a body of letters. What next? You could organize this into XML, you could visualize, you could regex to find and extract all of your locations, or persons, or... there are many possibilities.","title":"Stanford NER"},{"location":"supporting materials/quick-intro-r/","text":"A quick introduction to R and RStudio R is a powerful language for statistical exploring, visualizing, and manipulating all kinds of data, including textual. It is, however, not the most intutive of environments to work in. In which case, RStudio is what we need. Fortunately, you already have this installed for you in your DH Box; however, if you want to have it on your own computer, download R from RStudio and then the actual RStudio environment . In DH Box, you sign into RStudio with your DH Box credentials. Why do we bother with this? Why not just use Excel? One word: reproducibility. In Excel, the sequence of clicking that one does to do anything is next to impossible to effectively communicate to someone else. Excel also was built for business applications, and those biases are built into its DNA ( which can have some nasty effects .) R allows us to do scripted analyses. We write out the sequence of transformations or analyses to be done, making a kind of program that we can then feed our data into. Once we have a workflow that does what we need it to do, it becomes trivial to re-use the code on other datasets. What's more, when we do an analysis, we can publish the scripts and the data. We don't have to taken an author's word for it: we can re-run the analysis ourselves or build upon it. This course only touches in the lightest manner on the potential for R for doing digital history. Please see Lincoln Mullen's Computational Historical Thinking with Applications in R for more instruction. For now, we'll do a quick whistle-stop tour of using RStudio to do some analysis in R. You should always keep your research materials (scripts and associated data) organized in separate project folders. RStudio makes this easy for you. When you open RStudio to start a new project, click where it says 'Project (None)' at the right side of the interface. Click on the down arrow, and select New R project. Follow the prompts, and create it in a new directory. R keeps track of what you're up to in a project file, so that you can pick up where you left off. You should also keep your code under version control as well so that you can recover from disaster and/or share your code/data with collaborators; click on that link to get version control set up. Sometimes, it might happen that RStudio crashes (this can be, in DH Box, related to memory issues). If that happens if it just seems to 'hang' (and you've waited several minutes), you can refresh your browser, and go back to DH Box. You'll probably have to re-open your project file. Use the file panel at bottom right to find your \\*.rproj file (your R project file) and click on it to re-open. Save your work often, from the File Save menu. RStudio divides the screen up into four neat panes. The top left is for writing your analytical script (or code; I will use the two words interchangeably). The bottom left is the console where the analysis actually happens. The top right is an environment pane which will show you the variables you've created, your history (commands you've run), and (once it's configured) git; indeed, other tools and plugins will appear as tabs here. The bottom right gives you a preview of any plots or charts you create; once you create one if you click 'zoom' the chart will open in its own pop up so that you can see it better. The file explorer and the help text also appear under tabs in this box. You write your script in the script box, and then you can get RStudio to run the code one line at a time by clicking on Code Run Line (it runs the line of code where you left the cursor). If you select a number of lines of code and hit run line, all of that code will run. R scripts are text files that use .r as their file extension. The console is where the action happens. Click down in the console. Type 3 + 5 and hit enter. RStudio will run the calculation: [1] 8 ! The [1] indicates that this is the first result. Now type a = 3 and hit enter. Over in the top right, the 'Environment' pane updates to tell us that yes, a has a value of 3. Now type b = 5 . The Environment updates accordingly. Now type a + b . Hey, we can do algebra! Anything you can do in Excel, we can do in code here in RStudio. A very good introduction to how R (the language) works is at Try R on codeschool , an interactive tutorial in your browser. Go give some of that a shot.","title":"Quick intro to R"},{"location":"supporting materials/quick-intro-r/#a-quick-introduction-to-r-and-rstudio","text":"R is a powerful language for statistical exploring, visualizing, and manipulating all kinds of data, including textual. It is, however, not the most intutive of environments to work in. In which case, RStudio is what we need. Fortunately, you already have this installed for you in your DH Box; however, if you want to have it on your own computer, download R from RStudio and then the actual RStudio environment . In DH Box, you sign into RStudio with your DH Box credentials. Why do we bother with this? Why not just use Excel? One word: reproducibility. In Excel, the sequence of clicking that one does to do anything is next to impossible to effectively communicate to someone else. Excel also was built for business applications, and those biases are built into its DNA ( which can have some nasty effects .) R allows us to do scripted analyses. We write out the sequence of transformations or analyses to be done, making a kind of program that we can then feed our data into. Once we have a workflow that does what we need it to do, it becomes trivial to re-use the code on other datasets. What's more, when we do an analysis, we can publish the scripts and the data. We don't have to taken an author's word for it: we can re-run the analysis ourselves or build upon it. This course only touches in the lightest manner on the potential for R for doing digital history. Please see Lincoln Mullen's Computational Historical Thinking with Applications in R for more instruction. For now, we'll do a quick whistle-stop tour of using RStudio to do some analysis in R. You should always keep your research materials (scripts and associated data) organized in separate project folders. RStudio makes this easy for you. When you open RStudio to start a new project, click where it says 'Project (None)' at the right side of the interface. Click on the down arrow, and select New R project. Follow the prompts, and create it in a new directory. R keeps track of what you're up to in a project file, so that you can pick up where you left off. You should also keep your code under version control as well so that you can recover from disaster and/or share your code/data with collaborators; click on that link to get version control set up. Sometimes, it might happen that RStudio crashes (this can be, in DH Box, related to memory issues). If that happens if it just seems to 'hang' (and you've waited several minutes), you can refresh your browser, and go back to DH Box. You'll probably have to re-open your project file. Use the file panel at bottom right to find your \\*.rproj file (your R project file) and click on it to re-open. Save your work often, from the File Save menu. RStudio divides the screen up into four neat panes. The top left is for writing your analytical script (or code; I will use the two words interchangeably). The bottom left is the console where the analysis actually happens. The top right is an environment pane which will show you the variables you've created, your history (commands you've run), and (once it's configured) git; indeed, other tools and plugins will appear as tabs here. The bottom right gives you a preview of any plots or charts you create; once you create one if you click 'zoom' the chart will open in its own pop up so that you can see it better. The file explorer and the help text also appear under tabs in this box. You write your script in the script box, and then you can get RStudio to run the code one line at a time by clicking on Code Run Line (it runs the line of code where you left the cursor). If you select a number of lines of code and hit run line, all of that code will run. R scripts are text files that use .r as their file extension. The console is where the action happens. Click down in the console. Type 3 + 5 and hit enter. RStudio will run the calculation: [1] 8 ! The [1] indicates that this is the first result. Now type a = 3 and hit enter. Over in the top right, the 'Environment' pane updates to tell us that yes, a has a value of 3. Now type b = 5 . The Environment updates accordingly. Now type a + b . Hey, we can do algebra! Anything you can do in Excel, we can do in code here in RStudio. A very good introduction to how R (the language) works is at Try R on codeschool , an interactive tutorial in your browser. Go give some of that a shot.","title":"A quick introduction to R and RStudio"},{"location":"supporting materials/regex-ner/","text":"Further Munging the output of NER So far you've learned how to tag a corpus using the Stanford NER (visit GitHub for that exercise again, as a reminder ). There are several ways we might like to visualize that output. Unfortunately, depending on what you want to do with that data, you are going to have to go back to the data-munging cycle. Let us imagine that we wanted to visualize the locations mentioned in the letters as a kind of network. We will use regular expressions to further manipulate the data into a useful source - target list. REGEX on Mac to Useful Output (PC, please skip to next section) We need to organize those locations so that locations mentioned in a letter are connected to each other. To begin, we trim away any line that does not start with LOCATION. We do this by using our regular expression skills from Module 3, and adding in a few more commands: FIND: ^(?!.*LOCATION).*$ and replace with nothing. We have a few new commands in there: the ?! tells it to begin looking ahead for the literal phrase LOCATION, and then the dot and dollar sign .*$ let us know to end at the end of the line. In any case, this will delete everything that doesn't have the location tag in front. Now, let us also mark those blank lines we noticed as the start of a new letter. FIND: ^\\s*$ where: ^ marks the beginning of a line $ marks the end of a line \\s indicates \u2018whitespace\u2019 \\* indicates a zero or more repetition of the thing in front of it (in this case, whitespace) and we replace with the phrase, \u201cblankspace\u201d. Because there might be a few blank lines, you might see \u2018blankspace\u2019 in places. Where you\u2019ve got \u2018blankspaceblankspace\u2019, that\u2019s showing us the spaces between the original documents (whereas one \u2018blankspace\u2019 was where an ORGANIZATION or PERSON tag was removed). At this point, we might want to remove spaces between place names and use underscores instead, so that when we finally get this into a comma separated format, places like \u2018United States\u2019 will be \u2018United_States\u2019 rather than \u2018united, states\u2019. Find: -a single space Replace: \\_ Now, we want to reintroduce the space after \u2018LOCATION:\u2019, so Find: :_ -this is a colon, followed by an underscore Replace: : -this is a colon, followed by a space Now we want to get all of the locations for a single letter into a single line. So we want to get rid of new lines and LOCATION: Find: \\n(LOCATION:) Replace: It is beginning to look like a list. Let\u2019s replace \u2018blankspaceblankspace\u2019 with \u2018new-document\u2019. Find: blankspaceblankspace Replace: new-document And now let\u2019s get those single blankspace lines excised: Find \\n(blankspace) Replace: Now you have something that looks like the following: p new-document Houston Republic_of_Texas United_States Texas br new-document Texas br new-document United_States Town_of_Columbia br new-document United_States Texas br new-document United_States United_States_of_ Republic_of_Texas br new-document New_Orleans United_States_Govern br new-document United_States Houston United_States Texas united_states br new-document United_States New_Orleans United_States br new-document Houston United_States Texas United_States br new-document New_Orleans br /p Why not leave \u2018new-document\u2019 in the file? It\u2019ll make a handy marker. Let\u2019s replace the spaces with commas: Find: -a single space Replace: , Save your work as a ****.csv file. You now have a file that you can load into a variety of other platforms or tools to perform your analysis. Of course, NER is not perfect, especially when dealing with names like \u2018Houston\u2019 that were a personal name long before they were a place name. REGEX on PC to Useful Output: Open your tagged file in Notepad++. There are a lot of carriage returns, line breaks, and white spaces in this document that will make our regex very complicated and will in fact break it periodically, if we try to work with it as it is. Instead, let us remove all new lines and whitespaces and the outset, and use regex to put structure back. To begin, we want to get the entire document into a single line: Find: \\n Replace with nothing. Notepad++ reports the number of lines in the document at the bottom of the window. It should now report lines: 1. Let\u2019s introduce line breaks to correspond with the original letters (ie. a line break signals a new letter), since we want to visualize the network of locations mentioned where we join places together on the basis of being mentioned in the same letter. If we examine the original document, one candidate for something to search for, to signal a new letter, could be PERSON to PERSON but the named entity recognizer sometimes confuses persons for places or organizations; the object character recognition also makes things complicated. Since the letters are organized chronologically, and we can see \u2018Digitized by Google\u2019 at the bottom of every page (and since no writer in the 1840s is going to use the word digitized) let\u2019s use that as our page break marker. For the most part, one letter corresponds to one page in the book. It\u2019s not ideal, but this is something that the historian will have to discuss in her methods and conclusions. Perhaps, over the seven hundred odd letters in this collection, it doesn\u2019t actually make much difference. Find: (digitized) Replace: \\n\\1 You now have on the order of 700 odd lines in the document. Let\u2019s find the locations and put them on individual lines, so that we can strip out everything else. Find: (LOCATION)(.\\*?)(LOCATION) Replace: \\n\\2 In the search string above, the .\\* would look for everything between the first location on the line and the last location in the line, so we would get a lot of junk. By using .\\*? we just get the text between location and the next location tag. We need to replace the and from the location tags now, as those are characters with special meanings in our regular expressions. Turn off the regular expressions in Notepad++ by unticking the \u2018regular expression\u2019 box. Now search for , , and \\ in turn, replacing them with tildes: LOCATION Texas /LOCATION becomes ~Texas~~~ Now we want to delete in each line everything that isn\u2019t a location. Our locations start the line and are trailed by three tildes, so we just need to find those three tildes and everything that comes after, and delete. Turn regular expressions back on in the search window. Find: (~~~)(.*) Replace with nothing. Our marker for a new page in this book was the line that began with \u2018Digitized by\u2019. We need to delete that line in its entirety, leaving a blank line. Find: (Digitized)(.\\*) Replace: \\n Now we want to get those locations all in the same line again. We need to find the new line character followed by the tilde, and then delete that new line, replacing it with a comma: Find: (\\n)(~) Replace: , Now we remove extraneous blank lines. Find: \\s*$ Replace with nothing. We\u2019re almost there. Let\u2019s remove the comma that starts each line by searching for ^, (remembering that the carat character indicates the start of a line) and replacing with nothing. Save this as cleaned-locations.csv . Congratulations! You\u2019ve taken quite complicated output and cleaned it so that every place mentioned on a single page in the original publication is now on its own line, which means you can import it into a network visualization package, a spreadsheet, or some other tool! Now you can upload this CSV to Gephi.","title":"Going further with regex"},{"location":"supporting materials/regex-ner/#further-munging-the-output-of-ner","text":"So far you've learned how to tag a corpus using the Stanford NER (visit GitHub for that exercise again, as a reminder ). There are several ways we might like to visualize that output. Unfortunately, depending on what you want to do with that data, you are going to have to go back to the data-munging cycle. Let us imagine that we wanted to visualize the locations mentioned in the letters as a kind of network. We will use regular expressions to further manipulate the data into a useful source - target list.","title":"Further Munging the output of NER"},{"location":"supporting materials/regex-ner/#regex-on-mac-to-useful-output-pc-please-skip-to-next-section","text":"We need to organize those locations so that locations mentioned in a letter are connected to each other. To begin, we trim away any line that does not start with LOCATION. We do this by using our regular expression skills from Module 3, and adding in a few more commands: FIND: ^(?!.*LOCATION).*$ and replace with nothing. We have a few new commands in there: the ?! tells it to begin looking ahead for the literal phrase LOCATION, and then the dot and dollar sign .*$ let us know to end at the end of the line. In any case, this will delete everything that doesn't have the location tag in front. Now, let us also mark those blank lines we noticed as the start of a new letter. FIND: ^\\s*$ where: ^ marks the beginning of a line $ marks the end of a line \\s indicates \u2018whitespace\u2019 \\* indicates a zero or more repetition of the thing in front of it (in this case, whitespace) and we replace with the phrase, \u201cblankspace\u201d. Because there might be a few blank lines, you might see \u2018blankspace\u2019 in places. Where you\u2019ve got \u2018blankspaceblankspace\u2019, that\u2019s showing us the spaces between the original documents (whereas one \u2018blankspace\u2019 was where an ORGANIZATION or PERSON tag was removed). At this point, we might want to remove spaces between place names and use underscores instead, so that when we finally get this into a comma separated format, places like \u2018United States\u2019 will be \u2018United_States\u2019 rather than \u2018united, states\u2019. Find: -a single space Replace: \\_ Now, we want to reintroduce the space after \u2018LOCATION:\u2019, so Find: :_ -this is a colon, followed by an underscore Replace: : -this is a colon, followed by a space Now we want to get all of the locations for a single letter into a single line. So we want to get rid of new lines and LOCATION: Find: \\n(LOCATION:) Replace: It is beginning to look like a list. Let\u2019s replace \u2018blankspaceblankspace\u2019 with \u2018new-document\u2019. Find: blankspaceblankspace Replace: new-document And now let\u2019s get those single blankspace lines excised: Find \\n(blankspace) Replace: Now you have something that looks like the following: p new-document Houston Republic_of_Texas United_States Texas br new-document Texas br new-document United_States Town_of_Columbia br new-document United_States Texas br new-document United_States United_States_of_ Republic_of_Texas br new-document New_Orleans United_States_Govern br new-document United_States Houston United_States Texas united_states br new-document United_States New_Orleans United_States br new-document Houston United_States Texas United_States br new-document New_Orleans br /p Why not leave \u2018new-document\u2019 in the file? It\u2019ll make a handy marker. Let\u2019s replace the spaces with commas: Find: -a single space Replace: , Save your work as a ****.csv file. You now have a file that you can load into a variety of other platforms or tools to perform your analysis. Of course, NER is not perfect, especially when dealing with names like \u2018Houston\u2019 that were a personal name long before they were a place name.","title":"REGEX on Mac to Useful Output (PC, please skip to next section)"},{"location":"supporting materials/regex-ner/#regex-on-pc-to-useful-output","text":"Open your tagged file in Notepad++. There are a lot of carriage returns, line breaks, and white spaces in this document that will make our regex very complicated and will in fact break it periodically, if we try to work with it as it is. Instead, let us remove all new lines and whitespaces and the outset, and use regex to put structure back. To begin, we want to get the entire document into a single line: Find: \\n Replace with nothing. Notepad++ reports the number of lines in the document at the bottom of the window. It should now report lines: 1. Let\u2019s introduce line breaks to correspond with the original letters (ie. a line break signals a new letter), since we want to visualize the network of locations mentioned where we join places together on the basis of being mentioned in the same letter. If we examine the original document, one candidate for something to search for, to signal a new letter, could be PERSON to PERSON but the named entity recognizer sometimes confuses persons for places or organizations; the object character recognition also makes things complicated. Since the letters are organized chronologically, and we can see \u2018Digitized by Google\u2019 at the bottom of every page (and since no writer in the 1840s is going to use the word digitized) let\u2019s use that as our page break marker. For the most part, one letter corresponds to one page in the book. It\u2019s not ideal, but this is something that the historian will have to discuss in her methods and conclusions. Perhaps, over the seven hundred odd letters in this collection, it doesn\u2019t actually make much difference. Find: (digitized) Replace: \\n\\1 You now have on the order of 700 odd lines in the document. Let\u2019s find the locations and put them on individual lines, so that we can strip out everything else. Find: (LOCATION)(.\\*?)(LOCATION) Replace: \\n\\2 In the search string above, the .\\* would look for everything between the first location on the line and the last location in the line, so we would get a lot of junk. By using .\\*? we just get the text between location and the next location tag. We need to replace the and from the location tags now, as those are characters with special meanings in our regular expressions. Turn off the regular expressions in Notepad++ by unticking the \u2018regular expression\u2019 box. Now search for , , and \\ in turn, replacing them with tildes: LOCATION Texas /LOCATION becomes ~Texas~~~ Now we want to delete in each line everything that isn\u2019t a location. Our locations start the line and are trailed by three tildes, so we just need to find those three tildes and everything that comes after, and delete. Turn regular expressions back on in the search window. Find: (~~~)(.*) Replace with nothing. Our marker for a new page in this book was the line that began with \u2018Digitized by\u2019. We need to delete that line in its entirety, leaving a blank line. Find: (Digitized)(.\\*) Replace: \\n Now we want to get those locations all in the same line again. We need to find the new line character followed by the tilde, and then delete that new line, replacing it with a comma: Find: (\\n)(~) Replace: , Now we remove extraneous blank lines. Find: \\s*$ Replace with nothing. We\u2019re almost there. Let\u2019s remove the comma that starts each line by searching for ^, (remembering that the carat character indicates the start of a line) and replacing with nothing. Save this as cleaned-locations.csv . Congratulations! You\u2019ve taken quite complicated output and cleaned it so that every place mentioned on a single page in the original publication is now on its own line, which means you can import it into a network visualization package, a spreadsheet, or some other tool! Now you can upload this CSV to Gephi.","title":"REGEX on PC to Useful Output:"},{"location":"supporting materials/regex/","text":"A gentle introduction to Regular Expressions This text is adopted from the first drafts of The Macroscope which is published by Imperial College Press. Introduction A regular expression (also called regex) is a powerful tool for finding and manipulating text. At its simplest, a regular expression is just a way of looking through texts to locate patterns. A regular expression can help you find every line that begins with a number, or every instance of an email address, or whenever a word is used even if there are slight variations in how it's spelled. As long as you can describe the pattern you're looking for, regular expressions can help you find it. Once you've found your patterns, they can then help you manipulate your text so that it fits just what you need. Regular expressions can look pretty complex, but once you know the basic syntax and vocabulary, simple \u2018regexes\u2019 will be easy. Regular expressions can often be used right inside the 'Find and Replace' box in many text and document editors, such as Sublime Text, Atom, or Notepad++. You cannot use Microsoft Word, however! NB In text editors, you have to indicate that you wish to do a regex search. For instance, in Notepad++ when you do a search, to use regular expressions you must tick off the checkbox enabling them. Otherwise, Notepad++ will treat your search literally, looking for that exact text rather than the pattern . Similarly in Textwrangler, you need to tick off the box marked 'grep' when you bring up the search dialogue panel. In Sublime Text, you need to tick the box that has .* in the search panel to enable regular expression searches. Please also note that while this information on regex basics was initially written with the text editors Notepad++ and Textwrangler in mind, all that follows applies equally to other text editors that can work with regular expressions. For now, just read along. In the actual exercise, we will not be using a text editor, but you may wish to some day in the future. Some basic principles Protip: there are libraries of regular expressions, online. For example, if you want to find all postal codes, you can search \u201cregular expression Canadian postal code\u201d and learn what \u2018formula\u2019 to search for to find them. Let's say you're looking for all the instances of \"cat\" or \"dog\" in your document. When you type the vertical bar on your keyboard (it looks like | , shift+backslash on windows keyboards), that means 'or' in regular expressions. So, if your query is dog|cat and you press 'find', it will show you the first time either dog or cat appears in your text. If you want to replace every instance of either \"cat\" or \"dog\" in your document with the world \"animal\", you would open your find-and-replace box, put dog|cat in the search query, put animal in the 'replace' box, hit 'replace all', and watch your entire document fill up with references to animals instead of dogs and cats. The astute reader will have noticed a problem with the instructions above; simply replacing every instance of \"dog\" or \"cat\" with \"animal\" is bound to create problems. Simple searches don't differentiate between letters and spaces, so every time \"cat\" or \"dog\" appear within words, they'll also be replaced with \"animal\". \"catch\" will become \"animalch\"; \"dogma\" will become \"animalma\"; \"certificate\" will become \"certifianimale\". In this case, the solution appears simple; put a space before and after your search query, so now it reads: dog | cat With the spaces, \"animal\" replace \"dog\" or \"cat\" only in those instances where they're definitely complete words; that is, when they're separated by spaces. The even more astute reader will notice that this still does not solve our problem of replacing every instance of \"dog\" or \"cat\". What if the word comes at the beginning of a line, so it is not in front of a space? What if the word is at the end of a sentence or a clause, and thus followed by a punctuation? Luckily, in the language of regex, you can represent the beginning or end of a word using special characters. \\ means the beginning of a word. In some programs, like TextWrangler, this is used instead: \\b so if you search for \\ cat , (or, in TextWrangler, \\bcat )it will find \"cat\", \"catch\", and \"catsup\", but not \"copycat\", because your query searched for words beginning with \"cat\". For patterns at the end of the line, you would use: \\ or in TextWrangler, \\b again. The remainder of this walk-through imagines that you are using Notepad++, but if you\u2019re using Textwrangler, keep this quirk in mind. If you search for cat\\ it will find \"cat\" and \"copycat\", but not \"catch,\" because your query searched for words ending with -\"cat\". Regular expressions can be mixed, so if you wanted to find words only matching \"cat\", no matter where in the sentence, you'd search for \\ cat\\ which would find every instance. And, because all regular expressions can be mixed, if you searched for (in Notepad++; what would you change, if you were using TextWrangler?) \\ cat|dog\\ and replaced all with \"animal\", you would have a document that replaced all instances of \"dog\" or \"cat\" with \"animal\", no matter where in the sentence they appear. You can also search for variations within a single word using parentheses. For example if you were looking for instances of \"gray\" or \"grey\", instead of the search query gray|grey you could type gr(a|e)y instead. The parentheses signify a group, and like the order of operations in arithmetic, regular expressions read the parentheses before anything else. Similarly, if you wanted to find instances of either \"that dog\" or \"that cat\", you would search for: (that dog)|(that cat) Notice that the vertical bar | can appear either inside or outside the parentheses, depending on what you want to search for. The period character . in regular expressions directs the search to just find any character at all. For example, if we searched for: d.g the search would return \"dig\", \"dog\", \"dug\", and so forth. Another special character from our cheat sheet, the plus symbol + instructs the program to find any number of the previous character. If we search for do+g it would return any words that looked like \"dog\", \"doog\", \"dooog\", and so forth. Adding parentheses before the plus would make a search for repetitions of whatever is in the parentheses, for example querying (do)+g would return \"dog\", \"dodog\", \"dododog\", and so forth. Combining the plus + and period . characters can be particularly powerful in regular expressions, instructing the program to find any amount of any characters within your search. A search for d.+g for example, might return \"dried fruits are g\", because the string begins with \"d\" and ends with \"g\", and has various characters in the middle. Searching for simply .+ will yield query results that are entire lines of text, because you are searching for any character, and any amount of them. Parentheses in regular expressions are also very useful when replacing text. The text within a regular expression forms what's called a group, and the software you use to search remembers which groups you queried in order of their appearance. For example, if you search for (dogs)( and )(cats) which would find all instances of \"dogs and cats\" in your document, your program would remember \"dogs\" is group 1, \"and\" is group 2, and \"cats\" is group 3. Notepad++ remembers them as \"\\1\" , \"\\2\" , and \"\\3\" for each group respectively. If you wanted to switch the order of \"dogs\" and \"cats\" every time the phrase \"dogs and cats\" appeared in your document, you would type (dogs)( and )(cats) in the 'find' box, and \\3\\2\\1 in the 'replace' box. That would replace the entire string with group 3 (\"cats\") in the first spot, group 2 (\" and \") in the second spot, and group 1 (\"dogs\") in the last spot, thus changing the result to \"cats and dogs\". The vocabulary of regular expressions is pretty large, but there are many cheat sheets for regex online (one that I sometimes use is the regex lib cheat sheet ; another good one is the regex intro from Active State docs ). Now, continue on to the main exercise","title":"Introduction to Regular Expressions (regex)"},{"location":"supporting materials/regex/#a-gentle-introduction-to-regular-expressions","text":"This text is adopted from the first drafts of The Macroscope which is published by Imperial College Press.","title":"A gentle introduction to Regular Expressions"},{"location":"supporting materials/regex/#introduction","text":"A regular expression (also called regex) is a powerful tool for finding and manipulating text. At its simplest, a regular expression is just a way of looking through texts to locate patterns. A regular expression can help you find every line that begins with a number, or every instance of an email address, or whenever a word is used even if there are slight variations in how it's spelled. As long as you can describe the pattern you're looking for, regular expressions can help you find it. Once you've found your patterns, they can then help you manipulate your text so that it fits just what you need. Regular expressions can look pretty complex, but once you know the basic syntax and vocabulary, simple \u2018regexes\u2019 will be easy. Regular expressions can often be used right inside the 'Find and Replace' box in many text and document editors, such as Sublime Text, Atom, or Notepad++. You cannot use Microsoft Word, however! NB In text editors, you have to indicate that you wish to do a regex search. For instance, in Notepad++ when you do a search, to use regular expressions you must tick off the checkbox enabling them. Otherwise, Notepad++ will treat your search literally, looking for that exact text rather than the pattern . Similarly in Textwrangler, you need to tick off the box marked 'grep' when you bring up the search dialogue panel. In Sublime Text, you need to tick the box that has .* in the search panel to enable regular expression searches. Please also note that while this information on regex basics was initially written with the text editors Notepad++ and Textwrangler in mind, all that follows applies equally to other text editors that can work with regular expressions. For now, just read along. In the actual exercise, we will not be using a text editor, but you may wish to some day in the future.","title":"Introduction"},{"location":"supporting materials/regex/#some-basic-principles","text":"Protip: there are libraries of regular expressions, online. For example, if you want to find all postal codes, you can search \u201cregular expression Canadian postal code\u201d and learn what \u2018formula\u2019 to search for to find them. Let's say you're looking for all the instances of \"cat\" or \"dog\" in your document. When you type the vertical bar on your keyboard (it looks like | , shift+backslash on windows keyboards), that means 'or' in regular expressions. So, if your query is dog|cat and you press 'find', it will show you the first time either dog or cat appears in your text. If you want to replace every instance of either \"cat\" or \"dog\" in your document with the world \"animal\", you would open your find-and-replace box, put dog|cat in the search query, put animal in the 'replace' box, hit 'replace all', and watch your entire document fill up with references to animals instead of dogs and cats. The astute reader will have noticed a problem with the instructions above; simply replacing every instance of \"dog\" or \"cat\" with \"animal\" is bound to create problems. Simple searches don't differentiate between letters and spaces, so every time \"cat\" or \"dog\" appear within words, they'll also be replaced with \"animal\". \"catch\" will become \"animalch\"; \"dogma\" will become \"animalma\"; \"certificate\" will become \"certifianimale\". In this case, the solution appears simple; put a space before and after your search query, so now it reads: dog | cat With the spaces, \"animal\" replace \"dog\" or \"cat\" only in those instances where they're definitely complete words; that is, when they're separated by spaces. The even more astute reader will notice that this still does not solve our problem of replacing every instance of \"dog\" or \"cat\". What if the word comes at the beginning of a line, so it is not in front of a space? What if the word is at the end of a sentence or a clause, and thus followed by a punctuation? Luckily, in the language of regex, you can represent the beginning or end of a word using special characters. \\ means the beginning of a word. In some programs, like TextWrangler, this is used instead: \\b so if you search for \\ cat , (or, in TextWrangler, \\bcat )it will find \"cat\", \"catch\", and \"catsup\", but not \"copycat\", because your query searched for words beginning with \"cat\". For patterns at the end of the line, you would use: \\ or in TextWrangler, \\b again. The remainder of this walk-through imagines that you are using Notepad++, but if you\u2019re using Textwrangler, keep this quirk in mind. If you search for cat\\ it will find \"cat\" and \"copycat\", but not \"catch,\" because your query searched for words ending with -\"cat\". Regular expressions can be mixed, so if you wanted to find words only matching \"cat\", no matter where in the sentence, you'd search for \\ cat\\ which would find every instance. And, because all regular expressions can be mixed, if you searched for (in Notepad++; what would you change, if you were using TextWrangler?) \\ cat|dog\\ and replaced all with \"animal\", you would have a document that replaced all instances of \"dog\" or \"cat\" with \"animal\", no matter where in the sentence they appear. You can also search for variations within a single word using parentheses. For example if you were looking for instances of \"gray\" or \"grey\", instead of the search query gray|grey you could type gr(a|e)y instead. The parentheses signify a group, and like the order of operations in arithmetic, regular expressions read the parentheses before anything else. Similarly, if you wanted to find instances of either \"that dog\" or \"that cat\", you would search for: (that dog)|(that cat) Notice that the vertical bar | can appear either inside or outside the parentheses, depending on what you want to search for. The period character . in regular expressions directs the search to just find any character at all. For example, if we searched for: d.g the search would return \"dig\", \"dog\", \"dug\", and so forth. Another special character from our cheat sheet, the plus symbol + instructs the program to find any number of the previous character. If we search for do+g it would return any words that looked like \"dog\", \"doog\", \"dooog\", and so forth. Adding parentheses before the plus would make a search for repetitions of whatever is in the parentheses, for example querying (do)+g would return \"dog\", \"dodog\", \"dododog\", and so forth. Combining the plus + and period . characters can be particularly powerful in regular expressions, instructing the program to find any amount of any characters within your search. A search for d.+g for example, might return \"dried fruits are g\", because the string begins with \"d\" and ends with \"g\", and has various characters in the middle. Searching for simply .+ will yield query results that are entire lines of text, because you are searching for any character, and any amount of them. Parentheses in regular expressions are also very useful when replacing text. The text within a regular expression forms what's called a group, and the software you use to search remembers which groups you queried in order of their appearance. For example, if you search for (dogs)( and )(cats) which would find all instances of \"dogs and cats\" in your document, your program would remember \"dogs\" is group 1, \"and\" is group 2, and \"cats\" is group 3. Notepad++ remembers them as \"\\1\" , \"\\2\" , and \"\\3\" for each group respectively. If you wanted to switch the order of \"dogs\" and \"cats\" every time the phrase \"dogs and cats\" appeared in your document, you would type (dogs)( and )(cats) in the 'find' box, and \\3\\2\\1 in the 'replace' box. That would replace the entire string with group 3 (\"cats\") in the first spot, group 2 (\" and \") in the second spot, and group 1 (\"dogs\") in the last spot, thus changing the result to \"cats and dogs\". The vocabulary of regular expressions is pretty large, but there are many cheat sheets for regex online (one that I sometimes use is the regex lib cheat sheet ; another good one is the regex intro from Active State docs ).","title":"Some basic principles"},{"location":"supporting materials/regex/#now-continue-on-to-the-main-exercise","text":"","title":"Now, continue on to the main exercise"},{"location":"supporting materials/regexex/","text":"REGEX and the Republic of Texas Regex comes in several different flavours. A good text editor on your own computer like Sublime Text or Atom can do Regex searches and replaces from the find-and-replace box; Word cannot do that. Remember, Regex searches for patterns in the text. The correspondence of the Republic of Texas was collated into a single volume and published with a helpful index in 1911. It was scanned and OCR'd by Google, and is now available as a text file from the Internet Archive. You can see the OCR'd text at archive.org . We are going to grab the index from that file, and transform it using regex. There are several hundred entries in that index. You could clean them up by hand, deleting and cutting and pasting, but with the power of regex, we'll go from this: Sam Houston to A. B. Roman, September 12, 1842 101 Sam Houston to A. B. Roman, October 29, 1842 101 Correspondence for 1843-1846 \u2014 Isaac Van Zandt to Anson Jones, January 11, 1843 103 ...to nicely CSV-formatted table like this: Sam Houston, A. B. Roman, September 12 1842 Sam Houston, A. B. Roman, October 29 1842 Isaac Van Zandt, Anson Jones, January 11 1843 The change doesn't look like much, and you might think to yourself, 'hey, I could just do that by hand'. You could but it'd take you ages, and if you made a mistake somewhere, are you sure you could do this consistently, for a couple of hours at a time? Probably not. Your time is better spent figuring out the search and replace patterns, and then setting your machine loose to implement it. Data formatted like this could be fed into a network analysis program, for instance, or otherwise visualized and analyzed (which we will do in the next module). Regex as we are going to use in this tutorial allows us to go from unstructured to structured data. Getting started In Module 2, Exercise 4 , we learned how to automatically grab text using by interacting with APIs. In this particular exercise today, we'll quickly download the file using curl (it's like wget, though there are some differences between the two commands . It's good to know both). At the command line, type the following curl command: $ curl http://archive.org/stream/diplomaticcorre33statgoog/diplomaticcorre33statgoog_djvu.txt texas.txt The curl command downloads the txt file and the the pushes the result of the command to a file called texas.txt . Open texas.txt with Nano and delete everything except for the index of the list of letters (we just want the index). a. To select a lot of text in Nano, you set a starting point (a mark) with ctrl+shift+6 (the 'carat' symbol: ^). b. Then hit the down arrow on your keyboard, and you will highlight the text. c. When you've selected everything you want, hit ctrl+k to cut the text. That is, you\u2019re looking for the table of letters, starting with \u2018Sam Houston to J. Pinckney Henderson, December 31, 1836 51\u2019 and ending with \u2018Wm. Henry Daingerfield to Ebenezer Allen, February 2, 1846 1582\u2019. Your file will now have approximately 2000 lines in it. Notice that there is a lot of text that we are not interested in at the moment: page numbers, headers, footers, or categories. We're going to use regular expressions to get rid of them. What we want to end up with is a spreadsheet that is arranged in three columns: Sender, Recipient, Date Scroll down through the text; notice there are many lines which don't include a letter, because they're either header info, or blank, or some other extraneous text. We're going to get rid of all of those lines too. We want to keep every line that has this information in it: Sender to Recipient, Month, Date, Year, Page. Save the file in nano: ctrl+x, Y, enter WARNING: Regex can be very tricky. When I'm working with regex, I copy and paste some of the text I'm working on into the box at RegExr and fiddle with the pattern until it does what I want. In fact, spend some time looking at their examples before you go any further in this exercise. The workflow We start by finding every line that looks like a reference to a letter, and put a tilde (a ~ symbol) at the beginning of it so we know to save it for later. Next, we get rid of all the lines that don't start with tildes, so that we're left with only the relevant text. After this is done, we format the remaining text by putting commas in appropriate places, so we can import it into a spreadsheet and do further edits there. We're going to use the sed and grep commands at the command prompt in our DH Box. These commands work differently to achieve different goals. sed (where the 's' means 'stream' and 'ed' means 'editor') works by first identifying text that matches a pattern, and then swapping in the text we want to have, like the following: $ sed 's/old text/new text/g' filename grep works in the following way: $ grep 'PATTERN WE WANT' inputfile The above will print the results to the screen. If we want to redirect the results to a new file, we add this to the end: outputfile . Step One: Identifying lines that have correspondence Senders and Receivers in them Discussion: Read in full before doing any manipulation of your text! If you were using a text editor on your own computer, you would modify your regex appropriately: Notepad++: press ctrl+f or search- find to open the find dialogue box. In that box, go to the 'Replace' tab, and check the radio box for 'Regular expression' at the bottom of the search box. TextWrangler: hit command+f to open the find and replace dialogue box. Tick off the \u2018grep\u2019 radio button (which tells TextWrangler that we want to do a regex search) and the \u2018wraparound\u2019 button (which tells TextWrangler to search everywhere). Sublime text: command+f (Mac) / ctrl+f (Windows) opens the 'find' box (and shift+command+f (Mac) / shift+ctrl+f (Windows) opens find and replace). Tick the .\\* button to tell Sublime we're working with regular expressions. However, we're going to use the two commands sed and grep at the command prompt in DH Box . Remember from our basic introduction that there's a way to see if the word \"to\" appears in full. Type $ grep '\\bto\\b' texas.txt The results print out to the screen. This command finds every instance of the word \"to\" (and not, for instance, also \u2018potato\u2019 or \u2018tomorrow\u2019 try grep 'to' texas.txt instead to see the difference). We don't just want to find \"to\", but the entire line that contains it. We assume that every line that contains the word \u201cto\u201d in full is a line that has relevant letter information, and every line that does not is one we do not need. You learned earlier that the query .+ returns any amount of text, no matter what it says. Thus, the pattern that we will build when we are ready to use the sed command will include .+\\bto\\b.+ therefore, we edit every line which includes the word \"to\" in full (no matter what comes before or after it) and we do not edit any of the lines which do not contain \"to\". As mentioned earlier, we want to add a tilde ~ before each of the lines that look like letters, so we can save them for later. This involves the find-and-replace function, and a query identical to the one before, but with parentheses around it, so it looks like the following (.+\\ to\\ ) and the entire line is placed within a parenthetical group. Since this the first group in our search expression, we can replace that group with \\1 and put the tilde in front of it like so: ~\\1 . Copy and paste some of your text into RegExr.com . Write your regular expression (ie. what you're trying to find), and your substitution (ie. what you're replacing with) in the RegExr interface. Once you're satisfied that you've got it right, we put the complete expression into our sed command: $ sed -r -i.bak 's/(.+\\bto\\b.+)/~\\1/g' texas.txt Where: -r means extended regex. This saves us from having to 'escape' certain characters. -i.bak means make a backup of the original input file, in case things go wrong. -'s/old-pattern/newpattern/g' is how we find and switch what we're looking for. The final g means 'globally', everywhere in the file. texas.txt is the filename that we're looking to change. When you hit enter, the computer seems to pause for a moment, and then gives you the command prompt again. Type ls and you'll see that a new file, texas.txt.bak has been created. Type nano texas.txt and examine the file. You should now have ~ (tilde characters) at the start of each entry of the index! If for some reason your file does not have the tidles, or you've mangled your original file, you can replace texas.txt with the backup file you made like the following: $ mv old-file-name new-file-name , thus, $ mv texas.txt.bak texas.txt . Use Nano to confirm that you're back to where you needed to be, and try again. Step Two: Removing lines that aren\u2019t relevant Discussion After running the find-and-replace, you should note your document now has most of the lines with tildes in front of it, and a few which do not. The next step is to remove all the lines that do not include a tilde. If you were using a text editor on your own computer , the search string to find all lines which don't begin with tildes is \\n[^~].+ A \\n at the beginning of a query searches for a new line, which means it's going to start searching at the first character of each new line. However, given the evolution of computing, it may well be that this won\u2019t quite work on your system. For instance, Linux based systems use \\n for a new line, while Windows often uses \\r\\n , and older Macs just use \\r . These are the sorts of things that can drive us crazy, and so we digital historians need to keep that in mind! Since this will likely cause much frustration, your safest bet will be to save a copy of what you are working on, and then experiment to see what gives you the best result. In most cases, the syntax will be the following: \\r\\n[^~].+ Within a set of square brackets [] the carrot ^ means search for anything that isn't within these brackets (in this case, the tilde ~ ). The .+ as before means search for every remainding character in the line as well. All together, the query returns any full line which does not begin with a tilde; that is, the lines we did not mark as looking like letters. By finding all \\r\\n[^~].+ and replacing it with nothing, you effectively delete all the lines that don't look like the index entries. What you're left with is a series of entries and a series of blank lines. But DH Box makes the regex process so much easier. We are simply going to get grep to find all the lines that have a tilde in them, and write them to a new file like the following: $ grep '~' texas.txt index.txt Use Nano to confirm that this is true. Wasn't that easy? Step Three: Transforming into CSV format Discussion To turn this text file into a spreadsheet, we'll want to separate it out into one column for Sender, one for Recipient, and one for Date, each separated by a single comma. Notice that most lines have extraneous page numbers attached to them; we can get rid of those with regular expressions. There's also usually a comma separating the month-date and the year, which we'll get rid of as well. In the end, the first line should go from looking like the following: ~Sam Houston to J. Pinckney Henderson, December 31, 1836 51 to looking like the following: Sam Houston, J. Pinckney Henderson, December 31 1836 such that each data point is in its own column. You will start by removing the page number after the year and the comma between the year and the month-date. To do this, first locate the year on each line by using the regex: [0-9]{4} We can find any digit between 0 and 9 by searching for [0-9] , and {4} will find four of them together. Now extend that search out by appending .+ to the end of the query; as seen before, it will capture the entire rest of the line. The following query: [0-9]{4}.+ will return, for example, \"1836 51\", \"1839 52\", and \"1839 53\" from the first three lines of the text. We also want to capture the comma preceding the year, so add a comma and a space before the query, resulting in the following: , [0-9]{4}.+ which will return \", 1836 51\", \", 1839 52\", etc. The next step is making the parenthetical groups which will be used to remove parts of the text with find-and-replace. In this case, we want to remove the comma and everything after 'year', but not the year or the space before it. Thus our query will look like the following: (,)( [0-9]{4})(.+) with the comma as the first group \"\\1\" , the space and the year as the second \"\\2\" , and the rest of the line as the third \"\\3\" . Given that all we care about retaining is the second group (we want to keep the year, but not the comma or the page number), what will the replace look like? Find the dates using a regex, and replace so that only the second group in the expression is kept. You might want to consult the introduction to regex again before you execute this one. Remember, the first part of the sed command will be: sed -r -i.bak then the pattern to find, the pattern to replace with, and the file name. You want to use sed on the new index.txt file you made. Can you devise the right pattern? It will resemble the following: $ sed -r -i.bak the-pattern-to-find the-patter-to-replace-with index.txt Step Four: Removing the tildes Find the tildes that we used to mark off our text of interest, and replace them with nothing to delete them. Step Five: Separating Senders and Receivers Discussion Finally, to separate the Sender and Receiver by a comma, we find all instances of the word \"to\" and replace it with a comma. Although we used \\b and \\b to denote the beginning and end of a word earlier in the lesson, we don't exactly do that here. We include the space preceding \u201cto\u201d in the regular expression, as well as the \\b to denote the word ending. Once we find instances of the word and the space preceding it, to\\b we replace it with a comma , . Devise the regex to find the word, and replace with a comma. Step Six: Cleaning up Discussion You may notice that some lines still do not fit our criteria. Line 22, for example, reads \"Abner S. Lipscomb, James Hamilton and A. T. Bumley, AugUHt 15, \". It has an incomplete date; we don't need to worry about these for our purposes. More worrisome are lines, like 61 \"Copy and summary of instructions United States Department of State, \" which include none of the information we want. We can get rid of these lines later in a spreadsheet. The only non-standard lines we need to worry about with regular expressions are the ones with more than 2 commas, like line 178, \"A. J. Donelson, Secretary of State [Allen,. arf interim], December 10 1844\". Notice that our second column, the name of the Recipient, has a comma inside of it. If you were to import this directly into a spreadsheet, you would get four columns, one for Sender, two for Recipient, and one for date, which would break any analysis you would then like to run. Unfortunately these lines need to be fixed by hand, but happily regular expressions make finding them easy. The following query: .+,.+,.+, will show you every line with more than 2 commas, because it finds any line that has any set of characters, then a comma, then any other set, then another comma, and so forth. Use grep to find these. Lastly, at the top of the file, add a new line that simply reads \"Sender, Recipient, Date\". These will be the column headers. Make a copy as a CSV file by using the cp command in the DH Box command line like the following: $ cp index.txt cleaned-correspondence.csv Congratulations! You've now used regex to extract, transform, and clean historical text. As a CSV file, you could now load this data into a network analysis program such as Gephi to explore the ramifications of this correspondence network. Upload your file to your repository, and make a note of the original location of the file, the transformations that you've done, and the date/time. You will be using your cleaned-correspondence.csv file in the next exercise using Open Refine , where we'll sort out some of the messy OCR (fixing names, and so on). Regex patterns for Step Three to Step Six The pattern you want in step three is $ sed -r -i.bak 's/(,)( [0-9]{4})(.+)/\\2/g' index.txt The pattern for step four is $ sed -r -i.bak 's/~//g' index.txt The pattern for step five is $ sed -r -i.bak 's/(\\b to \\b)/,/g' index.txt The pattern for step six is $ grep -E \".+,.+,.+,\" index.txt HOWEVER , in DH Box, the command will use -r instead of -E like the following: $ grep -r \".+,.+,.+,\" index.txt The -E tells grep to treat the pattern as an extended regex (regex with a few more bells and whistles). On DH Box, the flag would be -r .","title":"Regex & the Republic of Texas"},{"location":"supporting materials/regexex/#regex-and-the-republic-of-texas","text":"Regex comes in several different flavours. A good text editor on your own computer like Sublime Text or Atom can do Regex searches and replaces from the find-and-replace box; Word cannot do that. Remember, Regex searches for patterns in the text. The correspondence of the Republic of Texas was collated into a single volume and published with a helpful index in 1911. It was scanned and OCR'd by Google, and is now available as a text file from the Internet Archive. You can see the OCR'd text at archive.org . We are going to grab the index from that file, and transform it using regex. There are several hundred entries in that index. You could clean them up by hand, deleting and cutting and pasting, but with the power of regex, we'll go from this: Sam Houston to A. B. Roman, September 12, 1842 101 Sam Houston to A. B. Roman, October 29, 1842 101 Correspondence for 1843-1846 \u2014 Isaac Van Zandt to Anson Jones, January 11, 1843 103 ...to nicely CSV-formatted table like this: Sam Houston, A. B. Roman, September 12 1842 Sam Houston, A. B. Roman, October 29 1842 Isaac Van Zandt, Anson Jones, January 11 1843 The change doesn't look like much, and you might think to yourself, 'hey, I could just do that by hand'. You could but it'd take you ages, and if you made a mistake somewhere, are you sure you could do this consistently, for a couple of hours at a time? Probably not. Your time is better spent figuring out the search and replace patterns, and then setting your machine loose to implement it. Data formatted like this could be fed into a network analysis program, for instance, or otherwise visualized and analyzed (which we will do in the next module). Regex as we are going to use in this tutorial allows us to go from unstructured to structured data.","title":"REGEX and the Republic of Texas"},{"location":"supporting materials/regexex/#getting-started","text":"In Module 2, Exercise 4 , we learned how to automatically grab text using by interacting with APIs. In this particular exercise today, we'll quickly download the file using curl (it's like wget, though there are some differences between the two commands . It's good to know both). At the command line, type the following curl command: $ curl http://archive.org/stream/diplomaticcorre33statgoog/diplomaticcorre33statgoog_djvu.txt texas.txt The curl command downloads the txt file and the the pushes the result of the command to a file called texas.txt . Open texas.txt with Nano and delete everything except for the index of the list of letters (we just want the index). a. To select a lot of text in Nano, you set a starting point (a mark) with ctrl+shift+6 (the 'carat' symbol: ^). b. Then hit the down arrow on your keyboard, and you will highlight the text. c. When you've selected everything you want, hit ctrl+k to cut the text. That is, you\u2019re looking for the table of letters, starting with \u2018Sam Houston to J. Pinckney Henderson, December 31, 1836 51\u2019 and ending with \u2018Wm. Henry Daingerfield to Ebenezer Allen, February 2, 1846 1582\u2019. Your file will now have approximately 2000 lines in it. Notice that there is a lot of text that we are not interested in at the moment: page numbers, headers, footers, or categories. We're going to use regular expressions to get rid of them. What we want to end up with is a spreadsheet that is arranged in three columns: Sender, Recipient, Date Scroll down through the text; notice there are many lines which don't include a letter, because they're either header info, or blank, or some other extraneous text. We're going to get rid of all of those lines too. We want to keep every line that has this information in it: Sender to Recipient, Month, Date, Year, Page. Save the file in nano: ctrl+x, Y, enter WARNING: Regex can be very tricky. When I'm working with regex, I copy and paste some of the text I'm working on into the box at RegExr and fiddle with the pattern until it does what I want. In fact, spend some time looking at their examples before you go any further in this exercise.","title":"Getting started"},{"location":"supporting materials/regexex/#the-workflow","text":"We start by finding every line that looks like a reference to a letter, and put a tilde (a ~ symbol) at the beginning of it so we know to save it for later. Next, we get rid of all the lines that don't start with tildes, so that we're left with only the relevant text. After this is done, we format the remaining text by putting commas in appropriate places, so we can import it into a spreadsheet and do further edits there. We're going to use the sed and grep commands at the command prompt in our DH Box. These commands work differently to achieve different goals. sed (where the 's' means 'stream' and 'ed' means 'editor') works by first identifying text that matches a pattern, and then swapping in the text we want to have, like the following: $ sed 's/old text/new text/g' filename grep works in the following way: $ grep 'PATTERN WE WANT' inputfile The above will print the results to the screen. If we want to redirect the results to a new file, we add this to the end: outputfile .","title":"The workflow"},{"location":"supporting materials/regexex/#step-one-identifying-lines-that-have-correspondence-senders-and-receivers-in-them","text":"Discussion: Read in full before doing any manipulation of your text! If you were using a text editor on your own computer, you would modify your regex appropriately: Notepad++: press ctrl+f or search- find to open the find dialogue box. In that box, go to the 'Replace' tab, and check the radio box for 'Regular expression' at the bottom of the search box. TextWrangler: hit command+f to open the find and replace dialogue box. Tick off the \u2018grep\u2019 radio button (which tells TextWrangler that we want to do a regex search) and the \u2018wraparound\u2019 button (which tells TextWrangler to search everywhere). Sublime text: command+f (Mac) / ctrl+f (Windows) opens the 'find' box (and shift+command+f (Mac) / shift+ctrl+f (Windows) opens find and replace). Tick the .\\* button to tell Sublime we're working with regular expressions. However, we're going to use the two commands sed and grep at the command prompt in DH Box . Remember from our basic introduction that there's a way to see if the word \"to\" appears in full. Type $ grep '\\bto\\b' texas.txt The results print out to the screen. This command finds every instance of the word \"to\" (and not, for instance, also \u2018potato\u2019 or \u2018tomorrow\u2019 try grep 'to' texas.txt instead to see the difference). We don't just want to find \"to\", but the entire line that contains it. We assume that every line that contains the word \u201cto\u201d in full is a line that has relevant letter information, and every line that does not is one we do not need. You learned earlier that the query .+ returns any amount of text, no matter what it says. Thus, the pattern that we will build when we are ready to use the sed command will include .+\\bto\\b.+ therefore, we edit every line which includes the word \"to\" in full (no matter what comes before or after it) and we do not edit any of the lines which do not contain \"to\". As mentioned earlier, we want to add a tilde ~ before each of the lines that look like letters, so we can save them for later. This involves the find-and-replace function, and a query identical to the one before, but with parentheses around it, so it looks like the following (.+\\ to\\ ) and the entire line is placed within a parenthetical group. Since this the first group in our search expression, we can replace that group with \\1 and put the tilde in front of it like so: ~\\1 . Copy and paste some of your text into RegExr.com . Write your regular expression (ie. what you're trying to find), and your substitution (ie. what you're replacing with) in the RegExr interface. Once you're satisfied that you've got it right, we put the complete expression into our sed command: $ sed -r -i.bak 's/(.+\\bto\\b.+)/~\\1/g' texas.txt Where: -r means extended regex. This saves us from having to 'escape' certain characters. -i.bak means make a backup of the original input file, in case things go wrong. -'s/old-pattern/newpattern/g' is how we find and switch what we're looking for. The final g means 'globally', everywhere in the file. texas.txt is the filename that we're looking to change. When you hit enter, the computer seems to pause for a moment, and then gives you the command prompt again. Type ls and you'll see that a new file, texas.txt.bak has been created. Type nano texas.txt and examine the file. You should now have ~ (tilde characters) at the start of each entry of the index! If for some reason your file does not have the tidles, or you've mangled your original file, you can replace texas.txt with the backup file you made like the following: $ mv old-file-name new-file-name , thus, $ mv texas.txt.bak texas.txt . Use Nano to confirm that you're back to where you needed to be, and try again.","title":"Step One: Identifying lines that have correspondence Senders and Receivers in them"},{"location":"supporting materials/regexex/#step-two-removing-lines-that-arent-relevant","text":"Discussion After running the find-and-replace, you should note your document now has most of the lines with tildes in front of it, and a few which do not. The next step is to remove all the lines that do not include a tilde. If you were using a text editor on your own computer , the search string to find all lines which don't begin with tildes is \\n[^~].+ A \\n at the beginning of a query searches for a new line, which means it's going to start searching at the first character of each new line. However, given the evolution of computing, it may well be that this won\u2019t quite work on your system. For instance, Linux based systems use \\n for a new line, while Windows often uses \\r\\n , and older Macs just use \\r . These are the sorts of things that can drive us crazy, and so we digital historians need to keep that in mind! Since this will likely cause much frustration, your safest bet will be to save a copy of what you are working on, and then experiment to see what gives you the best result. In most cases, the syntax will be the following: \\r\\n[^~].+ Within a set of square brackets [] the carrot ^ means search for anything that isn't within these brackets (in this case, the tilde ~ ). The .+ as before means search for every remainding character in the line as well. All together, the query returns any full line which does not begin with a tilde; that is, the lines we did not mark as looking like letters. By finding all \\r\\n[^~].+ and replacing it with nothing, you effectively delete all the lines that don't look like the index entries. What you're left with is a series of entries and a series of blank lines. But DH Box makes the regex process so much easier. We are simply going to get grep to find all the lines that have a tilde in them, and write them to a new file like the following: $ grep '~' texas.txt index.txt Use Nano to confirm that this is true. Wasn't that easy?","title":"Step Two: Removing lines that aren\u2019t relevant"},{"location":"supporting materials/regexex/#step-three-transforming-into-csv-format","text":"Discussion To turn this text file into a spreadsheet, we'll want to separate it out into one column for Sender, one for Recipient, and one for Date, each separated by a single comma. Notice that most lines have extraneous page numbers attached to them; we can get rid of those with regular expressions. There's also usually a comma separating the month-date and the year, which we'll get rid of as well. In the end, the first line should go from looking like the following: ~Sam Houston to J. Pinckney Henderson, December 31, 1836 51 to looking like the following: Sam Houston, J. Pinckney Henderson, December 31 1836 such that each data point is in its own column. You will start by removing the page number after the year and the comma between the year and the month-date. To do this, first locate the year on each line by using the regex: [0-9]{4} We can find any digit between 0 and 9 by searching for [0-9] , and {4} will find four of them together. Now extend that search out by appending .+ to the end of the query; as seen before, it will capture the entire rest of the line. The following query: [0-9]{4}.+ will return, for example, \"1836 51\", \"1839 52\", and \"1839 53\" from the first three lines of the text. We also want to capture the comma preceding the year, so add a comma and a space before the query, resulting in the following: , [0-9]{4}.+ which will return \", 1836 51\", \", 1839 52\", etc. The next step is making the parenthetical groups which will be used to remove parts of the text with find-and-replace. In this case, we want to remove the comma and everything after 'year', but not the year or the space before it. Thus our query will look like the following: (,)( [0-9]{4})(.+) with the comma as the first group \"\\1\" , the space and the year as the second \"\\2\" , and the rest of the line as the third \"\\3\" . Given that all we care about retaining is the second group (we want to keep the year, but not the comma or the page number), what will the replace look like? Find the dates using a regex, and replace so that only the second group in the expression is kept. You might want to consult the introduction to regex again before you execute this one. Remember, the first part of the sed command will be: sed -r -i.bak then the pattern to find, the pattern to replace with, and the file name. You want to use sed on the new index.txt file you made. Can you devise the right pattern? It will resemble the following: $ sed -r -i.bak the-pattern-to-find the-patter-to-replace-with index.txt","title":"Step Three: Transforming into CSV format"},{"location":"supporting materials/regexex/#step-four-removing-the-tildes","text":"Find the tildes that we used to mark off our text of interest, and replace them with nothing to delete them.","title":"Step Four: Removing the tildes"},{"location":"supporting materials/regexex/#step-five-separating-senders-and-receivers","text":"Discussion Finally, to separate the Sender and Receiver by a comma, we find all instances of the word \"to\" and replace it with a comma. Although we used \\b and \\b to denote the beginning and end of a word earlier in the lesson, we don't exactly do that here. We include the space preceding \u201cto\u201d in the regular expression, as well as the \\b to denote the word ending. Once we find instances of the word and the space preceding it, to\\b we replace it with a comma , . Devise the regex to find the word, and replace with a comma.","title":"Step Five: Separating Senders and Receivers"},{"location":"supporting materials/regexex/#step-six-cleaning-up","text":"Discussion You may notice that some lines still do not fit our criteria. Line 22, for example, reads \"Abner S. Lipscomb, James Hamilton and A. T. Bumley, AugUHt 15, \". It has an incomplete date; we don't need to worry about these for our purposes. More worrisome are lines, like 61 \"Copy and summary of instructions United States Department of State, \" which include none of the information we want. We can get rid of these lines later in a spreadsheet. The only non-standard lines we need to worry about with regular expressions are the ones with more than 2 commas, like line 178, \"A. J. Donelson, Secretary of State [Allen,. arf interim], December 10 1844\". Notice that our second column, the name of the Recipient, has a comma inside of it. If you were to import this directly into a spreadsheet, you would get four columns, one for Sender, two for Recipient, and one for date, which would break any analysis you would then like to run. Unfortunately these lines need to be fixed by hand, but happily regular expressions make finding them easy. The following query: .+,.+,.+, will show you every line with more than 2 commas, because it finds any line that has any set of characters, then a comma, then any other set, then another comma, and so forth. Use grep to find these. Lastly, at the top of the file, add a new line that simply reads \"Sender, Recipient, Date\". These will be the column headers. Make a copy as a CSV file by using the cp command in the DH Box command line like the following: $ cp index.txt cleaned-correspondence.csv Congratulations! You've now used regex to extract, transform, and clean historical text. As a CSV file, you could now load this data into a network analysis program such as Gephi to explore the ramifications of this correspondence network. Upload your file to your repository, and make a note of the original location of the file, the transformations that you've done, and the date/time. You will be using your cleaned-correspondence.csv file in the next exercise using Open Refine , where we'll sort out some of the messy OCR (fixing names, and so on).","title":"Step Six: Cleaning up"},{"location":"supporting materials/regexex/#regex-patterns-for-step-three-to-step-six","text":"The pattern you want in step three is $ sed -r -i.bak 's/(,)( [0-9]{4})(.+)/\\2/g' index.txt The pattern for step four is $ sed -r -i.bak 's/~//g' index.txt The pattern for step five is $ sed -r -i.bak 's/(\\b to \\b)/,/g' index.txt The pattern for step six is $ grep -E \".+,.+,.+,\" index.txt HOWEVER , in DH Box, the command will use -r instead of -E like the following: $ grep -r \".+,.+,.+,\" index.txt The -E tells grep to treat the pattern as an extended regex (regex with a few more bells and whistles). On DH Box, the flag would be -r .","title":"Regex patterns for Step Three to Step Six"},{"location":"supporting materials/tei/","text":"Close Reading with TEI (Text Encoding Initiative) This worksheet, and all related files, are released CC-BY. By M. H. Beals ; Adapted for HIST3814o by S Graham You will need the files in the Crafting Digital History GitHub module 3 folder . You need the folder tei-hist3907 . Select the Clone or Download button and choose download zip to download that repository as a zip file. Unzip it somewhere handy on your machine. Inside will be the subfolder named tei-hist3907 . Open the subfolder named tei-hist3907 . This exercise will explore a historical text and help you create a digital record of your analysis. Vetting a Website Visit the Recovered Histories Website . Examine the site's layout and read its introduction. What makes you believe this site is a trustworthy provider of historical texts? What makes you believe this site is NOT a trustworthy provider of historical texts? Finding a Source Visit the site's collections via the 'Browse' function. Locate the pamphlet Negro Slavery by Zachary Macaulay and open it. This is an abolitionist pamphlet regarding the Atlantic slave trade, presenting and examining evidence of how it is run. When you approach a primary source like this, it is tempting to read through it from beginning to end, to get an overview of its contents, and then 'mine' or 'cherry-pick' good quotations to include in your assessments. However, we are going to focus on examining a very small part of the text in a very high level of detail. Setting Up Your Workspace You will use your own machine rather than DH Box for this work. Arrange your workspace so that you have the scanned text of the pamphlet easily visible on one side of your screen. Open the blanktemplate.txt file in Sublime Text , Atom , Textwrangler or Notepad++ (or any text editor that understands encoding) and have that on the other side of the screen. The last lines will be /body /text /TEI /teiCorpus . Everything you write today should be just above /body tag. Transcribing Your Page Go to the following tag: biblScope 1 /biblScope and replace the number one (1) with the page number you are transcribing. Which page should you transcribe? Select a page in the document that you find interesting. Next, you will need to very carefully transcribe your page of text from the image into your document. Make sure you do not make any changes to grammar or structure in the text , even if you think the author has used poor grammar or misspelled a word. You do not need to worry about line breaks but should start every new paragraph (or heading) with a p and end every paragraph (or heading) with a /p . Once you have completed your transcription, look away from your computer for 30-45 seconds. Staring into the distance every 10-20 minutes will keep your eyes from straining. Also, shake out your hands at the wrists, to prevent repetitive stress injuries to your fingers. Encoding Your Transcription You are now going to encode or mark-up your text. Re-read your page and highlight / colour the following: Any persons mentioned (including any he/she if they refer to a specific person) Any places mentioned Any claims, assertions or arguments made Now that you have highlighted these, you are going to put proper code around them. For persons, surround your text with the following: persName key=\"Last, First\" from=\"YYYY\" to=\"YYYY\" role=\"Occupation\" ref=\"http://www.website.com/webpage.html\" /persName Inside the speech marks for key , include the real full name of the person mentioned In from and to , include their birth and death years, using ? for unknown years In role , put the occupation, role or 'claim to fame' for this individual. In ref , put the URL (link) to the Dictionary of National Biography, Wikipedia or other biography website where you found this information. If there is a in your link, you will need to replace this with amp; . For places, surround your text with the following: placeName key=\"Sheffield, United Kingdom\" ref=\"http://tools.wmflabs.org/geohack/geohack.php?pagename=Sheffield params=53_23_01_N_1_28_01_W_type:city_region:GB\" /placeName In key , put the city and country with best information you can find for the modern names for this location In ref , put a link to the relevant coordinates on Wikipedia GeoHack website . To obtain this, go to the Wikipedia page for this city and click on the latitude/longitude coordinates for the location. For large areas, such as entire countries or continents, just use the Wikipedia page URL. For claims or arguments, surround your text with the following: interp key=\"reason\" n=\"citation\" cert=\"high\" ref=\"http://www.website.com/webpage.html\" /interp In key , explain why you believe this claim is true or not In n , put a full citation to the relevant source In cert (short for certainty), put: high, medium, low or unknown In ref , put the link to the website where you got the information to assess this claim. When you are happy with your work, hit save your work, give it a useful name, make sure it has .xml as the extension, and save it and the .xsl file to your repository. Alex Gill has made The Short and Sweet TEI Handout which you might want to explore as well. When you embark on encoding documents for your own research, Northeastern University has some questions to think about to help you decide what kinds of tagging you'll need; these templates from HisTEI might be useful (open the whole project with OxygenXML for full functionality, but you can copy those templates in any editor). Viewing Your Encoded Text To see your encoded text, make sure your .xml and .xsl file are in the same folder. Open either Internet Explorer or Firefox . The following will not work in Google Chrome because it has different security settings. Making sure both your (page number).xml file and your 000style.xsl file are in the same folder (or both on your desktop), drag the icon for (page number).xml into your browser window. If you now see a colour-coded version of your text, Congratulations! If you hover over the coloured sections, you should see a pop-up with the additional information you entered. If your text comes up only in black, with no paragraph divisions or headings, or doesn't come up at all, something has gone wrong. Re-open your .xml file and check that you have: Placed p at the start of every paragraph, including the start of the page. Placed /p at the end of every paragraph, including the end of the page. Made sure all your persName , placeName and interp tags are properly enclosed in s. Made sure you have both an open and close \\ tag for each tag you use. Made sure you attribute values are fully enclosed in \"\" . Made sure you have a space between the \" of one attribute and the start of the next attribute. Made sure you do NOT have a space after the = of an attribute. If your text still does not appear formatted, you may need to remove the text one paragraph at a time (pasting it somewhere handy), refreshing your browser window, until it appears. This will help you identify which paragraph (or sentence) has the error within it. If you still don't see your text If you do not see the colour-coded version of your text, this might not necessarily mean that you've done something wrong. Some browsers will not perform the transformation, for security reasons. In which case, we can do the following: If you are on a Windows machine using Notepad++, go to 'Plugins' Plugin Tools. (If you are on Windows but aren't using Notepad++, Sublime and Atom probably have a similar functionality, but you will have to search to figure it out.) Select 'XML Tools' from the list, and install it. You'll probably have to restart the program to complete the plugin installation. Open up the 1.xml file in Notepad ++. Under 'Plugins' 'xml tools\" select 'XSL Transformation settings'. In the popup, click on the elipses: ... to open up the file finder, and select the 000style.xsl stylesheet. Click 'transform'. A new tab will open in Notepad++ with a fully-formed html file displaying your data according to the stylesheet. Save this new file and open it in a browser! You can also check 'validate' from the XML Tools menu in Notepad++, which will identify errors in your XML. If you're still having errors, a likely culprit might be the way your geographic URLs are encoded. Compare what you've got with what's in the 1.xml reference document. Advanced: If you install a WAMP or MAMP server, and put your xml and xsl files in the WWW folder, you should be able to see the transformation no problem at localhost\\myxml.xml (for example). (You can also use Python's built in webserver if you have Python on your machine all Mac users for instance do.) You can access the CND.xml , transformed into a CSV on my GitHub . If you right click and choose 'View page source', you'll see the original XML again! Save-as the page as whatever-you-want.csv and you can do some data mining on it. More on transformations I made a file I've called SG_transformer.xsl . Open that file in your text editor. What tags would it be looking for in the XML file? What might it do to your markup? What line would you change in your XML file to get it to point to this stylesheet? Write all this down in your open notebook. It is a good habit to get into to keep track of your thoughts when looking at ancillary files like this. If the nature of your project will involve a lot of transcription, you would be well advised to use an XML editor like OxygenXML , which has a free 1 month trial. The editor makes it easy to maintain consistency in your markup, and also, to quickly create stylesheets for whatever purpose you need. There are also a number of utility programs freely available that will convert XML to CSV or other formats. One such may be found online on Google code . But the best way to transform these XML files is with XSL.","title":"Text Encoding Initiative"},{"location":"supporting materials/tei/#close-reading-with-tei-text-encoding-initiative","text":"This worksheet, and all related files, are released CC-BY. By M. H. Beals ; Adapted for HIST3814o by S Graham You will need the files in the Crafting Digital History GitHub module 3 folder . You need the folder tei-hist3907 . Select the Clone or Download button and choose download zip to download that repository as a zip file. Unzip it somewhere handy on your machine. Inside will be the subfolder named tei-hist3907 . Open the subfolder named tei-hist3907 . This exercise will explore a historical text and help you create a digital record of your analysis.","title":"Close Reading with TEI (Text Encoding Initiative)"},{"location":"supporting materials/tei/#vetting-a-website","text":"Visit the Recovered Histories Website . Examine the site's layout and read its introduction. What makes you believe this site is a trustworthy provider of historical texts? What makes you believe this site is NOT a trustworthy provider of historical texts?","title":"Vetting a Website"},{"location":"supporting materials/tei/#finding-a-source","text":"Visit the site's collections via the 'Browse' function. Locate the pamphlet Negro Slavery by Zachary Macaulay and open it. This is an abolitionist pamphlet regarding the Atlantic slave trade, presenting and examining evidence of how it is run. When you approach a primary source like this, it is tempting to read through it from beginning to end, to get an overview of its contents, and then 'mine' or 'cherry-pick' good quotations to include in your assessments. However, we are going to focus on examining a very small part of the text in a very high level of detail.","title":"Finding a Source"},{"location":"supporting materials/tei/#setting-up-your-workspace","text":"You will use your own machine rather than DH Box for this work. Arrange your workspace so that you have the scanned text of the pamphlet easily visible on one side of your screen. Open the blanktemplate.txt file in Sublime Text , Atom , Textwrangler or Notepad++ (or any text editor that understands encoding) and have that on the other side of the screen. The last lines will be /body /text /TEI /teiCorpus . Everything you write today should be just above /body tag.","title":"Setting Up Your Workspace"},{"location":"supporting materials/tei/#transcribing-your-page","text":"Go to the following tag: biblScope 1 /biblScope and replace the number one (1) with the page number you are transcribing. Which page should you transcribe? Select a page in the document that you find interesting. Next, you will need to very carefully transcribe your page of text from the image into your document. Make sure you do not make any changes to grammar or structure in the text , even if you think the author has used poor grammar or misspelled a word. You do not need to worry about line breaks but should start every new paragraph (or heading) with a p and end every paragraph (or heading) with a /p . Once you have completed your transcription, look away from your computer for 30-45 seconds. Staring into the distance every 10-20 minutes will keep your eyes from straining. Also, shake out your hands at the wrists, to prevent repetitive stress injuries to your fingers.","title":"Transcribing Your Page"},{"location":"supporting materials/tei/#encoding-your-transcription","text":"You are now going to encode or mark-up your text. Re-read your page and highlight / colour the following: Any persons mentioned (including any he/she if they refer to a specific person) Any places mentioned Any claims, assertions or arguments made Now that you have highlighted these, you are going to put proper code around them. For persons, surround your text with the following: persName key=\"Last, First\" from=\"YYYY\" to=\"YYYY\" role=\"Occupation\" ref=\"http://www.website.com/webpage.html\" /persName Inside the speech marks for key , include the real full name of the person mentioned In from and to , include their birth and death years, using ? for unknown years In role , put the occupation, role or 'claim to fame' for this individual. In ref , put the URL (link) to the Dictionary of National Biography, Wikipedia or other biography website where you found this information. If there is a in your link, you will need to replace this with amp; . For places, surround your text with the following: placeName key=\"Sheffield, United Kingdom\" ref=\"http://tools.wmflabs.org/geohack/geohack.php?pagename=Sheffield params=53_23_01_N_1_28_01_W_type:city_region:GB\" /placeName In key , put the city and country with best information you can find for the modern names for this location In ref , put a link to the relevant coordinates on Wikipedia GeoHack website . To obtain this, go to the Wikipedia page for this city and click on the latitude/longitude coordinates for the location. For large areas, such as entire countries or continents, just use the Wikipedia page URL. For claims or arguments, surround your text with the following: interp key=\"reason\" n=\"citation\" cert=\"high\" ref=\"http://www.website.com/webpage.html\" /interp In key , explain why you believe this claim is true or not In n , put a full citation to the relevant source In cert (short for certainty), put: high, medium, low or unknown In ref , put the link to the website where you got the information to assess this claim. When you are happy with your work, hit save your work, give it a useful name, make sure it has .xml as the extension, and save it and the .xsl file to your repository. Alex Gill has made The Short and Sweet TEI Handout which you might want to explore as well. When you embark on encoding documents for your own research, Northeastern University has some questions to think about to help you decide what kinds of tagging you'll need; these templates from HisTEI might be useful (open the whole project with OxygenXML for full functionality, but you can copy those templates in any editor).","title":"Encoding Your Transcription"},{"location":"supporting materials/tei/#viewing-your-encoded-text","text":"To see your encoded text, make sure your .xml and .xsl file are in the same folder. Open either Internet Explorer or Firefox . The following will not work in Google Chrome because it has different security settings. Making sure both your (page number).xml file and your 000style.xsl file are in the same folder (or both on your desktop), drag the icon for (page number).xml into your browser window. If you now see a colour-coded version of your text, Congratulations! If you hover over the coloured sections, you should see a pop-up with the additional information you entered. If your text comes up only in black, with no paragraph divisions or headings, or doesn't come up at all, something has gone wrong. Re-open your .xml file and check that you have: Placed p at the start of every paragraph, including the start of the page. Placed /p at the end of every paragraph, including the end of the page. Made sure all your persName , placeName and interp tags are properly enclosed in s. Made sure you have both an open and close \\ tag for each tag you use. Made sure you attribute values are fully enclosed in \"\" . Made sure you have a space between the \" of one attribute and the start of the next attribute. Made sure you do NOT have a space after the = of an attribute. If your text still does not appear formatted, you may need to remove the text one paragraph at a time (pasting it somewhere handy), refreshing your browser window, until it appears. This will help you identify which paragraph (or sentence) has the error within it.","title":"Viewing Your Encoded Text"},{"location":"supporting materials/tei/#if-you-still-dont-see-your-text","text":"If you do not see the colour-coded version of your text, this might not necessarily mean that you've done something wrong. Some browsers will not perform the transformation, for security reasons. In which case, we can do the following: If you are on a Windows machine using Notepad++, go to 'Plugins' Plugin Tools. (If you are on Windows but aren't using Notepad++, Sublime and Atom probably have a similar functionality, but you will have to search to figure it out.) Select 'XML Tools' from the list, and install it. You'll probably have to restart the program to complete the plugin installation. Open up the 1.xml file in Notepad ++. Under 'Plugins' 'xml tools\" select 'XSL Transformation settings'. In the popup, click on the elipses: ... to open up the file finder, and select the 000style.xsl stylesheet. Click 'transform'. A new tab will open in Notepad++ with a fully-formed html file displaying your data according to the stylesheet. Save this new file and open it in a browser! You can also check 'validate' from the XML Tools menu in Notepad++, which will identify errors in your XML. If you're still having errors, a likely culprit might be the way your geographic URLs are encoded. Compare what you've got with what's in the 1.xml reference document. Advanced: If you install a WAMP or MAMP server, and put your xml and xsl files in the WWW folder, you should be able to see the transformation no problem at localhost\\myxml.xml (for example). (You can also use Python's built in webserver if you have Python on your machine all Mac users for instance do.) You can access the CND.xml , transformed into a CSV on my GitHub . If you right click and choose 'View page source', you'll see the original XML again! Save-as the page as whatever-you-want.csv and you can do some data mining on it.","title":"If you still don't see your text"},{"location":"supporting materials/tei/#more-on-transformations","text":"I made a file I've called SG_transformer.xsl . Open that file in your text editor. What tags would it be looking for in the XML file? What might it do to your markup? What line would you change in your XML file to get it to point to this stylesheet? Write all this down in your open notebook. It is a good habit to get into to keep track of your thoughts when looking at ancillary files like this. If the nature of your project will involve a lot of transcription, you would be well advised to use an XML editor like OxygenXML , which has a free 1 month trial. The editor makes it easy to maintain consistency in your markup, and also, to quickly create stylesheets for whatever purpose you need. There are also a number of utility programs freely available that will convert XML to CSV or other formats. One such may be found online on Google code . But the best way to transform these XML files is with XSL.","title":"More on transformations"},{"location":"supporting materials/topicmodel-r-dhbox/","text":"Topic Modeling in R, DH Box version In this exercise, we're going to grab the Colonial Newspaper Database from my GitHub page, do some exploratory visualizations, and then create a topic model whose output can then be visualized further in other platforms (including as a network in Gephi or other such package). At the appropriate point, I show you how to import a directory of texts rather than a single file of data, and to feed that into the script. Go to your DH Box, and click on RStudio. At the right side where it says 'Project (NONE)', click and create a new project in a new empty directory. (If you want to put this directory under version control with git, so that you can push your work to your GitHub account, please read the RStudio instructions .) In the script panel (top left; click on the green plus sign and select new R script if this pane isn't open) paste the following code and then run each line by putting the cursor in the line and hitting Code Run lines. install.packages(\"mallet\") library(\"mallet\") install.packages(\"RCurl\") library(\"RCurl\") In the future, now that you've installed these packages you won't have to again, so you can comment them out by placing a \\# in front. How to fix RCurl Package installation error When attempting to run install.packages(\"RCurl\") you may get an error along the lines of the following: checking for curl-config... no Cannot find curl-config ERROR: configuration failed for package \u2018RCurl\u2019 To fix this, navigate to the DH Box command line and type the following command: $ sudo apt-get install libcurl4-gnutls-dev When the installation is successful, you can now install RCurl by running the last two lines of the script: install.packages( RCurl ) library( RCurl ) Importing data directly from the web Melodee Beals has been using TEI to markup newspaper articles, creating the Colonial Newspapers Database (which she shared on GitHub). We then used GitHub Pages and an XLST stylesheet to convert that database into a table of comma-separated values . We are now going to topic model the text of those newspaper articles, to see what patterns of discourse may lie within. Now we want to tell RStudio to grab our data from our GitHub page. The thing is, RStudio can easily grab materials from websites where the url is http ; but when it is https (as it is with GitHub), things get a bit more fussy. So what we do is use a special package to grab the data, and then shove it into a variable that we can then tease apart for our analysis. x - getURL( https://raw.githubusercontent.com/shawngraham/exercise/gh-pages/CND.csv , .opts = list(ssl.verifypeer = FALSE)) That line reaches out to the webpage and grabs the information and puts it into a variable called x . documents - read.csv(text = x, col.names=c( Article_ID , Newspaper Title , Newspaper City , Newspaper Province , Newspaper Country , Year , Month , Day , Article Type , Text , Keywords ), colClasses=rep( character , 3), sep= , , quote= ) Now we've created a variable called documents and the read.csv command read all of the data pulled into x , and tells R that documents has columns called \"Newspaper Title\" etc. When we only want information from a particular column, we modify the variable slightly (eg. documents$Keywords would only look at the information in the keywords column). Let's go on a brief digression and actually do that, and see what we learn about this corpus: counts - table(documents$Newspaper.City) We tell R to make a new variable called counts , and fill it with the information from the column 'newspaper city' in documents . It counts them up! Let's make a simple barplot: barplot(counts, main= Cities , xlab= Number of Articles ) The plot will appear in the bottom right pane of RStudio. You can click on 'zoom' to see the plot in a popup window. You can also export it as a PNG or PDF file. Clearly, we\u2019re getting an Edinburgh/Glasgow perspective on things. And somewhere in our data, there\u2019s a mispelled \u2018Edinbugh\u2019. Do you see any other error(s) in the plot? How would you correct it(them)? Let's do the same thing for year, and count the number of articles per year in this corpus: years - table(documents$Year) barplot(years, main= Publication Year , xlab= Year , ylab= Number of Articles ) There\u2019s a lot of material in 1789, another peak around 1819, againg in the late 1830s. We can ask ourselves now: is this an artefact of the data, or of our collection methods? This would be a question a reviewer would want answers to. Let\u2019s assume for now that these two plots are \u2018true\u2019 that, for whatever reasons, only Edinburgh and Glasgow were concerned with these colonial reports, and that they were particulary interested during those three periods. This is already an interesting question that we as historians would want to explore. Try making some more visualizations like this of other aspects of the data. What other patterns do you see that are worth investigating? Now, let's return to getting our data ready to create a topic model. In the line below, note that there is a file called en.txt that it wants to load up. You need to create this file; it's a list of stopwords, or common words that we think do not add value to our model (words like 'the', 'and', 'of' and so on.) The decision of which words to exclude from our analysis is, of course, a theoretical position. To create that file, click on the 'New file' icon in the tool ribbon and select new text file. This will open a blank file in the edit window here. Copy and paste the list of words at en.txt into that blank file and save it as en.txt . This file was put together by Matt Jockers. mallet.instances - mallet.import(documents$Article_ID, documents$Text, en.txt , token.regexp = \\\\p{L}[\\\\p{L}\\\\p{P}]+\\\\p{L} ) That line above passes the article ID and the text of our newspaper articles to the Mallet routine. The stopwords list is generic; it might need to be curated to take into account the pecularities of your data. You might want to create your own, one for each project given the particulars of your project. Note that Jockers compiled his stoplist for his research in literary history of the 19th century. Your mileage may vary! Finally, the last bit after token.regexp applies a regular expression against our newspaper articles, cleaning them up. Reading data from a directory This is an alternative way of ingesting documents for topic modeling. Earlier, you learned how to use wget and some other scripts to download full text documents from Canadiana.org as well as from the Library and Archives Canada (the Canadian war diary). The code below loads those documents into Mallet, after which you can proceed to build a topic model. In the command line, cd into your folder that has your downloaded materials. At the command line, cd into your folder, and type $ pwd to get the full path. Copy it, go back to RStudio, and paste it into the line below between the \" quotation marks. documents - mallet.read.dir( /home/shawngraham/war-diary-text ) # your path will probably look like /home/your-account-in-dhbox/your-folder-of-materials mallet.instances - mallet.import(documents$id, documents$text, en.txt , token.regexp = \\\\p{L}[\\\\p{L}\\\\p{P}]+\\\\p{L} ) NB Do either one or the other, but not both: read from a file, where each row contains the complete text of the document, or read from a folder where each file contains the complete text of the document. Building the topic model The 'correct' number of topics is going to require trial-and-error. You don't want too few, because that would hide the complexity of your data; too many, and you're starting to split hair. #set the number of desired topics num.topics - 20 topic.model - MalletLDA(num.topics) Now we\u2019ve told Mallet how many topics to search for; this is a number you\u2019d want to fiddle with, to find the \u2018best\u2019 number of topics. The next line creates a variable topic.model which will eventually be filled by Mallet using the LDA approach, for 20 topics. Let\u2019s get some info on our topic model, on our distribution of words in these materials. topic.model$loadDocuments(mallet.instances) ## Get the vocabulary, and some statistics about word frequencies. ## These may be useful in further curating the stopword list. vocabulary - topic.model$getVocabulary() word.freqs - mallet.word.freqs(topic.model) head(word.freqs) It is handy to write our output to our project folder periodically; that way you can bring it into other programs if you want to visualize it or explore it further, or if you simply want to have a record of what was going on at a particular point. The line below uses the write.csv command, where the first element is the variable and the second the filename. write.csv(word.freqs, word-freqs.csv ) Word frequencies are handy to look at because it will tell you if you've got words that are 'drowning out' the others. Some of these words you might want to consider adding to your stop words list (and thus, going back to where we first created the mallet.instances and restarting the analysis). By the way, do you see how you might create a bar plot of word frequencies? Now we hit the heavy lifting: generating a topic model. Some of the comments below are very technical; just make sure to run each line of code! (Original code here came from Ben Marwick's analysis of the Day of Archaeology .) ## Optimize hyperparameters every 20 iterations, ## after 50 burn-in iterations. topic.model$setAlphaOptimization(20, 50) ## Now train a model. Note that hyperparameter optimization is on, by default. ## We can specify the number of iterations. Here we'll use a large-ish round number. ## When you run the next line, a *lot* of information will scroll through your console. ## Just be patient and wait til it hits that 1000 iteration. topic.model$train(1000) ## Run through a few iterations where we pick the best topic for each token, ## rather than sampling from the posterior distribution. topic.model$maximize(10) ## Get the probability of topics in documents and the probability of words in topics. ## By default, these functions return raw word counts. Here we want probabilities, ## so we normalize, and add smoothing so that nothing has exactly 0 probability. doc.topics - mallet.doc.topics(topic.model, smoothed=T, normalized=T) topic.words - mallet.topic.words(topic.model, smoothed=T, normalized=T) Congratulations! You now have a topic model. Let\u2019s look at some of our topics. What are the top words in topic 7? Notice that R indexes from 1, so this will be the topic that mallet called topic 6: mallet.top.words(topic.model, topic.words[7,]) Now we\u2019ll write the distribution of the topics by document (ie. newspaper article) to a CSV file that we could explore/visualize with other tools. Then, we\u2019ll take a look at the key words describing each topic. topic.docs - t(doc.topics) topic.docs - topic.docs / rowSums(topic.docs) write.csv(topic.docs, topics-docs.csv ) # that file enables you to see what topics are most present in what issues/documents ## Get a vector containing short names for the topics topics.labels - rep( , num.topics) for (topic in 1:num.topics) topics.labels[topic] - paste(mallet.top.words(topic.model, topic.words[topic,], num.top.words=5)$words, collapse= ) # have a look at keywords for each topic topics.labels write.csv(topics.labels, topics-labels.csv ) Some interesting patterns suggest themselves already! But a list of words doesn\u2019t capture the relative importance of particular words in particular topics. A word might appear in more than one topic, for instance, but really dominate one rather than the other. When you examine the CSV files, you'll notice that each document is given a series of percentages; these add up to 1, and are indicating the percentage which the different topics contribute to the overall composition of that document. Look for the largest numbers to get a sense of what's going on. We could ask R to cluster similarly composed documents together though... A simple histogram plot(hclust(dist(topic.words)), labels=topics.labels) Do you see any interesting clusters? Topics that end up in the same clusters we interpret as being related in some fashion. The plot is a bit crowded; in RStudio you can open it in a new window by clicking 'zoom' to see the dendrogram more clearly. You can also Google 'hclust cran-r' to find tutorials to make a better plot. One thing we can do is to plot it again without labels, to see the structure a bit better: plot(hclust(dist(topic.words))) Now, if we want to get really fancy, we can make a network visualization of how topics interlink due to their distribution in documents. The next bit of code does that, and saves in .graphml format, which packages like Gephi can read. topic_docs - data.frame(topic.docs) names(topic_docs) - documents$article_id install.packages( cluster ) library(cluster) topic_df_dist - as.matrix(daisy(t(topic_docs), metric = euclidean , stand = TRUE)) # Change row values to zero if less than row minimum plus row standard deviation # keep only closely related documents and avoid a dense spagetti diagram # that's difficult to interpret (hat-tip: http://stackoverflow.com/a/16047196/1036500) topic_df_dist[ sweep(topic_df_dist, 1, (apply(topic_df_dist,1,min) + apply(topic_df_dist,1,sd) )) 0 ] - 0 install.packages( igraph ) # the line above would normally install igraph. However, the latest version is not compatible # with this version of R. Thus, go to command line in DH Box, cd to the R folder, cd x86_64-pc-linux-gnu-library, cd 3.0 folder. # wget the older version of igraph that'll work: https://cran.r-project.org/src/contrib/Archive/igraph/igraph_0.6-3.tar.gz # then, at command line, run the following R command: $ R CMD INSTALL igraph_0.6-3.tar.gz # this'll install igraph. Indeed, for any package you can look for older versions of it by slotting in # the name of the package in the url above and browsing the archive. # Remember, we're working with R version 3.03, from 2013, so we need stuff earlier than that. # once installed, call it: library(igraph) # we transform the information from the previous code block into a network g - as.undirected(graph.adjacency(topic_df_dist)) # then we specify the layout and the number of iterations to make it pretty layout1 - layout.fruchterman.reingold(g, niter=100) #then we plot it out plot(g, layout=layout1, edge.curved = TRUE, vertex.size = 1, vertex.color= grey , edge.arrow.size = 0, vertex.label.dist=0.5, vertex.label = NA) When you look at this network, you can see clusters of documents by virtue of largely shared topics. We export this data in a text format called graphml , which can be opened by any text editor, and visualized in nearly any network analysis program for further refinement and analysis. It might be interesting to explore why some issues are so topically focussed, for instance. write.graph(g, file= cnd.graphml , format= graphml ) There are many ways of visualizing and transforming our data. This document only captures a small fraction of the kinds of things you could do. Another good exploration is at Matthew Jockers' website or the global stopwords lists . Ben Marwick does really fun things with the Day of Archaeology blog posts and indeed, some of the code above comes from Marwick\u2019s explorations. Keep your R scripts in your open notebook, and somebody might come along and use them, cite them, improve them, share them! Keep also all your data. Visit my own work for an example .","title":"Topic Modeling, DH Box"},{"location":"supporting materials/topicmodel-r-dhbox/#topic-modeling-in-r-dh-box-version","text":"In this exercise, we're going to grab the Colonial Newspaper Database from my GitHub page, do some exploratory visualizations, and then create a topic model whose output can then be visualized further in other platforms (including as a network in Gephi or other such package). At the appropriate point, I show you how to import a directory of texts rather than a single file of data, and to feed that into the script. Go to your DH Box, and click on RStudio. At the right side where it says 'Project (NONE)', click and create a new project in a new empty directory. (If you want to put this directory under version control with git, so that you can push your work to your GitHub account, please read the RStudio instructions .) In the script panel (top left; click on the green plus sign and select new R script if this pane isn't open) paste the following code and then run each line by putting the cursor in the line and hitting Code Run lines. install.packages(\"mallet\") library(\"mallet\") install.packages(\"RCurl\") library(\"RCurl\") In the future, now that you've installed these packages you won't have to again, so you can comment them out by placing a \\# in front.","title":"Topic Modeling in R, DH Box version"},{"location":"supporting materials/topicmodel-r-dhbox/#how-to-fix-rcurl-package-installation-error","text":"When attempting to run install.packages(\"RCurl\") you may get an error along the lines of the following: checking for curl-config... no Cannot find curl-config ERROR: configuration failed for package \u2018RCurl\u2019 To fix this, navigate to the DH Box command line and type the following command: $ sudo apt-get install libcurl4-gnutls-dev When the installation is successful, you can now install RCurl by running the last two lines of the script: install.packages( RCurl ) library( RCurl )","title":"How to fix RCurl Package installation error"},{"location":"supporting materials/topicmodel-r-dhbox/#importing-data-directly-from-the-web","text":"Melodee Beals has been using TEI to markup newspaper articles, creating the Colonial Newspapers Database (which she shared on GitHub). We then used GitHub Pages and an XLST stylesheet to convert that database into a table of comma-separated values . We are now going to topic model the text of those newspaper articles, to see what patterns of discourse may lie within. Now we want to tell RStudio to grab our data from our GitHub page. The thing is, RStudio can easily grab materials from websites where the url is http ; but when it is https (as it is with GitHub), things get a bit more fussy. So what we do is use a special package to grab the data, and then shove it into a variable that we can then tease apart for our analysis. x - getURL( https://raw.githubusercontent.com/shawngraham/exercise/gh-pages/CND.csv , .opts = list(ssl.verifypeer = FALSE)) That line reaches out to the webpage and grabs the information and puts it into a variable called x . documents - read.csv(text = x, col.names=c( Article_ID , Newspaper Title , Newspaper City , Newspaper Province , Newspaper Country , Year , Month , Day , Article Type , Text , Keywords ), colClasses=rep( character , 3), sep= , , quote= ) Now we've created a variable called documents and the read.csv command read all of the data pulled into x , and tells R that documents has columns called \"Newspaper Title\" etc. When we only want information from a particular column, we modify the variable slightly (eg. documents$Keywords would only look at the information in the keywords column). Let's go on a brief digression and actually do that, and see what we learn about this corpus: counts - table(documents$Newspaper.City) We tell R to make a new variable called counts , and fill it with the information from the column 'newspaper city' in documents . It counts them up! Let's make a simple barplot: barplot(counts, main= Cities , xlab= Number of Articles ) The plot will appear in the bottom right pane of RStudio. You can click on 'zoom' to see the plot in a popup window. You can also export it as a PNG or PDF file. Clearly, we\u2019re getting an Edinburgh/Glasgow perspective on things. And somewhere in our data, there\u2019s a mispelled \u2018Edinbugh\u2019. Do you see any other error(s) in the plot? How would you correct it(them)? Let's do the same thing for year, and count the number of articles per year in this corpus: years - table(documents$Year) barplot(years, main= Publication Year , xlab= Year , ylab= Number of Articles ) There\u2019s a lot of material in 1789, another peak around 1819, againg in the late 1830s. We can ask ourselves now: is this an artefact of the data, or of our collection methods? This would be a question a reviewer would want answers to. Let\u2019s assume for now that these two plots are \u2018true\u2019 that, for whatever reasons, only Edinburgh and Glasgow were concerned with these colonial reports, and that they were particulary interested during those three periods. This is already an interesting question that we as historians would want to explore. Try making some more visualizations like this of other aspects of the data. What other patterns do you see that are worth investigating? Now, let's return to getting our data ready to create a topic model. In the line below, note that there is a file called en.txt that it wants to load up. You need to create this file; it's a list of stopwords, or common words that we think do not add value to our model (words like 'the', 'and', 'of' and so on.) The decision of which words to exclude from our analysis is, of course, a theoretical position. To create that file, click on the 'New file' icon in the tool ribbon and select new text file. This will open a blank file in the edit window here. Copy and paste the list of words at en.txt into that blank file and save it as en.txt . This file was put together by Matt Jockers. mallet.instances - mallet.import(documents$Article_ID, documents$Text, en.txt , token.regexp = \\\\p{L}[\\\\p{L}\\\\p{P}]+\\\\p{L} ) That line above passes the article ID and the text of our newspaper articles to the Mallet routine. The stopwords list is generic; it might need to be curated to take into account the pecularities of your data. You might want to create your own, one for each project given the particulars of your project. Note that Jockers compiled his stoplist for his research in literary history of the 19th century. Your mileage may vary! Finally, the last bit after token.regexp applies a regular expression against our newspaper articles, cleaning them up.","title":"Importing data directly from the web"},{"location":"supporting materials/topicmodel-r-dhbox/#reading-data-from-a-directory","text":"This is an alternative way of ingesting documents for topic modeling. Earlier, you learned how to use wget and some other scripts to download full text documents from Canadiana.org as well as from the Library and Archives Canada (the Canadian war diary). The code below loads those documents into Mallet, after which you can proceed to build a topic model. In the command line, cd into your folder that has your downloaded materials. At the command line, cd into your folder, and type $ pwd to get the full path. Copy it, go back to RStudio, and paste it into the line below between the \" quotation marks. documents - mallet.read.dir( /home/shawngraham/war-diary-text ) # your path will probably look like /home/your-account-in-dhbox/your-folder-of-materials mallet.instances - mallet.import(documents$id, documents$text, en.txt , token.regexp = \\\\p{L}[\\\\p{L}\\\\p{P}]+\\\\p{L} ) NB Do either one or the other, but not both: read from a file, where each row contains the complete text of the document, or read from a folder where each file contains the complete text of the document.","title":"Reading data from a directory"},{"location":"supporting materials/topicmodel-r-dhbox/#building-the-topic-model","text":"The 'correct' number of topics is going to require trial-and-error. You don't want too few, because that would hide the complexity of your data; too many, and you're starting to split hair. #set the number of desired topics num.topics - 20 topic.model - MalletLDA(num.topics) Now we\u2019ve told Mallet how many topics to search for; this is a number you\u2019d want to fiddle with, to find the \u2018best\u2019 number of topics. The next line creates a variable topic.model which will eventually be filled by Mallet using the LDA approach, for 20 topics. Let\u2019s get some info on our topic model, on our distribution of words in these materials. topic.model$loadDocuments(mallet.instances) ## Get the vocabulary, and some statistics about word frequencies. ## These may be useful in further curating the stopword list. vocabulary - topic.model$getVocabulary() word.freqs - mallet.word.freqs(topic.model) head(word.freqs) It is handy to write our output to our project folder periodically; that way you can bring it into other programs if you want to visualize it or explore it further, or if you simply want to have a record of what was going on at a particular point. The line below uses the write.csv command, where the first element is the variable and the second the filename. write.csv(word.freqs, word-freqs.csv ) Word frequencies are handy to look at because it will tell you if you've got words that are 'drowning out' the others. Some of these words you might want to consider adding to your stop words list (and thus, going back to where we first created the mallet.instances and restarting the analysis). By the way, do you see how you might create a bar plot of word frequencies? Now we hit the heavy lifting: generating a topic model. Some of the comments below are very technical; just make sure to run each line of code! (Original code here came from Ben Marwick's analysis of the Day of Archaeology .) ## Optimize hyperparameters every 20 iterations, ## after 50 burn-in iterations. topic.model$setAlphaOptimization(20, 50) ## Now train a model. Note that hyperparameter optimization is on, by default. ## We can specify the number of iterations. Here we'll use a large-ish round number. ## When you run the next line, a *lot* of information will scroll through your console. ## Just be patient and wait til it hits that 1000 iteration. topic.model$train(1000) ## Run through a few iterations where we pick the best topic for each token, ## rather than sampling from the posterior distribution. topic.model$maximize(10) ## Get the probability of topics in documents and the probability of words in topics. ## By default, these functions return raw word counts. Here we want probabilities, ## so we normalize, and add smoothing so that nothing has exactly 0 probability. doc.topics - mallet.doc.topics(topic.model, smoothed=T, normalized=T) topic.words - mallet.topic.words(topic.model, smoothed=T, normalized=T) Congratulations! You now have a topic model. Let\u2019s look at some of our topics. What are the top words in topic 7? Notice that R indexes from 1, so this will be the topic that mallet called topic 6: mallet.top.words(topic.model, topic.words[7,]) Now we\u2019ll write the distribution of the topics by document (ie. newspaper article) to a CSV file that we could explore/visualize with other tools. Then, we\u2019ll take a look at the key words describing each topic. topic.docs - t(doc.topics) topic.docs - topic.docs / rowSums(topic.docs) write.csv(topic.docs, topics-docs.csv ) # that file enables you to see what topics are most present in what issues/documents ## Get a vector containing short names for the topics topics.labels - rep( , num.topics) for (topic in 1:num.topics) topics.labels[topic] - paste(mallet.top.words(topic.model, topic.words[topic,], num.top.words=5)$words, collapse= ) # have a look at keywords for each topic topics.labels write.csv(topics.labels, topics-labels.csv ) Some interesting patterns suggest themselves already! But a list of words doesn\u2019t capture the relative importance of particular words in particular topics. A word might appear in more than one topic, for instance, but really dominate one rather than the other. When you examine the CSV files, you'll notice that each document is given a series of percentages; these add up to 1, and are indicating the percentage which the different topics contribute to the overall composition of that document. Look for the largest numbers to get a sense of what's going on. We could ask R to cluster similarly composed documents together though...","title":"Building the topic model"},{"location":"supporting materials/topicmodel-r-dhbox/#a-simple-histogram","text":"plot(hclust(dist(topic.words)), labels=topics.labels) Do you see any interesting clusters? Topics that end up in the same clusters we interpret as being related in some fashion. The plot is a bit crowded; in RStudio you can open it in a new window by clicking 'zoom' to see the dendrogram more clearly. You can also Google 'hclust cran-r' to find tutorials to make a better plot. One thing we can do is to plot it again without labels, to see the structure a bit better: plot(hclust(dist(topic.words))) Now, if we want to get really fancy, we can make a network visualization of how topics interlink due to their distribution in documents. The next bit of code does that, and saves in .graphml format, which packages like Gephi can read. topic_docs - data.frame(topic.docs) names(topic_docs) - documents$article_id install.packages( cluster ) library(cluster) topic_df_dist - as.matrix(daisy(t(topic_docs), metric = euclidean , stand = TRUE)) # Change row values to zero if less than row minimum plus row standard deviation # keep only closely related documents and avoid a dense spagetti diagram # that's difficult to interpret (hat-tip: http://stackoverflow.com/a/16047196/1036500) topic_df_dist[ sweep(topic_df_dist, 1, (apply(topic_df_dist,1,min) + apply(topic_df_dist,1,sd) )) 0 ] - 0 install.packages( igraph ) # the line above would normally install igraph. However, the latest version is not compatible # with this version of R. Thus, go to command line in DH Box, cd to the R folder, cd x86_64-pc-linux-gnu-library, cd 3.0 folder. # wget the older version of igraph that'll work: https://cran.r-project.org/src/contrib/Archive/igraph/igraph_0.6-3.tar.gz # then, at command line, run the following R command: $ R CMD INSTALL igraph_0.6-3.tar.gz # this'll install igraph. Indeed, for any package you can look for older versions of it by slotting in # the name of the package in the url above and browsing the archive. # Remember, we're working with R version 3.03, from 2013, so we need stuff earlier than that. # once installed, call it: library(igraph) # we transform the information from the previous code block into a network g - as.undirected(graph.adjacency(topic_df_dist)) # then we specify the layout and the number of iterations to make it pretty layout1 - layout.fruchterman.reingold(g, niter=100) #then we plot it out plot(g, layout=layout1, edge.curved = TRUE, vertex.size = 1, vertex.color= grey , edge.arrow.size = 0, vertex.label.dist=0.5, vertex.label = NA) When you look at this network, you can see clusters of documents by virtue of largely shared topics. We export this data in a text format called graphml , which can be opened by any text editor, and visualized in nearly any network analysis program for further refinement and analysis. It might be interesting to explore why some issues are so topically focussed, for instance. write.graph(g, file= cnd.graphml , format= graphml ) There are many ways of visualizing and transforming our data. This document only captures a small fraction of the kinds of things you could do. Another good exploration is at Matthew Jockers' website or the global stopwords lists . Ben Marwick does really fun things with the Day of Archaeology blog posts and indeed, some of the code above comes from Marwick\u2019s explorations. Keep your R scripts in your open notebook, and somebody might come along and use them, cite them, improve them, share them! Keep also all your data. Visit my own work for an example .","title":"A simple histogram"},{"location":"supporting materials/topicmodel-r-yourmachine/","text":"Topic Modeling in R On Your Own machine In this exercise, we're going to grab an archived copy of Melodee Beals' Colonial Newspaper Database from my GitHub page, do some exploratory visualizations, and then create a topic model whose output can then be visualized further in other platforms (including as a network in Gephi). The walkthrough can be found on the HIST3907b website . Each gray block is something to copy-and-paste into your script window in RStudio. Then, put the cursor at the start of the first line, and hit ctrl+enter to get RStudio to execute each line. In the walkthrough, when you get to another gray block, just copy and paste it into your script window after the earlier block. Work your way through the walkthrough. The walkthrough gives you an indication of what the output should look like as you move through it. (The walkthrough was written inside R, and then turned into HTML using an R package called 'Knitr'. You can see that this has implications for open research! For reference, visit the original Rmd (R markdown) file that generated the walkthrough .) When you start RStudio the first time for this exercise make sure to create a new project in a new directory. By the way, when you run the line topic.model$train(1000) your console will fill up with data as it iterates 1000 times over the entire corpus, fitting a topic model to it. This is as it should be! In this way, you'll build up an entire script for topic modeling materials you find on the web. You can then save your script and upload it to your open notebook. In the future, you'd be able to make just a few changes here and there in order to grab and explore different data. Make a note in your open notebook about your process and your observations. Going further If you wanted to use that script on the materials you collected in Module 2, you would have to tell R to load up those materials from a directory, rather than by reading a CSV file. Take a look at my script for topic modeling the Ferguson Grand Jury documents , especially the following line: documents - mallet.read.dir(\"originaldocs/1000chunks/\") You feed it the path to your documents. If you are on a Windows machine, the path would look a bit different, like the following: \"C:\\\\research\\\\originaldocs\\\\1000chunks\\\\\"","title":"Topic Modeling, Local"},{"location":"supporting materials/topicmodel-r-yourmachine/#topic-modeling-in-r-on-your-own-machine","text":"In this exercise, we're going to grab an archived copy of Melodee Beals' Colonial Newspaper Database from my GitHub page, do some exploratory visualizations, and then create a topic model whose output can then be visualized further in other platforms (including as a network in Gephi). The walkthrough can be found on the HIST3907b website . Each gray block is something to copy-and-paste into your script window in RStudio. Then, put the cursor at the start of the first line, and hit ctrl+enter to get RStudio to execute each line. In the walkthrough, when you get to another gray block, just copy and paste it into your script window after the earlier block. Work your way through the walkthrough. The walkthrough gives you an indication of what the output should look like as you move through it. (The walkthrough was written inside R, and then turned into HTML using an R package called 'Knitr'. You can see that this has implications for open research! For reference, visit the original Rmd (R markdown) file that generated the walkthrough .) When you start RStudio the first time for this exercise make sure to create a new project in a new directory. By the way, when you run the line topic.model$train(1000) your console will fill up with data as it iterates 1000 times over the entire corpus, fitting a topic model to it. This is as it should be! In this way, you'll build up an entire script for topic modeling materials you find on the web. You can then save your script and upload it to your open notebook. In the future, you'd be able to make just a few changes here and there in order to grab and explore different data. Make a note in your open notebook about your process and your observations. Going further If you wanted to use that script on the materials you collected in Module 2, you would have to tell R to load up those materials from a directory, rather than by reading a CSV file. Take a look at my script for topic modeling the Ferguson Grand Jury documents , especially the following line: documents - mallet.read.dir(\"originaldocs/1000chunks/\") You feed it the path to your documents. If you are on a Windows machine, the path would look a bit different, like the following: \"C:\\\\research\\\\originaldocs\\\\1000chunks\\\\\"","title":"Topic Modeling in R On Your Own machine"}]}